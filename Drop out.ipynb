{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SleepDataPreparation_and_Classification.ipynb의 사본의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/SleepCapstone/blob/master/Drop%20out.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAGJRSb3xOT1"
      },
      "source": [
        "한림대학교 소프트웨어 융합대학\n",
        "빅데이터 캡스톤 프로젝트\n",
        "\n",
        "딥러닝에 기반한 수면 분류 모델 개발"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SaQVdPHgfmx",
        "outputId": "6ccabd11-3b6f-479d-a28b-def34f09a28b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohoEZomKw0cz",
        "outputId": "7a86f6a9-60fa-4717-b321-333932113823",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd drive/My\\ Drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvna-5vRzmTQ",
        "outputId": "cc68960d-7dc3-4d7b-af8e-f19fa0e326be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# GPU 가용성 체크\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Oct 29 08:27:49 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF2HFogi4Nzq",
        "outputId": "57949eca-2d4c-4043-eb45-3ebe23b80c01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pyedflib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyedflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/1d/f10ea5017a47398fda885c3c313973cf032c19fd6eb773d8e5b816ac3efc/pyEDFlib-0.1.19.tar.gz (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 2.8MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from pyedflib) (1.18.5)\n",
            "Building wheels for collected packages: pyedflib\n",
            "  Building wheel for pyedflib (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyedflib: filename=pyEDFlib-0.1.19-cp36-cp36m-linux_x86_64.whl size=925659 sha256=9745182a6c70dedfefbaf7ebd5e53d269b15f945417889d62763c53aa1436722\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/b7/24/a565e4f7471617165f1b040651b52d87ad1885aaf32e02d4f9\n",
            "Successfully built pyedflib\n",
            "Installing collected packages: pyedflib\n",
            "Successfully installed pyedflib-0.1.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1zNHBNc4beq"
      },
      "source": [
        "import numpy as np\n",
        "from pyedflib import highlevel\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n9vmRl3x-1e"
      },
      "source": [
        "def search_signals_npy(dirname):\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for file in filenames if file.endswith(\".npy\")]\n",
        "    return filenames\n",
        "\n",
        "def search_correct_signals_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vSGY6sO9vgC",
        "outputId": "a52b635c-81e7-4b35-d37a-f47bbab53a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "\n",
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  '/content/drive/My Drive/data_2013_with_npy/origin_npy/annotations/remove_wake/'\n",
        "signals_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "total_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list:\n",
        "    label = np.load(path + filename)\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    \n",
        "    signals = np.load(signals_path+signals_filename)\n",
        "    \n",
        "    \n",
        "    #print('remove start index : %d / remove end index : %d'%(remove_start_index,remove_end_index))\n",
        "    #print(np.bincount(label,minlength=6))\n",
        "    if len(label) !=len(signals[0])//30//fs:\n",
        "        print('file is fault!!!')\n",
        "    for i in range(6):\n",
        "        total_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "print(total_label)\n",
        "\n",
        "x = np.arange(len(total_label))\n",
        "\n",
        "plt.bar(x,total_label,width=0.7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4258  2762 17340  5575  7522    59]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 6 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVVZuGLx-ADW",
        "outputId": "15e54d61-32d6-4c9a-a523-647404509485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  '/content/drive/My Drive/data_2013_with_npy/origin_npy/annotations/remove_wake/'\n",
        "signals_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "random.shuffle(annotations_npy_list)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "trainDataset_count = 30\n",
        "testDataset_count = len(annotations_npy_list)-trainDataset_count\n",
        "\n",
        "print(trainDataset_count)\n",
        "print(testDataset_count)\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4001EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4191EP-Hypnogram.npy']\n",
            "['SC4141EU-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy']\n",
            "30\n",
            "8\n",
            "[12.66021362  7.56008011 44.66622163 15.57743658 19.39919893  0.13684913]\n",
            "[ 6.15404976  6.57755426 52.38221281 12.01694018 22.6310217   0.23822128]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAKH9Mb0_NF3"
      },
      "source": [
        "signals_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "save_train_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/train/'\n",
        "save_test_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/test/'\n",
        "\n",
        "os.makedirs(save_train_path,exist_ok=True)\n",
        "os.makedirs(save_test_path,exist_ok=True)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_train_path+filename)\n",
        "    \n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_test_path+filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4EYtlUH-e9f",
        "outputId": "4e91ea3c-47f4-423e-b8b6-8c48aee6df7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def search_correct_annotations_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename\n",
        "\n",
        "train_path =  '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/train/'\n",
        "test_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/annotations/remove_wake/'\n",
        "\n",
        "train_list = search_signals_npy(train_path)\n",
        "test_list = search_signals_npy(test_path)\n",
        "\n",
        "print(train_list)\n",
        "print(test_list)\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in train_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in test_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4141EU-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy']\n",
            "['SC4142EU-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy']\n",
            "[12.66021362  7.56008011 44.66622163 15.57743658 19.39919893  0.13684913]\n",
            "[ 6.15404976  6.57755426 52.38221281 12.01694018 22.6310217   0.23822128]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBwqVERu_aw-"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pylab as plt\n",
        "from  torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSQaEXQr_h8m"
      },
      "source": [
        "def data_preprocessing_torch(signals): # 하나의 데이터셋에 대한 data_preprocessing (using torch)\n",
        "    signals = (signals - signals.mean(dim=1).unsqueeze(1))/signals.std(dim=1).unsqueeze(1)\n",
        "\n",
        "    return signals\n",
        "\n",
        "def data_preprocessing_oneToOne_torch(signals,min,max,max_value):\n",
        "    signals_std = (signals + max_value) / (2*max_value)\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def data_preprocessing_minmax_torch(signals,min,max):\n",
        "    signals_std = (signals - signals.min(dim=1).unsqueeze(1)) / (\n",
        "            signals.max(dim=1).unsqueeze(1) - signals.min(dim=1).unsqueeze(1))\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def get_dataset_one_channel_norm_withoutCut(dirname,annotations_dir,data_path,use_noise=True,epsilon=0.5,noise_scale=2e-6,preprocessing=True,norm_methods='Standard'):\n",
        "    # npy read!\n",
        "    path = dirname + data_path\n",
        "    signals = np.load(path)\n",
        "\n",
        "    signals = torch.from_numpy(signals).float().to(device)\n",
        "\n",
        "    if use_noise:\n",
        "        if np.random.rand() < epsilon:\n",
        "            # noise = np.random.normal(loc=0,scale=noise_scale,size=signals.shape)\n",
        "            # signals = signals + noise\n",
        "            noise = torch.normal(mean=0., std=noise_scale, size=signals.shape).to(device)\n",
        "            signals = signals + noise\n",
        "\n",
        "    if preprocessing:\n",
        "        if norm_methods == 'Standard':\n",
        "            signals = data_preprocessing_torch(signals)\n",
        "        elif norm_methods == 'minmax':\n",
        "            signals = data_preprocessing_minmax_torch(signals,0,1)\n",
        "        elif norm_methods == 'oneToOne':\n",
        "            signals = data_preprocessing_oneToOne_torch(signals,-1,1,1e-4)\n",
        "    label = get_annotations(annotations_dir, data_path)\n",
        "    return signals, label\n",
        "\n",
        "# model conv layer weight init function\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:         # Conv weight init\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "        \n",
        "def suffle_dataset_list(dataset_list): # 데이터 셔플\n",
        "    random.shuffle(dataset_list)\n",
        "    return dataset_list\n",
        "\n",
        "\n",
        "# npy파일을 통해 label을 가져오는 함수\n",
        "def get_annotations(label_dir,file_name):\n",
        "    label_path = label_dir + file_name\n",
        "    label = np.load(label_path)\n",
        "    return label\n",
        "\n",
        "def signals_expand_torch_one_channel(signals): # 2차원 데이터를 3차원으로 변환 (8,N) -> (batch,8,6000) 형태로\n",
        "    signals = signals.expand(1,1,-1)\n",
        "    #print(signals.shape)\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    signals = signals.reshape(-1, 3000, 1) # 형태 변환\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    return signals"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx4M7Rfq_lwo"
      },
      "source": [
        "class DeepSleepNet_Classification(nn.Module):  # input channel = 8channel / output = 5\n",
        "    def __init__(self,in_channel=1,out_channel=6,layer=[64,128,128,128],sample_rate = 100):\n",
        "        super(DeepSleepNet_Classification, self).__init__()\n",
        "\n",
        "        \n",
        "        self.conv1d_1 = nn.Conv1d(1,16, kernel_size=300, stride=50)\n",
        "        self.conv1d_2 = nn.Conv1d(16, 32, kernel_size=10, stride=5)\n",
        "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "        self.fc1 = nn.Linear(320,160)\n",
        "        self.fc2 = nn.Linear(160,out_channel) \n",
        "\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.conv1d_1(input)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.conv1d_2(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.ReLU(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYZ6YSZHArdJ",
        "outputId": "1b435e06-f13e-48f2-c53f-a592228473f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = DeepSleepNet_Classification(in_channel=1,out_channel=6)\n",
        "summary(model.cuda(),(1,3000))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv1d-1               [-1, 16, 55]           4,816\n",
            "              ReLU-2               [-1, 16, 55]               0\n",
            "            Conv1d-3               [-1, 32, 10]           5,152\n",
            "              ReLU-4               [-1, 32, 10]               0\n",
            "            Linear-5                  [-1, 160]          51,360\n",
            "              ReLU-6                  [-1, 160]               0\n",
            "            Linear-7                    [-1, 6]             966\n",
            "              ReLU-8                    [-1, 6]               0\n",
            "================================================================\n",
            "Total params: 62,294\n",
            "Trainable params: 62,294\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.02\n",
            "Params size (MB): 0.24\n",
            "Estimated Total Size (MB): 0.27\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li3ftAC1Az0Q"
      },
      "source": [
        "def search_npy_list(dirname):  # 매개변수 dir에서 모든 npy파일을 찾고 fold에 따른 dataset 나누기\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for _, file in enumerate(filenames) if file.endswith(\".npy\")]\n",
        "    return filenames\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQH_VRRFBGF1"
      },
      "source": [
        "def train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, test_signal_dir,annotations_dir\n",
        "                               ,epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                               layer_filters=[64,128,256,512],first_conv=[200,40,100],optim='Adam',lf='CE',\n",
        "                               epsilon=0.7,noise_scale=2e-6,min_value=-1e-4,max_value=1e-4,preprocessing=True,\n",
        "                               norm_methods='Standard',use_noise=True,loss_type='softmax'):\n",
        "    # Adam optimizer param\n",
        "    b1 = 0.5\n",
        "    b2 = 0.999\n",
        "\n",
        "    beta = 0.001\n",
        "\n",
        "    check_file = open(logging_filename, 'w')  # logging file\n",
        "\n",
        "    print('Preproceesing  : ',preprocessing)\n",
        "    print('min/max value : %f/%f'%(min_value,max_value))\n",
        "    print('noise scale : ',noise_scale)\n",
        "    print('loss function : ',lf)\n",
        "    print('epsilon : ',epsilon)\n",
        "    print('norm methods : ',norm_methods)\n",
        "\n",
        "    print('logging file name : ', logging_filename)\n",
        "    print('save file name : ', save_filename)\n",
        "    print('layer filters : ',layer_filters)\n",
        "    print('fisrt_conv info : ',first_conv)\n",
        "    print('loss type : ',loss_type)\n",
        "    print('training data oversampling noise : ',use_noise)\n",
        "    best_accuracy = 0.\n",
        "    best_epoch = 0\n",
        "\n",
        "    train_dataset_list = search_npy_list(train_signal_dir)\n",
        "    test_dataset_list = search_npy_list(test_signal_dir)\n",
        "\n",
        "    train_dataset_len = len(train_dataset_list)\n",
        "    test_dataset_len = len(test_dataset_list)\n",
        "\n",
        "    print('train_dataset length : ', len(train_dataset_list))\n",
        "    print(train_dataset_list)\n",
        "\n",
        "    print('test_dataset length : ',test_dataset_len)\n",
        "    print(test_dataset_list)\n",
        "\n",
        "\n",
        "    model = DeepSleepNet_Classification()\n",
        "\n",
        "    #model = resnet18_200hz(in_channel=1,layer_filters=layer_filters,first_conv=first_conv,use_batchnorm=True,num_classes=5)\n",
        "\n",
        "    model.apply(weights_init) # weight init\n",
        "\n",
        "    cuda = torch.cuda.is_available()\n",
        "\n",
        "    if cuda:\n",
        "        print('can use CUDA!!!')\n",
        "        model = model.cuda()\n",
        "    #summary(model,[1,6000])\n",
        "    print('torch.cuda.device_count() : ', torch.cuda.device_count())\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print('Multi GPU Activation !!!')\n",
        "        #model = nn.DataParallel(model)\n",
        "\n",
        "    # loss funcition\n",
        "    if lf == 'CE':\n",
        "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "    elif lf == 'CEW':\n",
        "        samples_per_cls = [27,15,41,5,11]\n",
        "        no_of_classes = 5\n",
        "        effective_num = 1.0 - np.power(beta,samples_per_cls)\n",
        "        #print(effective_num)\n",
        "        weights = (1.0 - beta) / np.array(effective_num)\n",
        "        #print(weights)\n",
        "        weights = weights / np.sum(weights) * no_of_classes\n",
        "        weights = torch.tensor(weights).float()\n",
        "        weights = weights.to(device)\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=weights).to(device)\n",
        "    elif lf == 'FL':\n",
        "        loss_fn = FocalLoss(gamma=2).to(device)\n",
        "    elif lf == 'CBL':\n",
        "        loss_fn = CB_loss(samples_per_cls=[27,15,41,5,11],no_of_classes=5,loss_type=loss_type,beta=0.9999,gamma=2.0)\n",
        "    #loss_fn = FocalLoss(gamma=2).to(device)\n",
        "\n",
        "    # optimizer ADAM (SGD의 경우에는 정상적으로 학습이 진행되지 않았음)\n",
        "    if optim == 'Adam':\n",
        "        print('Optimizer : Adam')\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "    elif optim == 'RMS':\n",
        "        print('Optimizer : RMSprop')\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "    elif optim == 'SGD':\n",
        "        print('Optimizer : SGD')\n",
        "        optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=gamma, patience=10,\n",
        "                                                           min_lr=1e-6)\n",
        "    #stride = 40 일 때, batch_size = 20이면 16GB정도의 메모리 사용\n",
        "    batch_size = 5\n",
        "    norm_square = 2\n",
        "\n",
        "    train_batch_size = math.ceil(train_dataset_len / batch_size)\n",
        "    print('train_batch_size : ',train_batch_size)\n",
        "\n",
        "    test_batch_size = test_dataset_len\n",
        "\n",
        "    best_accuracy = 0.\n",
        "    stop_count = 0\n",
        "    for epoch in range(epochs):\n",
        "        train_dataset = suffle_dataset_list(train_dataset_list) # 매 epoch마다 train_dataset shuffle !\n",
        "        count = 0  # check batch\n",
        "        train_total_loss = 0.0\n",
        "        train_total_count = 0\n",
        "        train_total_data = 0\n",
        "\n",
        "        val_total_loss = 0.0\n",
        "        val_total_count = 0\n",
        "        val_total_data = 0\n",
        "\n",
        "        test_total_loss = 0.0\n",
        "        test_total_count = 0\n",
        "        test_total_data = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "\n",
        "        output_str = 'current_lr : %f\\n'%(optimizer.state_dict()['param_groups'][0]['lr'])\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "        for index, file_name in enumerate(train_dataset):\n",
        "            #print('index : ',index)\n",
        "            if index % batch_size == 0:\n",
        "                batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "            else:\n",
        "                new_signal, new_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "                batch_signal = torch.cat((batch_signal, new_signal),dim=1)\n",
        "                batch_label = np.concatenate((batch_label, new_label))\n",
        "            count += 1\n",
        "            if count == batch_size or index == len(train_dataset) - 1:  # batch 학습 시작!\n",
        "                batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                #batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                # batch_signal = torch.from_numpy(batch_signal).float().to(device)\n",
        "                batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "                optimizer.zero_grad()\n",
        "                # print(batch_signal.shape)\n",
        "                # print(batch_signal)\n",
        "                pred = model(batch_signal)\n",
        "                norm = 0\n",
        "\n",
        "                for parameter in model.parameters():\n",
        "                    norm += torch.norm(parameter, p=norm_square)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label) + beta * norm\n",
        "                #print('loss : ',loss.item())\n",
        "                # loss = loss_fn(pred, batch_label)\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                train_total_loss += loss.item()\n",
        "\n",
        "                train_total_count += check_count\n",
        "                train_total_data += len(batch_signal)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "                count = 0\n",
        "\n",
        "        train_total_loss /= train_batch_size\n",
        "        train_accuracy = train_total_count / train_total_data * 100\n",
        "\n",
        "        output_str = 'train dataset : %d/%d epochs spend time : %.4f sec / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, train_total_loss,\n",
        "                        train_total_count, train_total_data, train_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "    \n",
        "\n",
        "        #check test dataset\n",
        "        start_time = time.time()\n",
        "        for file_name in test_dataset_list:\n",
        "            batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(test_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=False,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "            batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "\n",
        "            batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred = model(batch_signal)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label)\n",
        "\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                test_total_loss += loss.item()\n",
        "                test_total_count += check_count\n",
        "                test_total_data += len(batch_signal)\n",
        "\n",
        "                # 사용하지 않는 변수 제거\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "        test_total_loss /= test_batch_size\n",
        "        test_accuracy = test_total_count / test_total_data * 100\n",
        "\n",
        "\n",
        "\n",
        "        output_str = 'test dataset : %d/%d epochs spend time : %.4f sec  / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, test_total_loss,\n",
        "                        test_total_count, test_total_data, test_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "        scheduler.step(float(test_total_loss))\n",
        "        #scheduler.step()\n",
        "\n",
        "        if epoch == 0:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_epoch = epoch\n",
        "            save_file = save_filename\n",
        "            #save_file = save_path + 'best_SleepEEGNet_CNN_channel%d.pth'%channel\n",
        "            torch.save(model.state_dict(),save_file)\n",
        "            stop_count = 0\n",
        "        else:\n",
        "            if best_accuracy < test_accuracy:\n",
        "                best_accuracy = test_accuracy\n",
        "                best_epoch = epoch\n",
        "                save_file = save_filename\n",
        "                torch.save(model.state_dict(), save_file)\n",
        "                stop_count = 0\n",
        "            else:\n",
        "                stop_count += 1\n",
        "        if stop_count > 30:\n",
        "            print('Early Stopping')\n",
        "            break\n",
        "\n",
        "        output_str = 'best epoch : %d/%d / val accuracy : %f%%\\n' \\\n",
        "                     % (best_epoch+1, epochs, best_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        print('=' * 30)\n",
        "\n",
        "\n",
        "    output_str = 'best epoch : %d/%d / accuracy : %f%%\\n' \\\n",
        "                 % (best_epoch+1, epochs, best_accuracy)\n",
        "    sys.stdout.write(output_str)\n",
        "    check_file.write(output_str)\n",
        "    print('=' * 30)\n",
        "\n",
        "    check_file.close()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-qN1b_NBMC1",
        "outputId": "ae5544ed-2344-4366-8bd8-b372bb80754d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "save_filename = './train.pth'\n",
        "logging_filename = './logging.txt'\n",
        "train_signal_dir = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/train/'\n",
        "test_signal_dir = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_dir = '/content/drive/My Drive/data_2013_with_npy/origin_npy/annotations/remove_wake/'\n",
        "train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, \n",
        "                           test_signal_dir,annotations_dir,\n",
        "                           epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                           layer_filters=[64,128,256,512],first_conv=[200,40,100],\n",
        "                           optim='Adam',lf='CE',epsilon=0,noise_scale=0,\n",
        "                           min_value=-0,max_value=0,preprocessing=True,\n",
        "                           norm_methods='Standard',use_noise=False,loss_type='softmax')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preproceesing  :  True\n",
            "min/max value : 0.000000/0.000000\n",
            "noise scale :  0\n",
            "loss function :  CE\n",
            "epsilon :  0\n",
            "norm methods :  Standard\n",
            "logging file name :  ./logging.txt\n",
            "save file name :  ./train.pth\n",
            "layer filters :  [64, 128, 256, 512]\n",
            "fisrt_conv info :  [200, 40, 100]\n",
            "loss type :  softmax\n",
            "training data oversampling noise :  False\n",
            "train_dataset length :  30\n",
            "['SC4141EU-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy']\n",
            "test_dataset length :  8\n",
            "['SC4142EU-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy']\n",
            "can use CUDA!!!\n",
            "torch.cuda.device_count() :  1\n",
            "Optimizer : Adam\n",
            "train_batch_size :  6\n",
            "current_lr : 0.001000\n",
            "train dataset : 1/2000 epochs spend time : 1.5547 sec / total_loss : 1.7122 correct : 12806/29960 -> 42.7437%\n",
            "test dataset : 1/2000 epochs spend time : 0.2979 sec  / total_loss : 1.4927 correct : 3958/7556 -> 52.3822%\n",
            "best epoch : 1/2000 / val accuracy : 52.382213%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 2/2000 epochs spend time : 1.4060 sec / total_loss : 1.5513 correct : 13285/29960 -> 44.3425%\n",
            "test dataset : 2/2000 epochs spend time : 0.2850 sec  / total_loss : 1.4509 correct : 3969/7556 -> 52.5278%\n",
            "best epoch : 2/2000 / val accuracy : 52.527792%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 3/2000 epochs spend time : 1.4591 sec / total_loss : 1.5029 correct : 13582/29960 -> 45.3338%\n",
            "test dataset : 3/2000 epochs spend time : 0.3104 sec  / total_loss : 1.3911 correct : 3994/7556 -> 52.8587%\n",
            "best epoch : 3/2000 / val accuracy : 52.858655%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 4/2000 epochs spend time : 1.4577 sec / total_loss : 1.4522 correct : 14479/29960 -> 48.3278%\n",
            "test dataset : 4/2000 epochs spend time : 0.2858 sec  / total_loss : 1.3491 correct : 4200/7556 -> 55.5850%\n",
            "best epoch : 4/2000 / val accuracy : 55.584966%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 5/2000 epochs spend time : 1.3829 sec / total_loss : 1.3692 correct : 14688/29960 -> 49.0254%\n",
            "test dataset : 5/2000 epochs spend time : 0.2873 sec  / total_loss : 1.1841 correct : 4342/7556 -> 57.4643%\n",
            "best epoch : 5/2000 / val accuracy : 57.464267%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 6/2000 epochs spend time : 1.4285 sec / total_loss : 1.2843 correct : 15529/29960 -> 51.8324%\n",
            "test dataset : 6/2000 epochs spend time : 0.2742 sec  / total_loss : 1.1532 correct : 4065/7556 -> 53.7983%\n",
            "best epoch : 5/2000 / val accuracy : 57.464267%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 7/2000 epochs spend time : 1.4127 sec / total_loss : 1.2386 correct : 15333/29960 -> 51.1782%\n",
            "test dataset : 7/2000 epochs spend time : 0.2813 sec  / total_loss : 1.1862 correct : 4059/7556 -> 53.7189%\n",
            "best epoch : 5/2000 / val accuracy : 57.464267%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 8/2000 epochs spend time : 1.3936 sec / total_loss : 1.1897 correct : 15764/29960 -> 52.6168%\n",
            "test dataset : 8/2000 epochs spend time : 0.2926 sec  / total_loss : 1.0285 correct : 4482/7556 -> 59.3171%\n",
            "best epoch : 8/2000 / val accuracy : 59.317099%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 9/2000 epochs spend time : 1.4299 sec / total_loss : 1.1325 correct : 17566/29960 -> 58.6315%\n",
            "test dataset : 9/2000 epochs spend time : 0.2938 sec  / total_loss : 1.0246 correct : 4468/7556 -> 59.1318%\n",
            "best epoch : 8/2000 / val accuracy : 59.317099%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 10/2000 epochs spend time : 1.4663 sec / total_loss : 1.1276 correct : 17227/29960 -> 57.5000%\n",
            "test dataset : 10/2000 epochs spend time : 0.2910 sec  / total_loss : 1.0143 correct : 4963/7556 -> 65.6829%\n",
            "best epoch : 10/2000 / val accuracy : 65.682901%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 11/2000 epochs spend time : 1.3824 sec / total_loss : 1.1051 correct : 18000/29960 -> 60.0801%\n",
            "test dataset : 11/2000 epochs spend time : 0.2860 sec  / total_loss : 1.0240 correct : 5087/7556 -> 67.3240%\n",
            "best epoch : 11/2000 / val accuracy : 67.323981%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 12/2000 epochs spend time : 1.4546 sec / total_loss : 1.0965 correct : 17979/29960 -> 60.0100%\n",
            "test dataset : 12/2000 epochs spend time : 0.2827 sec  / total_loss : 1.0196 correct : 5036/7556 -> 66.6490%\n",
            "best epoch : 11/2000 / val accuracy : 67.323981%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 13/2000 epochs spend time : 1.3937 sec / total_loss : 1.0777 correct : 18159/29960 -> 60.6108%\n",
            "test dataset : 13/2000 epochs spend time : 0.2861 sec  / total_loss : 0.9987 correct : 4873/7556 -> 64.4918%\n",
            "best epoch : 11/2000 / val accuracy : 67.323981%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 14/2000 epochs spend time : 1.4250 sec / total_loss : 1.0963 correct : 18058/29960 -> 60.2737%\n",
            "test dataset : 14/2000 epochs spend time : 0.2997 sec  / total_loss : 0.9553 correct : 4968/7556 -> 65.7491%\n",
            "best epoch : 11/2000 / val accuracy : 67.323981%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 15/2000 epochs spend time : 1.4305 sec / total_loss : 1.0639 correct : 18354/29960 -> 61.2617%\n",
            "test dataset : 15/2000 epochs spend time : 0.2936 sec  / total_loss : 0.9803 correct : 5148/7556 -> 68.1313%\n",
            "best epoch : 15/2000 / val accuracy : 68.131286%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 16/2000 epochs spend time : 1.4591 sec / total_loss : 1.0442 correct : 19048/29960 -> 63.5781%\n",
            "test dataset : 16/2000 epochs spend time : 0.2821 sec  / total_loss : 0.9898 correct : 5090/7556 -> 67.3637%\n",
            "best epoch : 15/2000 / val accuracy : 68.131286%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 17/2000 epochs spend time : 1.4778 sec / total_loss : 1.0729 correct : 18485/29960 -> 61.6989%\n",
            "test dataset : 17/2000 epochs spend time : 0.2950 sec  / total_loss : 0.9235 correct : 5267/7556 -> 69.7062%\n",
            "best epoch : 17/2000 / val accuracy : 69.706194%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 18/2000 epochs spend time : 1.4505 sec / total_loss : 1.0194 correct : 18464/29960 -> 61.6288%\n",
            "test dataset : 18/2000 epochs spend time : 0.2942 sec  / total_loss : 0.9228 correct : 4702/7556 -> 62.2287%\n",
            "best epoch : 17/2000 / val accuracy : 69.706194%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 19/2000 epochs spend time : 1.4466 sec / total_loss : 1.0316 correct : 18462/29960 -> 61.6222%\n",
            "test dataset : 19/2000 epochs spend time : 0.2825 sec  / total_loss : 1.0015 correct : 4966/7556 -> 65.7226%\n",
            "best epoch : 17/2000 / val accuracy : 69.706194%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 20/2000 epochs spend time : 1.4610 sec / total_loss : 1.0305 correct : 18442/29960 -> 61.5554%\n",
            "test dataset : 20/2000 epochs spend time : 0.2831 sec  / total_loss : 0.9097 correct : 5038/7556 -> 66.6755%\n",
            "best epoch : 17/2000 / val accuracy : 69.706194%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 21/2000 epochs spend time : 1.4178 sec / total_loss : 0.9890 correct : 18896/29960 -> 63.0708%\n",
            "test dataset : 21/2000 epochs spend time : 0.2900 sec  / total_loss : 0.8658 correct : 5277/7556 -> 69.8385%\n",
            "best epoch : 21/2000 / val accuracy : 69.838539%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 22/2000 epochs spend time : 1.3760 sec / total_loss : 0.9806 correct : 19065/29960 -> 63.6348%\n",
            "test dataset : 22/2000 epochs spend time : 0.2748 sec  / total_loss : 0.9547 correct : 4805/7556 -> 63.5918%\n",
            "best epoch : 21/2000 / val accuracy : 69.838539%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 23/2000 epochs spend time : 1.4537 sec / total_loss : 1.0002 correct : 18755/29960 -> 62.6001%\n",
            "test dataset : 23/2000 epochs spend time : 0.2801 sec  / total_loss : 0.9306 correct : 5154/7556 -> 68.2107%\n",
            "best epoch : 21/2000 / val accuracy : 69.838539%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 24/2000 epochs spend time : 1.5028 sec / total_loss : 0.9601 correct : 19244/29960 -> 64.2323%\n",
            "test dataset : 24/2000 epochs spend time : 0.2741 sec  / total_loss : 0.9372 correct : 4812/7556 -> 63.6845%\n",
            "best epoch : 21/2000 / val accuracy : 69.838539%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 25/2000 epochs spend time : 1.4290 sec / total_loss : 0.9915 correct : 18584/29960 -> 62.0294%\n",
            "test dataset : 25/2000 epochs spend time : 0.3012 sec  / total_loss : 0.8580 correct : 5200/7556 -> 68.8195%\n",
            "best epoch : 21/2000 / val accuracy : 69.838539%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 26/2000 epochs spend time : 1.4786 sec / total_loss : 0.9385 correct : 19378/29960 -> 64.6796%\n",
            "test dataset : 26/2000 epochs spend time : 0.2798 sec  / total_loss : 0.8067 correct : 5411/7556 -> 71.6120%\n",
            "best epoch : 26/2000 / val accuracy : 71.611964%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 27/2000 epochs spend time : 1.4766 sec / total_loss : 0.9349 correct : 19597/29960 -> 65.4105%\n",
            "test dataset : 27/2000 epochs spend time : 0.2820 sec  / total_loss : 0.8705 correct : 5013/7556 -> 66.3446%\n",
            "best epoch : 26/2000 / val accuracy : 71.611964%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 28/2000 epochs spend time : 1.4739 sec / total_loss : 0.9877 correct : 18916/29960 -> 63.1375%\n",
            "test dataset : 28/2000 epochs spend time : 0.2970 sec  / total_loss : 0.8121 correct : 5330/7556 -> 70.5400%\n",
            "best epoch : 26/2000 / val accuracy : 71.611964%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 29/2000 epochs spend time : 1.4192 sec / total_loss : 0.9453 correct : 19384/29960 -> 64.6996%\n",
            "test dataset : 29/2000 epochs spend time : 0.2835 sec  / total_loss : 0.9723 correct : 4890/7556 -> 64.7168%\n",
            "best epoch : 26/2000 / val accuracy : 71.611964%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 30/2000 epochs spend time : 1.4239 sec / total_loss : 0.9564 correct : 19054/29960 -> 63.5981%\n",
            "test dataset : 30/2000 epochs spend time : 0.2848 sec  / total_loss : 0.9503 correct : 4874/7556 -> 64.5050%\n",
            "best epoch : 26/2000 / val accuracy : 71.611964%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 31/2000 epochs spend time : 1.5345 sec / total_loss : 0.9608 correct : 19275/29960 -> 64.3358%\n",
            "test dataset : 31/2000 epochs spend time : 0.2907 sec  / total_loss : 0.7982 correct : 5434/7556 -> 71.9164%\n",
            "best epoch : 31/2000 / val accuracy : 71.916358%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 32/2000 epochs spend time : 1.4301 sec / total_loss : 0.9680 correct : 19191/29960 -> 64.0554%\n",
            "test dataset : 32/2000 epochs spend time : 0.2976 sec  / total_loss : 0.7857 correct : 5380/7556 -> 71.2017%\n",
            "best epoch : 31/2000 / val accuracy : 71.916358%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 33/2000 epochs spend time : 1.4287 sec / total_loss : 0.9391 correct : 19640/29960 -> 65.5541%\n",
            "test dataset : 33/2000 epochs spend time : 0.2827 sec  / total_loss : 0.8794 correct : 5068/7556 -> 67.0725%\n",
            "best epoch : 31/2000 / val accuracy : 71.916358%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 34/2000 epochs spend time : 1.4738 sec / total_loss : 0.9671 correct : 19188/29960 -> 64.0454%\n",
            "test dataset : 34/2000 epochs spend time : 0.2897 sec  / total_loss : 0.8306 correct : 5355/7556 -> 70.8708%\n",
            "best epoch : 31/2000 / val accuracy : 71.916358%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 35/2000 epochs spend time : 1.4807 sec / total_loss : 0.9123 correct : 19879/29960 -> 66.3518%\n",
            "test dataset : 35/2000 epochs spend time : 0.3007 sec  / total_loss : 0.7933 correct : 5392/7556 -> 71.3605%\n",
            "best epoch : 31/2000 / val accuracy : 71.916358%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 36/2000 epochs spend time : 1.5336 sec / total_loss : 0.9549 correct : 19153/29960 -> 63.9286%\n",
            "test dataset : 36/2000 epochs spend time : 0.2962 sec  / total_loss : 0.9184 correct : 5115/7556 -> 67.6945%\n",
            "best epoch : 31/2000 / val accuracy : 71.916358%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 37/2000 epochs spend time : 1.4736 sec / total_loss : 0.9171 correct : 19432/29960 -> 64.8598%\n",
            "test dataset : 37/2000 epochs spend time : 0.2927 sec  / total_loss : 0.7958 correct : 5344/7556 -> 70.7253%\n",
            "best epoch : 31/2000 / val accuracy : 71.916358%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 38/2000 epochs spend time : 1.4661 sec / total_loss : 0.8998 correct : 19864/29960 -> 66.3017%\n",
            "test dataset : 38/2000 epochs spend time : 0.3157 sec  / total_loss : 0.7853 correct : 5463/7556 -> 72.3002%\n",
            "best epoch : 38/2000 / val accuracy : 72.300159%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 39/2000 epochs spend time : 1.4337 sec / total_loss : 0.8941 correct : 19867/29960 -> 66.3117%\n",
            "test dataset : 39/2000 epochs spend time : 0.2797 sec  / total_loss : 0.8775 correct : 5255/7556 -> 69.5474%\n",
            "best epoch : 38/2000 / val accuracy : 72.300159%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 40/2000 epochs spend time : 1.3869 sec / total_loss : 0.8845 correct : 19903/29960 -> 66.4319%\n",
            "test dataset : 40/2000 epochs spend time : 0.2966 sec  / total_loss : 0.7537 correct : 5457/7556 -> 72.2208%\n",
            "best epoch : 38/2000 / val accuracy : 72.300159%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 41/2000 epochs spend time : 1.4967 sec / total_loss : 0.9183 correct : 19447/29960 -> 64.9099%\n",
            "test dataset : 41/2000 epochs spend time : 0.3129 sec  / total_loss : 0.8682 correct : 5203/7556 -> 68.8592%\n",
            "best epoch : 38/2000 / val accuracy : 72.300159%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 42/2000 epochs spend time : 1.4524 sec / total_loss : 0.8993 correct : 19564/29960 -> 65.3004%\n",
            "test dataset : 42/2000 epochs spend time : 0.2923 sec  / total_loss : 0.7635 correct : 5503/7556 -> 72.8295%\n",
            "best epoch : 42/2000 / val accuracy : 72.829539%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 43/2000 epochs spend time : 1.4358 sec / total_loss : 0.8722 correct : 20172/29960 -> 67.3298%\n",
            "test dataset : 43/2000 epochs spend time : 0.2966 sec  / total_loss : 0.7787 correct : 5485/7556 -> 72.5913%\n",
            "best epoch : 42/2000 / val accuracy : 72.829539%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 44/2000 epochs spend time : 1.4733 sec / total_loss : 0.8941 correct : 19925/29960 -> 66.5053%\n",
            "test dataset : 44/2000 epochs spend time : 0.2903 sec  / total_loss : 0.8673 correct : 5265/7556 -> 69.6797%\n",
            "best epoch : 42/2000 / val accuracy : 72.829539%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 45/2000 epochs spend time : 1.3999 sec / total_loss : 0.9010 correct : 19578/29960 -> 65.3471%\n",
            "test dataset : 45/2000 epochs spend time : 0.2917 sec  / total_loss : 0.8078 correct : 5390/7556 -> 71.3340%\n",
            "best epoch : 42/2000 / val accuracy : 72.829539%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 46/2000 epochs spend time : 1.4565 sec / total_loss : 0.8701 correct : 20246/29960 -> 67.5768%\n",
            "test dataset : 46/2000 epochs spend time : 0.2961 sec  / total_loss : 0.7356 correct : 5577/7556 -> 73.8089%\n",
            "best epoch : 46/2000 / val accuracy : 73.808894%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 47/2000 epochs spend time : 1.5004 sec / total_loss : 0.8940 correct : 20065/29960 -> 66.9726%\n",
            "test dataset : 47/2000 epochs spend time : 0.3097 sec  / total_loss : 0.8243 correct : 5391/7556 -> 71.3473%\n",
            "best epoch : 46/2000 / val accuracy : 73.808894%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 48/2000 epochs spend time : 1.4763 sec / total_loss : 0.8957 correct : 19861/29960 -> 66.2917%\n",
            "test dataset : 48/2000 epochs spend time : 0.2877 sec  / total_loss : 0.8260 correct : 5291/7556 -> 70.0238%\n",
            "best epoch : 46/2000 / val accuracy : 73.808894%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 49/2000 epochs spend time : 1.4164 sec / total_loss : 0.8822 correct : 20254/29960 -> 67.6035%\n",
            "test dataset : 49/2000 epochs spend time : 0.2873 sec  / total_loss : 0.7265 correct : 5543/7556 -> 73.3589%\n",
            "best epoch : 46/2000 / val accuracy : 73.808894%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 50/2000 epochs spend time : 1.5260 sec / total_loss : 0.8748 correct : 20165/29960 -> 67.3064%\n",
            "test dataset : 50/2000 epochs spend time : 0.2908 sec  / total_loss : 0.8459 correct : 5333/7556 -> 70.5797%\n",
            "best epoch : 46/2000 / val accuracy : 73.808894%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 51/2000 epochs spend time : 1.4290 sec / total_loss : 0.8987 correct : 19948/29960 -> 66.5821%\n",
            "test dataset : 51/2000 epochs spend time : 0.2917 sec  / total_loss : 0.7521 correct : 5570/7556 -> 73.7163%\n",
            "best epoch : 46/2000 / val accuracy : 73.808894%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 52/2000 epochs spend time : 1.5124 sec / total_loss : 0.8705 correct : 20149/29960 -> 67.2530%\n",
            "test dataset : 52/2000 epochs spend time : 0.2921 sec  / total_loss : 0.7467 correct : 5554/7556 -> 73.5045%\n",
            "best epoch : 46/2000 / val accuracy : 73.808894%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 53/2000 epochs spend time : 1.4763 sec / total_loss : 0.8856 correct : 20171/29960 -> 67.3264%\n",
            "test dataset : 53/2000 epochs spend time : 0.2908 sec  / total_loss : 0.8354 correct : 5085/7556 -> 67.2975%\n",
            "best epoch : 46/2000 / val accuracy : 73.808894%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 54/2000 epochs spend time : 1.5035 sec / total_loss : 0.9082 correct : 19800/29960 -> 66.0881%\n",
            "test dataset : 54/2000 epochs spend time : 0.2917 sec  / total_loss : 0.7487 correct : 5541/7556 -> 73.3325%\n",
            "best epoch : 46/2000 / val accuracy : 73.808894%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 55/2000 epochs spend time : 1.4579 sec / total_loss : 0.8560 correct : 20387/29960 -> 68.0474%\n",
            "test dataset : 55/2000 epochs spend time : 0.3042 sec  / total_loss : 0.8127 correct : 5350/7556 -> 70.8047%\n",
            "best epoch : 46/2000 / val accuracy : 73.808894%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 56/2000 epochs spend time : 1.4403 sec / total_loss : 0.8553 correct : 20420/29960 -> 68.1575%\n",
            "test dataset : 56/2000 epochs spend time : 0.2987 sec  / total_loss : 0.8536 correct : 5273/7556 -> 69.7856%\n",
            "best epoch : 46/2000 / val accuracy : 73.808894%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 57/2000 epochs spend time : 1.5016 sec / total_loss : 0.8598 correct : 20354/29960 -> 67.9372%\n",
            "test dataset : 57/2000 epochs spend time : 0.3139 sec  / total_loss : 0.7803 correct : 5490/7556 -> 72.6575%\n",
            "best epoch : 46/2000 / val accuracy : 73.808894%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 58/2000 epochs spend time : 1.4812 sec / total_loss : 0.8489 correct : 20518/29960 -> 68.4846%\n",
            "test dataset : 58/2000 epochs spend time : 0.2880 sec  / total_loss : 0.7384 correct : 5575/7556 -> 73.7824%\n",
            "best epoch : 46/2000 / val accuracy : 73.808894%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 59/2000 epochs spend time : 1.4884 sec / total_loss : 0.8476 correct : 20502/29960 -> 68.4312%\n",
            "test dataset : 59/2000 epochs spend time : 0.3021 sec  / total_loss : 0.7546 correct : 5539/7556 -> 73.3060%\n",
            "best epoch : 46/2000 / val accuracy : 73.808894%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 60/2000 epochs spend time : 1.4758 sec / total_loss : 0.8592 correct : 20316/29960 -> 67.8104%\n",
            "test dataset : 60/2000 epochs spend time : 0.3019 sec  / total_loss : 0.7108 correct : 5610/7556 -> 74.2456%\n",
            "best epoch : 60/2000 / val accuracy : 74.245633%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 61/2000 epochs spend time : 1.4475 sec / total_loss : 0.8617 correct : 20154/29960 -> 67.2697%\n",
            "test dataset : 61/2000 epochs spend time : 0.2964 sec  / total_loss : 0.7273 correct : 5593/7556 -> 74.0206%\n",
            "best epoch : 60/2000 / val accuracy : 74.245633%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 62/2000 epochs spend time : 1.4647 sec / total_loss : 0.8453 correct : 20476/29960 -> 68.3445%\n",
            "test dataset : 62/2000 epochs spend time : 0.2805 sec  / total_loss : 0.7558 correct : 5544/7556 -> 73.3722%\n",
            "best epoch : 60/2000 / val accuracy : 74.245633%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 63/2000 epochs spend time : 1.3793 sec / total_loss : 0.8745 correct : 19967/29960 -> 66.6455%\n",
            "test dataset : 63/2000 epochs spend time : 0.2837 sec  / total_loss : 0.7225 correct : 5555/7556 -> 73.5177%\n",
            "best epoch : 60/2000 / val accuracy : 74.245633%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 64/2000 epochs spend time : 1.4311 sec / total_loss : 0.8491 correct : 20459/29960 -> 68.2877%\n",
            "test dataset : 64/2000 epochs spend time : 0.2881 sec  / total_loss : 0.7257 correct : 5561/7556 -> 73.5971%\n",
            "best epoch : 60/2000 / val accuracy : 74.245633%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 65/2000 epochs spend time : 1.4649 sec / total_loss : 0.8753 correct : 20054/29960 -> 66.9359%\n",
            "test dataset : 65/2000 epochs spend time : 0.2851 sec  / total_loss : 0.7446 correct : 5570/7556 -> 73.7163%\n",
            "best epoch : 60/2000 / val accuracy : 74.245633%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 66/2000 epochs spend time : 1.4187 sec / total_loss : 0.8718 correct : 20289/29960 -> 67.7203%\n",
            "test dataset : 66/2000 epochs spend time : 0.2848 sec  / total_loss : 0.7150 correct : 5585/7556 -> 73.9148%\n",
            "best epoch : 60/2000 / val accuracy : 74.245633%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 67/2000 epochs spend time : 1.4738 sec / total_loss : 0.8554 correct : 20392/29960 -> 68.0641%\n",
            "test dataset : 67/2000 epochs spend time : 0.3049 sec  / total_loss : 0.7126 correct : 5624/7556 -> 74.4309%\n",
            "best epoch : 67/2000 / val accuracy : 74.430916%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 68/2000 epochs spend time : 1.4351 sec / total_loss : 0.8536 correct : 20436/29960 -> 68.2109%\n",
            "test dataset : 68/2000 epochs spend time : 0.2853 sec  / total_loss : 0.7047 correct : 5609/7556 -> 74.2324%\n",
            "best epoch : 67/2000 / val accuracy : 74.430916%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 69/2000 epochs spend time : 1.4483 sec / total_loss : 0.8377 correct : 20392/29960 -> 68.0641%\n",
            "test dataset : 69/2000 epochs spend time : 0.2882 sec  / total_loss : 0.7042 correct : 5586/7556 -> 73.9280%\n",
            "best epoch : 67/2000 / val accuracy : 74.430916%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 70/2000 epochs spend time : 1.4344 sec / total_loss : 0.8313 correct : 20683/29960 -> 69.0354%\n",
            "test dataset : 70/2000 epochs spend time : 0.3079 sec  / total_loss : 0.7256 correct : 5603/7556 -> 74.1530%\n",
            "best epoch : 67/2000 / val accuracy : 74.430916%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 71/2000 epochs spend time : 1.4075 sec / total_loss : 0.8398 correct : 20556/29960 -> 68.6115%\n",
            "test dataset : 71/2000 epochs spend time : 0.2834 sec  / total_loss : 0.7962 correct : 5318/7556 -> 70.3812%\n",
            "best epoch : 67/2000 / val accuracy : 74.430916%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 72/2000 epochs spend time : 1.4457 sec / total_loss : 0.8304 correct : 20532/29960 -> 68.5314%\n",
            "test dataset : 72/2000 epochs spend time : 0.2724 sec  / total_loss : 0.6998 correct : 5644/7556 -> 74.6956%\n",
            "best epoch : 72/2000 / val accuracy : 74.695606%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 73/2000 epochs spend time : 1.4262 sec / total_loss : 0.8318 correct : 20510/29960 -> 68.4579%\n",
            "test dataset : 73/2000 epochs spend time : 0.3143 sec  / total_loss : 0.7633 correct : 5491/7556 -> 72.6707%\n",
            "best epoch : 72/2000 / val accuracy : 74.695606%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 74/2000 epochs spend time : 1.3799 sec / total_loss : 0.8484 correct : 20391/29960 -> 68.0607%\n",
            "test dataset : 74/2000 epochs spend time : 0.2900 sec  / total_loss : 0.7650 correct : 5430/7556 -> 71.8634%\n",
            "best epoch : 72/2000 / val accuracy : 74.695606%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 75/2000 epochs spend time : 1.5069 sec / total_loss : 0.8263 correct : 20559/29960 -> 68.6215%\n",
            "test dataset : 75/2000 epochs spend time : 0.2844 sec  / total_loss : 0.7540 correct : 5485/7556 -> 72.5913%\n",
            "best epoch : 72/2000 / val accuracy : 74.695606%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 76/2000 epochs spend time : 1.4727 sec / total_loss : 0.8461 correct : 20296/29960 -> 67.7437%\n",
            "test dataset : 76/2000 epochs spend time : 0.2956 sec  / total_loss : 0.7012 correct : 5679/7556 -> 75.1588%\n",
            "best epoch : 76/2000 / val accuracy : 75.158814%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 77/2000 epochs spend time : 1.4735 sec / total_loss : 0.8081 correct : 20971/29960 -> 69.9967%\n",
            "test dataset : 77/2000 epochs spend time : 0.2934 sec  / total_loss : 0.7757 correct : 5460/7556 -> 72.2605%\n",
            "best epoch : 76/2000 / val accuracy : 75.158814%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 78/2000 epochs spend time : 1.4267 sec / total_loss : 0.8278 correct : 20568/29960 -> 68.6515%\n",
            "test dataset : 78/2000 epochs spend time : 0.2826 sec  / total_loss : 0.6916 correct : 5664/7556 -> 74.9603%\n",
            "best epoch : 76/2000 / val accuracy : 75.158814%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 79/2000 epochs spend time : 1.4631 sec / total_loss : 0.8193 correct : 20703/29960 -> 69.1021%\n",
            "test dataset : 79/2000 epochs spend time : 0.2883 sec  / total_loss : 0.7657 correct : 5507/7556 -> 72.8825%\n",
            "best epoch : 76/2000 / val accuracy : 75.158814%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 80/2000 epochs spend time : 1.4113 sec / total_loss : 0.8166 correct : 20806/29960 -> 69.4459%\n",
            "test dataset : 80/2000 epochs spend time : 0.2954 sec  / total_loss : 0.8137 correct : 5203/7556 -> 68.8592%\n",
            "best epoch : 76/2000 / val accuracy : 75.158814%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 81/2000 epochs spend time : 1.4311 sec / total_loss : 0.8490 correct : 20281/29960 -> 67.6936%\n",
            "test dataset : 81/2000 epochs spend time : 0.2866 sec  / total_loss : 0.7886 correct : 5411/7556 -> 71.6120%\n",
            "best epoch : 76/2000 / val accuracy : 75.158814%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 82/2000 epochs spend time : 1.4221 sec / total_loss : 0.8229 correct : 20650/29960 -> 68.9252%\n",
            "test dataset : 82/2000 epochs spend time : 0.2852 sec  / total_loss : 0.7453 correct : 5519/7556 -> 73.0413%\n",
            "best epoch : 76/2000 / val accuracy : 75.158814%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 83/2000 epochs spend time : 1.3885 sec / total_loss : 0.8115 correct : 20739/29960 -> 69.2223%\n",
            "test dataset : 83/2000 epochs spend time : 0.2896 sec  / total_loss : 0.6829 correct : 5687/7556 -> 75.2647%\n",
            "best epoch : 83/2000 / val accuracy : 75.264690%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 84/2000 epochs spend time : 1.4544 sec / total_loss : 0.8242 correct : 20580/29960 -> 68.6916%\n",
            "test dataset : 84/2000 epochs spend time : 0.2970 sec  / total_loss : 0.7148 correct : 5624/7556 -> 74.4309%\n",
            "best epoch : 83/2000 / val accuracy : 75.264690%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 85/2000 epochs spend time : 1.4387 sec / total_loss : 0.8340 correct : 20529/29960 -> 68.5214%\n",
            "test dataset : 85/2000 epochs spend time : 0.2884 sec  / total_loss : 0.7314 correct : 5592/7556 -> 74.0074%\n",
            "best epoch : 83/2000 / val accuracy : 75.264690%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 86/2000 epochs spend time : 1.4437 sec / total_loss : 0.8249 correct : 20766/29960 -> 69.3124%\n",
            "test dataset : 86/2000 epochs spend time : 0.2968 sec  / total_loss : 0.7110 correct : 5619/7556 -> 74.3647%\n",
            "best epoch : 83/2000 / val accuracy : 75.264690%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 87/2000 epochs spend time : 1.4549 sec / total_loss : 0.8159 correct : 20705/29960 -> 69.1088%\n",
            "test dataset : 87/2000 epochs spend time : 0.3054 sec  / total_loss : 0.6897 correct : 5668/7556 -> 75.0132%\n",
            "best epoch : 83/2000 / val accuracy : 75.264690%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 88/2000 epochs spend time : 1.4771 sec / total_loss : 0.8056 correct : 20755/29960 -> 69.2757%\n",
            "test dataset : 88/2000 epochs spend time : 0.2862 sec  / total_loss : 0.7712 correct : 5488/7556 -> 72.6310%\n",
            "best epoch : 83/2000 / val accuracy : 75.264690%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 89/2000 epochs spend time : 1.3743 sec / total_loss : 0.8097 correct : 20596/29960 -> 68.7450%\n",
            "test dataset : 89/2000 epochs spend time : 0.2786 sec  / total_loss : 0.6738 correct : 5693/7556 -> 75.3441%\n",
            "best epoch : 89/2000 / val accuracy : 75.344097%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 90/2000 epochs spend time : 1.4200 sec / total_loss : 0.8192 correct : 20676/29960 -> 69.0120%\n",
            "test dataset : 90/2000 epochs spend time : 0.2880 sec  / total_loss : 0.8134 correct : 5373/7556 -> 71.1091%\n",
            "best epoch : 89/2000 / val accuracy : 75.344097%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 91/2000 epochs spend time : 1.4467 sec / total_loss : 0.8011 correct : 21024/29960 -> 70.1736%\n",
            "test dataset : 91/2000 epochs spend time : 0.2830 sec  / total_loss : 0.6808 correct : 5709/7556 -> 75.5558%\n",
            "best epoch : 91/2000 / val accuracy : 75.555850%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 92/2000 epochs spend time : 1.4139 sec / total_loss : 0.7839 correct : 20976/29960 -> 70.0134%\n",
            "test dataset : 92/2000 epochs spend time : 0.2883 sec  / total_loss : 0.7475 correct : 5547/7556 -> 73.4119%\n",
            "best epoch : 91/2000 / val accuracy : 75.555850%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 93/2000 epochs spend time : 1.4248 sec / total_loss : 0.8214 correct : 20522/29960 -> 68.4980%\n",
            "test dataset : 93/2000 epochs spend time : 0.2906 sec  / total_loss : 0.7311 correct : 5598/7556 -> 74.0868%\n",
            "best epoch : 91/2000 / val accuracy : 75.555850%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 94/2000 epochs spend time : 1.4243 sec / total_loss : 0.7820 correct : 21423/29960 -> 71.5053%\n",
            "test dataset : 94/2000 epochs spend time : 0.2886 sec  / total_loss : 0.7310 correct : 5571/7556 -> 73.7295%\n",
            "best epoch : 91/2000 / val accuracy : 75.555850%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 95/2000 epochs spend time : 1.4387 sec / total_loss : 0.8049 correct : 20998/29960 -> 70.0868%\n",
            "test dataset : 95/2000 epochs spend time : 0.2890 sec  / total_loss : 0.7285 correct : 5574/7556 -> 73.7692%\n",
            "best epoch : 91/2000 / val accuracy : 75.555850%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 96/2000 epochs spend time : 1.3995 sec / total_loss : 0.8060 correct : 20908/29960 -> 69.7864%\n",
            "test dataset : 96/2000 epochs spend time : 0.2891 sec  / total_loss : 0.6779 correct : 5714/7556 -> 75.6220%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 97/2000 epochs spend time : 1.5043 sec / total_loss : 0.7960 correct : 21095/29960 -> 70.4105%\n",
            "test dataset : 97/2000 epochs spend time : 0.2869 sec  / total_loss : 0.8318 correct : 5202/7556 -> 68.8460%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 98/2000 epochs spend time : 1.4459 sec / total_loss : 0.8253 correct : 20674/29960 -> 69.0053%\n",
            "test dataset : 98/2000 epochs spend time : 0.2810 sec  / total_loss : 0.7035 correct : 5666/7556 -> 74.9868%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 99/2000 epochs spend time : 1.4511 sec / total_loss : 0.7861 correct : 21293/29960 -> 71.0714%\n",
            "test dataset : 99/2000 epochs spend time : 0.2889 sec  / total_loss : 0.6661 correct : 5707/7556 -> 75.5294%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 100/2000 epochs spend time : 1.4817 sec / total_loss : 0.8083 correct : 20714/29960 -> 69.1389%\n",
            "test dataset : 100/2000 epochs spend time : 0.2905 sec  / total_loss : 0.6678 correct : 5705/7556 -> 75.5029%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 101/2000 epochs spend time : 1.3987 sec / total_loss : 0.7929 correct : 21388/29960 -> 71.3885%\n",
            "test dataset : 101/2000 epochs spend time : 0.2796 sec  / total_loss : 0.6624 correct : 5708/7556 -> 75.5426%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 102/2000 epochs spend time : 1.4220 sec / total_loss : 0.7740 correct : 21073/29960 -> 70.3371%\n",
            "test dataset : 102/2000 epochs spend time : 0.2886 sec  / total_loss : 0.7108 correct : 5621/7556 -> 74.3912%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 103/2000 epochs spend time : 1.4760 sec / total_loss : 0.7925 correct : 21180/29960 -> 70.6943%\n",
            "test dataset : 103/2000 epochs spend time : 0.2912 sec  / total_loss : 0.7491 correct : 5497/7556 -> 72.7501%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 104/2000 epochs spend time : 1.4085 sec / total_loss : 0.8196 correct : 20655/29960 -> 68.9419%\n",
            "test dataset : 104/2000 epochs spend time : 0.2814 sec  / total_loss : 0.7317 correct : 5554/7556 -> 73.5045%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 105/2000 epochs spend time : 1.4733 sec / total_loss : 0.7685 correct : 21323/29960 -> 71.1716%\n",
            "test dataset : 105/2000 epochs spend time : 0.2860 sec  / total_loss : 0.7501 correct : 5537/7556 -> 73.2795%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 106/2000 epochs spend time : 1.4393 sec / total_loss : 0.7656 correct : 21371/29960 -> 71.3318%\n",
            "test dataset : 106/2000 epochs spend time : 0.2827 sec  / total_loss : 0.7124 correct : 5593/7556 -> 74.0206%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 107/2000 epochs spend time : 1.4116 sec / total_loss : 0.7833 correct : 21094/29960 -> 70.4072%\n",
            "test dataset : 107/2000 epochs spend time : 0.2941 sec  / total_loss : 0.7225 correct : 5598/7556 -> 74.0868%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 108/2000 epochs spend time : 1.4010 sec / total_loss : 0.7630 correct : 21645/29960 -> 72.2463%\n",
            "test dataset : 108/2000 epochs spend time : 0.2785 sec  / total_loss : 0.6852 correct : 5681/7556 -> 75.1853%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 109/2000 epochs spend time : 1.4971 sec / total_loss : 0.7850 correct : 21017/29960 -> 70.1502%\n",
            "test dataset : 109/2000 epochs spend time : 0.2830 sec  / total_loss : 0.6792 correct : 5708/7556 -> 75.5426%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 110/2000 epochs spend time : 1.3954 sec / total_loss : 0.7879 correct : 21200/29960 -> 70.7610%\n",
            "test dataset : 110/2000 epochs spend time : 0.2778 sec  / total_loss : 0.6946 correct : 5678/7556 -> 75.1456%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 111/2000 epochs spend time : 1.4200 sec / total_loss : 0.7747 correct : 21286/29960 -> 71.0481%\n",
            "test dataset : 111/2000 epochs spend time : 0.2809 sec  / total_loss : 0.6847 correct : 5660/7556 -> 74.9074%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 112/2000 epochs spend time : 1.4456 sec / total_loss : 0.7562 correct : 21399/29960 -> 71.4252%\n",
            "test dataset : 112/2000 epochs spend time : 0.2885 sec  / total_loss : 0.6752 correct : 5712/7556 -> 75.5956%\n",
            "best epoch : 96/2000 / val accuracy : 75.622022%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 113/2000 epochs spend time : 1.4586 sec / total_loss : 0.7408 correct : 21720/29960 -> 72.4967%\n",
            "test dataset : 113/2000 epochs spend time : 0.2979 sec  / total_loss : 0.6646 correct : 5741/7556 -> 75.9794%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 114/2000 epochs spend time : 1.4787 sec / total_loss : 0.7314 correct : 21787/29960 -> 72.7203%\n",
            "test dataset : 114/2000 epochs spend time : 0.2827 sec  / total_loss : 0.7020 correct : 5608/7556 -> 74.2192%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 115/2000 epochs spend time : 1.4825 sec / total_loss : 0.7335 correct : 21744/29960 -> 72.5768%\n",
            "test dataset : 115/2000 epochs spend time : 0.2822 sec  / total_loss : 0.6980 correct : 5611/7556 -> 74.2589%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 116/2000 epochs spend time : 1.4335 sec / total_loss : 0.7324 correct : 21754/29960 -> 72.6101%\n",
            "test dataset : 116/2000 epochs spend time : 0.2982 sec  / total_loss : 0.7019 correct : 5644/7556 -> 74.6956%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 117/2000 epochs spend time : 1.4686 sec / total_loss : 0.7212 correct : 21975/29960 -> 73.3478%\n",
            "test dataset : 117/2000 epochs spend time : 0.3157 sec  / total_loss : 0.6889 correct : 5671/7556 -> 75.0529%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 118/2000 epochs spend time : 1.4516 sec / total_loss : 0.7399 correct : 21533/29960 -> 71.8725%\n",
            "test dataset : 118/2000 epochs spend time : 0.2760 sec  / total_loss : 0.6652 correct : 5737/7556 -> 75.9264%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 119/2000 epochs spend time : 1.3700 sec / total_loss : 0.7187 correct : 21944/29960 -> 73.2443%\n",
            "test dataset : 119/2000 epochs spend time : 0.2818 sec  / total_loss : 0.6839 correct : 5690/7556 -> 75.3044%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 120/2000 epochs spend time : 1.4413 sec / total_loss : 0.7076 correct : 22059/29960 -> 73.6282%\n",
            "test dataset : 120/2000 epochs spend time : 0.3078 sec  / total_loss : 0.6572 correct : 5736/7556 -> 75.9132%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 121/2000 epochs spend time : 1.4285 sec / total_loss : 0.7101 correct : 22059/29960 -> 73.6282%\n",
            "test dataset : 121/2000 epochs spend time : 0.2953 sec  / total_loss : 0.7648 correct : 5414/7556 -> 71.6517%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 122/2000 epochs spend time : 1.4759 sec / total_loss : 0.7324 correct : 21743/29960 -> 72.5734%\n",
            "test dataset : 122/2000 epochs spend time : 0.2893 sec  / total_loss : 0.7019 correct : 5651/7556 -> 74.7882%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 123/2000 epochs spend time : 1.4344 sec / total_loss : 0.7076 correct : 21985/29960 -> 73.3812%\n",
            "test dataset : 123/2000 epochs spend time : 0.3101 sec  / total_loss : 0.6765 correct : 5654/7556 -> 74.8280%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 124/2000 epochs spend time : 1.4431 sec / total_loss : 0.7202 correct : 21891/29960 -> 73.0674%\n",
            "test dataset : 124/2000 epochs spend time : 0.3039 sec  / total_loss : 0.6828 correct : 5636/7556 -> 74.5897%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 125/2000 epochs spend time : 1.4861 sec / total_loss : 0.7294 correct : 21684/29960 -> 72.3765%\n",
            "test dataset : 125/2000 epochs spend time : 0.2845 sec  / total_loss : 0.6586 correct : 5738/7556 -> 75.9397%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 126/2000 epochs spend time : 1.4364 sec / total_loss : 0.7234 correct : 21836/29960 -> 72.8838%\n",
            "test dataset : 126/2000 epochs spend time : 0.2833 sec  / total_loss : 0.6952 correct : 5648/7556 -> 74.7485%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 127/2000 epochs spend time : 1.3762 sec / total_loss : 0.7057 correct : 21961/29960 -> 73.3011%\n",
            "test dataset : 127/2000 epochs spend time : 0.2973 sec  / total_loss : 0.6965 correct : 5650/7556 -> 74.7750%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 128/2000 epochs spend time : 1.4640 sec / total_loss : 0.7000 correct : 22319/29960 -> 74.4960%\n",
            "test dataset : 128/2000 epochs spend time : 0.2819 sec  / total_loss : 0.6776 correct : 5660/7556 -> 74.9074%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 129/2000 epochs spend time : 1.4645 sec / total_loss : 0.7138 correct : 21917/29960 -> 73.1542%\n",
            "test dataset : 129/2000 epochs spend time : 0.2899 sec  / total_loss : 0.6665 correct : 5693/7556 -> 75.3441%\n",
            "best epoch : 113/2000 / val accuracy : 75.979354%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 130/2000 epochs spend time : 1.4308 sec / total_loss : 0.7061 correct : 22171/29960 -> 74.0020%\n",
            "test dataset : 130/2000 epochs spend time : 0.2824 sec  / total_loss : 0.6507 correct : 5757/7556 -> 76.1911%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 131/2000 epochs spend time : 1.4388 sec / total_loss : 0.7042 correct : 22166/29960 -> 73.9853%\n",
            "test dataset : 131/2000 epochs spend time : 0.2997 sec  / total_loss : 0.6713 correct : 5713/7556 -> 75.6088%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 132/2000 epochs spend time : 1.4734 sec / total_loss : 0.6919 correct : 22305/29960 -> 74.4493%\n",
            "test dataset : 132/2000 epochs spend time : 0.2870 sec  / total_loss : 0.7387 correct : 5496/7556 -> 72.7369%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 133/2000 epochs spend time : 1.3866 sec / total_loss : 0.7137 correct : 22073/29960 -> 73.6749%\n",
            "test dataset : 133/2000 epochs spend time : 0.2894 sec  / total_loss : 0.6756 correct : 5711/7556 -> 75.5823%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 134/2000 epochs spend time : 1.4288 sec / total_loss : 0.6968 correct : 22273/29960 -> 74.3425%\n",
            "test dataset : 134/2000 epochs spend time : 0.3022 sec  / total_loss : 0.6837 correct : 5630/7556 -> 74.5103%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 135/2000 epochs spend time : 1.4896 sec / total_loss : 0.6952 correct : 22176/29960 -> 74.0187%\n",
            "test dataset : 135/2000 epochs spend time : 0.2789 sec  / total_loss : 0.6916 correct : 5645/7556 -> 74.7088%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 136/2000 epochs spend time : 1.4491 sec / total_loss : 0.6932 correct : 22158/29960 -> 73.9586%\n",
            "test dataset : 136/2000 epochs spend time : 0.2919 sec  / total_loss : 0.6509 correct : 5713/7556 -> 75.6088%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 137/2000 epochs spend time : 1.4245 sec / total_loss : 0.7002 correct : 22154/29960 -> 73.9453%\n",
            "test dataset : 137/2000 epochs spend time : 0.2879 sec  / total_loss : 0.6751 correct : 5705/7556 -> 75.5029%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 138/2000 epochs spend time : 1.4873 sec / total_loss : 0.6886 correct : 22341/29960 -> 74.5694%\n",
            "test dataset : 138/2000 epochs spend time : 0.2989 sec  / total_loss : 0.6589 correct : 5747/7556 -> 76.0588%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 139/2000 epochs spend time : 1.4202 sec / total_loss : 0.6917 correct : 22229/29960 -> 74.1956%\n",
            "test dataset : 139/2000 epochs spend time : 0.2813 sec  / total_loss : 0.7289 correct : 5531/7556 -> 73.2001%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 140/2000 epochs spend time : 1.4435 sec / total_loss : 0.6877 correct : 22401/29960 -> 74.7697%\n",
            "test dataset : 140/2000 epochs spend time : 0.2857 sec  / total_loss : 0.6464 correct : 5746/7556 -> 76.0455%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 141/2000 epochs spend time : 1.4719 sec / total_loss : 0.6888 correct : 22244/29960 -> 74.2457%\n",
            "test dataset : 141/2000 epochs spend time : 0.2895 sec  / total_loss : 0.6864 correct : 5644/7556 -> 74.6956%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 142/2000 epochs spend time : 1.4485 sec / total_loss : 0.6778 correct : 22487/29960 -> 75.0567%\n",
            "test dataset : 142/2000 epochs spend time : 0.2804 sec  / total_loss : 0.6577 correct : 5749/7556 -> 76.0852%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 143/2000 epochs spend time : 1.4561 sec / total_loss : 0.6890 correct : 22115/29960 -> 73.8151%\n",
            "test dataset : 143/2000 epochs spend time : 0.2847 sec  / total_loss : 0.7471 correct : 5495/7556 -> 72.7237%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 144/2000 epochs spend time : 1.4181 sec / total_loss : 0.6692 correct : 22528/29960 -> 75.1936%\n",
            "test dataset : 144/2000 epochs spend time : 0.2943 sec  / total_loss : 0.7037 correct : 5605/7556 -> 74.1795%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 145/2000 epochs spend time : 1.4711 sec / total_loss : 0.6863 correct : 22432/29960 -> 74.8732%\n",
            "test dataset : 145/2000 epochs spend time : 0.2938 sec  / total_loss : 0.6718 correct : 5699/7556 -> 75.4235%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 146/2000 epochs spend time : 1.4221 sec / total_loss : 0.6846 correct : 22384/29960 -> 74.7130%\n",
            "test dataset : 146/2000 epochs spend time : 0.2868 sec  / total_loss : 0.6835 correct : 5657/7556 -> 74.8677%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 147/2000 epochs spend time : 1.4802 sec / total_loss : 0.7044 correct : 22163/29960 -> 73.9753%\n",
            "test dataset : 147/2000 epochs spend time : 0.2871 sec  / total_loss : 0.6843 correct : 5695/7556 -> 75.3706%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 148/2000 epochs spend time : 1.4381 sec / total_loss : 0.6719 correct : 22541/29960 -> 75.2370%\n",
            "test dataset : 148/2000 epochs spend time : 0.2822 sec  / total_loss : 0.7027 correct : 5611/7556 -> 74.2589%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 149/2000 epochs spend time : 1.4145 sec / total_loss : 0.6659 correct : 22706/29960 -> 75.7877%\n",
            "test dataset : 149/2000 epochs spend time : 0.2909 sec  / total_loss : 0.6585 correct : 5705/7556 -> 75.5029%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 150/2000 epochs spend time : 1.5038 sec / total_loss : 0.6812 correct : 22377/29960 -> 74.6896%\n",
            "test dataset : 150/2000 epochs spend time : 0.2866 sec  / total_loss : 0.6581 correct : 5706/7556 -> 75.5161%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 151/2000 epochs spend time : 1.4611 sec / total_loss : 0.6751 correct : 22568/29960 -> 75.3271%\n",
            "test dataset : 151/2000 epochs spend time : 0.2982 sec  / total_loss : 0.6768 correct : 5665/7556 -> 74.9735%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 152/2000 epochs spend time : 1.4718 sec / total_loss : 0.6865 correct : 22186/29960 -> 74.0521%\n",
            "test dataset : 152/2000 epochs spend time : 0.2981 sec  / total_loss : 0.6477 correct : 5727/7556 -> 75.7941%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 153/2000 epochs spend time : 1.4509 sec / total_loss : 0.6615 correct : 22654/29960 -> 75.6142%\n",
            "test dataset : 153/2000 epochs spend time : 0.2786 sec  / total_loss : 0.6809 correct : 5668/7556 -> 75.0132%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 154/2000 epochs spend time : 1.4370 sec / total_loss : 0.6601 correct : 22815/29960 -> 76.1515%\n",
            "test dataset : 154/2000 epochs spend time : 0.2883 sec  / total_loss : 0.6684 correct : 5681/7556 -> 75.1853%\n",
            "best epoch : 130/2000 / val accuracy : 76.191106%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 155/2000 epochs spend time : 1.4742 sec / total_loss : 0.6547 correct : 22811/29960 -> 76.1382%\n",
            "test dataset : 155/2000 epochs spend time : 0.2879 sec  / total_loss : 0.6468 correct : 5777/7556 -> 76.4558%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 156/2000 epochs spend time : 1.4976 sec / total_loss : 0.6561 correct : 22693/29960 -> 75.7443%\n",
            "test dataset : 156/2000 epochs spend time : 0.3124 sec  / total_loss : 0.6511 correct : 5757/7556 -> 76.1911%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 157/2000 epochs spend time : 1.4614 sec / total_loss : 0.6531 correct : 22801/29960 -> 76.1048%\n",
            "test dataset : 157/2000 epochs spend time : 0.3066 sec  / total_loss : 0.6590 correct : 5745/7556 -> 76.0323%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 158/2000 epochs spend time : 1.4734 sec / total_loss : 0.6460 correct : 22921/29960 -> 76.5053%\n",
            "test dataset : 158/2000 epochs spend time : 0.3220 sec  / total_loss : 0.6563 correct : 5717/7556 -> 75.6617%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 159/2000 epochs spend time : 1.5126 sec / total_loss : 0.6538 correct : 22699/29960 -> 75.7644%\n",
            "test dataset : 159/2000 epochs spend time : 0.3054 sec  / total_loss : 0.6741 correct : 5710/7556 -> 75.5691%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 160/2000 epochs spend time : 1.4390 sec / total_loss : 0.6510 correct : 22898/29960 -> 76.4286%\n",
            "test dataset : 160/2000 epochs spend time : 0.3128 sec  / total_loss : 0.6535 correct : 5754/7556 -> 76.1514%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 161/2000 epochs spend time : 1.4877 sec / total_loss : 0.6573 correct : 22733/29960 -> 75.8778%\n",
            "test dataset : 161/2000 epochs spend time : 0.2832 sec  / total_loss : 0.6717 correct : 5718/7556 -> 75.6750%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 162/2000 epochs spend time : 1.4395 sec / total_loss : 0.6497 correct : 22828/29960 -> 76.1949%\n",
            "test dataset : 162/2000 epochs spend time : 0.3007 sec  / total_loss : 0.6536 correct : 5760/7556 -> 76.2308%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 163/2000 epochs spend time : 1.4049 sec / total_loss : 0.6442 correct : 22938/29960 -> 76.5621%\n",
            "test dataset : 163/2000 epochs spend time : 0.2950 sec  / total_loss : 0.6570 correct : 5728/7556 -> 75.8073%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 164/2000 epochs spend time : 1.4593 sec / total_loss : 0.6430 correct : 22969/29960 -> 76.6656%\n",
            "test dataset : 164/2000 epochs spend time : 0.2954 sec  / total_loss : 0.6593 correct : 5733/7556 -> 75.8735%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 165/2000 epochs spend time : 1.4492 sec / total_loss : 0.6423 correct : 22873/29960 -> 76.3451%\n",
            "test dataset : 165/2000 epochs spend time : 0.2912 sec  / total_loss : 0.6635 correct : 5721/7556 -> 75.7147%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 166/2000 epochs spend time : 1.4130 sec / total_loss : 0.6350 correct : 22981/29960 -> 76.7056%\n",
            "test dataset : 166/2000 epochs spend time : 0.2989 sec  / total_loss : 0.6568 correct : 5742/7556 -> 75.9926%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 167/2000 epochs spend time : 1.4628 sec / total_loss : 0.6445 correct : 22989/29960 -> 76.7323%\n",
            "test dataset : 167/2000 epochs spend time : 0.2855 sec  / total_loss : 0.6527 correct : 5747/7556 -> 76.0588%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 168/2000 epochs spend time : 1.4712 sec / total_loss : 0.6414 correct : 22924/29960 -> 76.5154%\n",
            "test dataset : 168/2000 epochs spend time : 0.2908 sec  / total_loss : 0.6559 correct : 5748/7556 -> 76.0720%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 169/2000 epochs spend time : 1.4617 sec / total_loss : 0.6395 correct : 22971/29960 -> 76.6722%\n",
            "test dataset : 169/2000 epochs spend time : 0.3013 sec  / total_loss : 0.6641 correct : 5713/7556 -> 75.6088%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 170/2000 epochs spend time : 1.4209 sec / total_loss : 0.6439 correct : 22962/29960 -> 76.6422%\n",
            "test dataset : 170/2000 epochs spend time : 0.2807 sec  / total_loss : 0.6580 correct : 5740/7556 -> 75.9661%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 171/2000 epochs spend time : 1.5004 sec / total_loss : 0.6360 correct : 23003/29960 -> 76.7790%\n",
            "test dataset : 171/2000 epochs spend time : 0.2897 sec  / total_loss : 0.6573 correct : 5733/7556 -> 75.8735%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 172/2000 epochs spend time : 1.4175 sec / total_loss : 0.6418 correct : 22991/29960 -> 76.7390%\n",
            "test dataset : 172/2000 epochs spend time : 0.2874 sec  / total_loss : 0.6535 correct : 5740/7556 -> 75.9661%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 173/2000 epochs spend time : 1.4314 sec / total_loss : 0.6385 correct : 23009/29960 -> 76.7991%\n",
            "test dataset : 173/2000 epochs spend time : 0.2879 sec  / total_loss : 0.6532 correct : 5731/7556 -> 75.8470%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 174/2000 epochs spend time : 1.4928 sec / total_loss : 0.6356 correct : 22958/29960 -> 76.6288%\n",
            "test dataset : 174/2000 epochs spend time : 0.2955 sec  / total_loss : 0.6553 correct : 5735/7556 -> 75.8999%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 175/2000 epochs spend time : 1.4165 sec / total_loss : 0.6395 correct : 23012/29960 -> 76.8091%\n",
            "test dataset : 175/2000 epochs spend time : 0.2864 sec  / total_loss : 0.6608 correct : 5734/7556 -> 75.8867%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 176/2000 epochs spend time : 1.4982 sec / total_loss : 0.6374 correct : 23041/29960 -> 76.9059%\n",
            "test dataset : 176/2000 epochs spend time : 0.2943 sec  / total_loss : 0.6618 correct : 5726/7556 -> 75.7808%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 177/2000 epochs spend time : 1.4604 sec / total_loss : 0.6349 correct : 23074/29960 -> 77.0160%\n",
            "test dataset : 177/2000 epochs spend time : 0.2937 sec  / total_loss : 0.6572 correct : 5734/7556 -> 75.8867%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 178/2000 epochs spend time : 1.4482 sec / total_loss : 0.6347 correct : 23069/29960 -> 76.9993%\n",
            "test dataset : 178/2000 epochs spend time : 0.2968 sec  / total_loss : 0.6553 correct : 5734/7556 -> 75.8867%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 179/2000 epochs spend time : 1.4634 sec / total_loss : 0.6374 correct : 23045/29960 -> 76.9192%\n",
            "test dataset : 179/2000 epochs spend time : 0.3000 sec  / total_loss : 0.6570 correct : 5734/7556 -> 75.8867%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 180/2000 epochs spend time : 1.5555 sec / total_loss : 0.6350 correct : 23047/29960 -> 76.9259%\n",
            "test dataset : 180/2000 epochs spend time : 0.2973 sec  / total_loss : 0.6587 correct : 5731/7556 -> 75.8470%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 181/2000 epochs spend time : 1.4513 sec / total_loss : 0.6333 correct : 23079/29960 -> 77.0327%\n",
            "test dataset : 181/2000 epochs spend time : 0.2971 sec  / total_loss : 0.6590 correct : 5728/7556 -> 75.8073%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 182/2000 epochs spend time : 1.4698 sec / total_loss : 0.6344 correct : 23112/29960 -> 77.1429%\n",
            "test dataset : 182/2000 epochs spend time : 0.3030 sec  / total_loss : 0.6568 correct : 5738/7556 -> 75.9397%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 183/2000 epochs spend time : 1.5037 sec / total_loss : 0.6330 correct : 23086/29960 -> 77.0561%\n",
            "test dataset : 183/2000 epochs spend time : 0.3052 sec  / total_loss : 0.6556 correct : 5732/7556 -> 75.8602%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 184/2000 epochs spend time : 1.5251 sec / total_loss : 0.6345 correct : 23062/29960 -> 76.9760%\n",
            "test dataset : 184/2000 epochs spend time : 0.3181 sec  / total_loss : 0.6565 correct : 5736/7556 -> 75.9132%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 185/2000 epochs spend time : 1.4510 sec / total_loss : 0.6310 correct : 23111/29960 -> 77.1395%\n",
            "test dataset : 185/2000 epochs spend time : 0.2992 sec  / total_loss : 0.6575 correct : 5735/7556 -> 75.8999%\n",
            "best epoch : 155/2000 / val accuracy : 76.455797%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 186/2000 epochs spend time : 1.4487 sec / total_loss : 0.6284 correct : 23076/29960 -> 77.0227%\n",
            "test dataset : 186/2000 epochs spend time : 0.2928 sec  / total_loss : 0.6587 correct : 5726/7556 -> 75.7808%\n",
            "Early Stopping\n",
            "best epoch : 155/2000 / accuracy : 76.455797%\n",
            "==============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCE73qoRBUC2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}