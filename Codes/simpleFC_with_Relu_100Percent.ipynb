{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SleepDataPreparation_and_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/SleepCapstone/blob/master/Codes/simpleFC_with_Relu_100Percent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAGJRSb3xOT1"
      },
      "source": [
        "한림대학교 소프트웨어 융합대학\n",
        "빅데이터 캡스톤 프로젝트\n",
        "\n",
        "딥러닝에 기반한 수면 분류 모델 개발"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SaQVdPHgfmx",
        "outputId": "8b83df10-640c-43c0-f9b0-7858bc088073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohoEZomKw0cz",
        "outputId": "63fc7732-8abe-4dc9-ecc4-7a581bb0fc62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd drive/My\\ Drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvna-5vRzmTQ",
        "outputId": "fbd64331-e409-4096-dbb7-be8140bddd6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "# GPU 가용성 체크\n",
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Oct 15 01:33:50 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1zNHBNc4beq"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def search_signals_npy(dirname):\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for file in filenames if file.endswith(\".npy\")]\n",
        "    return filenames\n",
        "\n",
        "def search_correct_signals_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vSGY6sO9vgC",
        "outputId": "cf10f38f-389d-457e-c242-05a6c3679bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "\n",
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  'annotations/npy/remove_wake/'\n",
        "signals_path = 'signals/npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "total_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list:\n",
        "    label = np.load(path + filename)\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    \n",
        "    signals = np.load(signals_path+signals_filename)\n",
        "    \n",
        "    #print('remove start index : %d / remove end index : %d'%(remove_start_index,remove_end_index))\n",
        "    #print(np.bincount(label,minlength=6))\n",
        "    if len(label) !=len(signals[0])//30//fs:\n",
        "        print('file is fault!!!')\n",
        "    for i in range(6):\n",
        "        total_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "print(total_label)\n",
        "\n",
        "x = np.arange(len(total_label))\n",
        "\n",
        "plt.bar(x,total_label,width=0.7)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4336  2804 17799  5703  7717    61]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 6 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASZklEQVR4nO3df4xd9Xnn8fdnTUgjEgQJU8u1Ye2mDhJBXScZEaT8ULZswEAUk6qittTgpmycKCAlSqXWdP8gmxSJdptmFSnLymksjJrg0BKEFZwSh6KiSHXwOHEBQygDMWIsx57GSWk2K7Imz/5xv9Oemhl7PPd67th+v6SjOec533Puc4TwZ86PeyZVhSTpzPYfht2AJGn4DANJkmEgSTIMJEkYBpIk4KxhNzBXF1xwQS1fvnzYbUjSKWX37t3/VFUjR9dP2TBYvnw5Y2Njw25Dkk4pSZ6fru5lIkmSYSBJMgwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkMYtvICfZDLwPOFRVl7baV4GL25DzgJ9U1aoky4GngKfbup1V9dG2zduAO4HXANuBj1dVJXk98FVgObAPuL6qfjyAY9NpYPnGB4bdwivsu/3aYbcgDdxszgzuBFZ3C1X121W1qqpWAfcCX+usfnZq3VQQNHcAHwZWtmlqnxuBh6pqJfBQW5YkzaPjhkFVPQIcnm5dkgDXA3cfax9JlgDnVtXO6v2dzbuA69rqNcCWNr+lU5ckzZN+7xm8CzhYVc90aiuSfC/J3yV5V6stBSY6YyZaDWBxVR1o8z8EFs/0YUk2JBlLMjY5Odln65KkKf2GwTr+/VnBAeCiqnoL8EngK0nOne3O2llDHWP9pqoararRkZFXvIFVkjRHc36FdZKzgN8E3jZVq6qXgJfa/O4kzwJvAvYDyzqbL2s1gINJllTVgXY56dBce5IkzU0/Zwb/Bfh+Vf3r5Z8kI0kWtflfpXej+Ll2GejFJJe3+ww3APe3zbYB69v8+k5dkjRPjhsGSe4G/h64OMlEkhvbqrW88sbxu4HHkuwB/hr4aFVN3Xz+GPAXwDjwLPCNVr8deG+SZ+gFzO19HI8kaQ6Oe5moqtbNUP/daWr30nvUdLrxY8Cl09R/BFxxvD4kSSeP30CWJBkGkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEliFmGQZHOSQ0me6NQ+lWR/kj1tuqaz7pYk40meTnJVp7661caTbOzUVyT5Tqt/NcnZgzxASdLxzebM4E5g9TT1z1XVqjZtB0hyCbAWeHPb5n8lWZRkEfAF4GrgEmBdGwvwJ21fvwb8GLixnwOSJJ2444ZBVT0CHJ7l/tYAW6vqpar6ATAOXNam8ap6rqp+DmwF1iQJ8BvAX7fttwDXneAxSJL61M89g5uTPNYuI53fakuBFzpjJlptpvobgJ9U1ZGj6tNKsiHJWJKxycnJPlqXJHXNNQzuAN4IrAIOAJ8dWEfHUFWbqmq0qkZHRkbm4yMl6Yxw1lw2qqqDU/NJvgh8vS3uBy7sDF3WasxQ/xFwXpKz2tlBd7wkaZ7M6cwgyZLO4geAqSeNtgFrk7w6yQpgJfAosAtY2Z4cOpveTeZtVVXAw8Bvte3XA/fPpSdJ0twd98wgyd3Ae4ALkkwAtwLvSbIKKGAf8BGAqtqb5B7gSeAIcFNVvdz2czPwILAI2FxVe9tH/CGwNckfA98DvjSwo5Mkzcpxw6Cq1k1TnvEf7Kq6Dbhtmvp2YPs09efoPW0kSRoSv4EsSTIMJEmGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJLELMIgyeYkh5I80an9jyTfT/JYkvuSnNfqy5P83yR72vS/O9u8LcnjScaTfD5JWv31SXYkeab9PP9kHKgkaWazOTO4E1h9VG0HcGlV/Trwj8AtnXXPVtWqNn20U78D+DCwsk1T+9wIPFRVK4GH2rIkaR4dNwyq6hHg8FG1b1bVkba4E1h2rH0kWQKcW1U7q6qAu4Dr2uo1wJY2v6VTlyTNk0HcM/g94Bud5RVJvpfk75K8q9WWAhOdMROtBrC4qg60+R8Ci2f6oCQbkowlGZucnBxA65Ik6DMMkvw34Ajw5VY6AFxUVW8BPgl8Jcm5s91fO2uoY6zfVFWjVTU6MjLSR+eSpK6z5rphkt8F3gdc0f4Rp6peAl5q87uTPAu8CdjPv7+UtKzVAA4mWVJVB9rlpENz7UmSNDdzOjNIshr4A+D9VfWzTn0kyaI2/6v0bhQ/1y4DvZjk8vYU0Q3A/W2zbcD6Nr++U5ckzZPjnhkkuRt4D3BBkgngVnpPD70a2NGeEN3Znhx6N/DpJP8P+AXw0aqauvn8MXpPJr2G3j2GqfsMtwP3JLkReB64fiBHJkmateOGQVWtm6b8pRnG3gvcO8O6MeDSaeo/Aq44Xh+SpJPHbyBLkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCQxyzBIsjnJoSRPdGqvT7IjyTPt5/mtniSfTzKe5LEkb+1ss76NfybJ+k79bUkeb9t8PkkGeZCSpGOb7ZnBncDqo2obgYeqaiXwUFsGuBpY2aYNwB3QCw/gVuDtwGXArVMB0sZ8uLPd0Z8lSTqJZhUGVfUIcPio8hpgS5vfAlzXqd9VPTuB85IsAa4CdlTV4ar6MbADWN3WnVtVO6uqgLs6+5IkzYN+7hksrqoDbf6HwOI2vxR4oTNuotWOVZ+Ypv4KSTYkGUsyNjk52UfrkqSugdxAbr/R1yD2dZzP2VRVo1U1OjIycrI/TpLOGP2EwcF2iYf281Cr7wcu7Ixb1mrHqi+bpi5Jmif9hME2YOqJoPXA/Z36De2posuBf26Xkx4ErkxyfrtxfCXwYFv3YpLL21NEN3T2JUmaB2fNZlCSu4H3ABckmaD3VNDtwD1JbgSeB65vw7cD1wDjwM+ADwFU1eEknwF2tXGfrqqpm9Ifo/fE0muAb7RJkjRPZhUGVbVuhlVXTDO2gJtm2M9mYPM09THg0tn0IkkaPL+BLEkyDCRJhoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CSRB9hkOTiJHs604tJPpHkU0n2d+rXdLa5Jcl4kqeTXNWpr2618SQb+z0oSdKJOWuuG1bV08AqgCSLgP3AfcCHgM9V1Z91xye5BFgLvBn4FeBbSd7UVn8BeC8wAexKsq2qnpxrb5Lm3/KNDwy7hVfYd/u1w27hlDHnMDjKFcCzVfV8kpnGrAG2VtVLwA+SjAOXtXXjVfUcQJKtbaxhIEnzZFD3DNYCd3eWb07yWJLNSc5vtaXAC50xE602U12SNE/6DoMkZwPvB/6qle4A3kjvEtIB4LP9fkbnszYkGUsyNjk5OajdStIZbxBnBlcD362qgwBVdbCqXq6qXwBf5N8uBe0HLuxst6zVZqq/QlVtqqrRqhodGRkZQOuSJBhMGKyjc4koyZLOug8AT7T5bcDaJK9OsgJYCTwK7AJWJlnRzjLWtrGSpHnS1w3kJOfQewroI53ynyZZBRSwb2pdVe1Ncg+9G8NHgJuq6uW2n5uBB4FFwOaq2ttPX5KkE9NXGFTV/wHecFTtg8cYfxtw2zT17cD2fnqRJM2d30CWJBkGkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSfT59wwknbjlGx8YdgvT2nf7tcNuQUPkmYEkyTCQJBkGkiQMA0kSAwiDJPuSPJ5kT5KxVnt9kh1Jnmk/z2/1JPl8kvEkjyV5a2c/69v4Z5Ks77cvSdLsDerM4D9X1aqqGm3LG4GHqmol8FBbBrgaWNmmDcAd0AsP4Fbg7cBlwK1TASJJOvlO1mWiNcCWNr8FuK5Tv6t6dgLnJVkCXAXsqKrDVfVjYAew+iT1Jkk6yiDCoIBvJtmdZEOrLa6qA23+h8DiNr8UeKGz7USrzVSXJM2DQXzp7J1VtT/JLwM7kny/u7KqKkkN4HNoYbMB4KKLLhrELiVJDODMoKr2t5+HgPvoXfM/2C7/0H4easP3Axd2Nl/WajPVj/6sTVU1WlWjIyMj/bYuSWr6CoMk5yR53dQ8cCXwBLANmHoiaD1wf5vfBtzQniq6HPjndjnpQeDKJOe3G8dXtpokaR70e5loMXBfkql9faWq/ibJLuCeJDcCzwPXt/HbgWuAceBnwIcAqupwks8Au9q4T1fV4T57kyTNUl9hUFXPAf9pmvqPgCumqRdw0wz72gxs7qcfSdLc+A1kSZJhIEk6Q/+ewUJ8n7zvkpc0TJ4ZSJIMA0mSYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkiTP0L52djhbiX28D/4KbdKqY85lBkguTPJzkySR7k3y81T+VZH+SPW26prPNLUnGkzyd5KpOfXWrjSfZ2N8hSZJOVD9nBkeA36+q7yZ5HbA7yY627nNV9WfdwUkuAdYCbwZ+BfhWkje11V8A3gtMALuSbKuqJ/voTZJ0AuYcBlV1ADjQ5v8lyVPA0mNssgbYWlUvAT9IMg5c1taNV9VzAEm2trGGgSTNk4HcQE6yHHgL8J1WujnJY0k2Jzm/1ZYCL3Q2m2i1merTfc6GJGNJxiYnJwfRuiSJAYRBktcC9wKfqKoXgTuANwKr6J05fLbfz5hSVZuqarSqRkdGRga1W0k64/X1NFGSV9ELgi9X1dcAqupgZ/0Xga+3xf3AhZ3Nl7Uax6hLkuZBP08TBfgS8FRV/XmnvqQz7APAE21+G7A2yauTrABWAo8Cu4CVSVYkOZveTeZtc+1LknTi+jkzeAfwQeDxJHta7Y+AdUlWAQXsAz4CUFV7k9xD78bwEeCmqnoZIMnNwIPAImBzVe3toy9J0gnq52mibwOZZtX2Y2xzG3DbNPXtx9pOknRy+ToKSZJhIEkyDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkFlAYJFmd5Okk40k2DrsfSTqTLIgwSLII+AJwNXAJsC7JJcPtSpLOHGcNu4HmMmC8qp4DSLIVWAM8OdSuJJ3Rlm98YNgtvMK+2689KftNVZ2UHZ9QE8lvAaur6r+25Q8Cb6+qm48atwHY0BYvBp6e10andwHwT8NuYsBOx2OC0/O4PKZTx0I5rv9YVSNHFxfKmcGsVNUmYNOw++hKMlZVo8PuY5BOx2OC0/O4PKZTx0I/rgVxzwDYD1zYWV7WapKkebBQwmAXsDLJiiRnA2uBbUPuSZLOGAviMlFVHUlyM/AgsAjYXFV7h9zWbC2oy1YDcjoeE5yex+UxnToW9HEtiBvIkqThWiiXiSRJQ2QYSJIMg7k6HV+fkWRzkkNJnhh2L4OS5MIkDyd5MsneJB8fdk+DkOSXkjya5B/acf33Yfc0KEkWJflekq8Pu5dBSLIvyeNJ9iQZG3Y/M/GewRy012f8I/BeYILe01DrquqU/sZ0kncDPwXuqqpLh93PICRZAiypqu8meR2wG7juNPhvFeCcqvppklcB3wY+XlU7h9xa35J8EhgFzq2q9w27n34l2QeMVtVC+MLZjDwzmJt/fX1GVf0cmHp9ximtqh4BDg+7j0GqqgNV9d02/y/AU8DS4XbVv+r5aVt8VZtO+d/skiwDrgX+Yti9nGkMg7lZCrzQWZ7gNPgH5nSXZDnwFuA7w+1kMNrllD3AIWBHVZ0Ox/U/gT8AfjHsRgaogG8m2d1eqbMgGQY6IyR5LXAv8ImqenHY/QxCVb1cVavofWP/siSn9KW9JO8DDlXV7mH3MmDvrKq30nsr803tcuyCYxjMja/POIW0a+r3Al+uqq8Nu59Bq6qfAA8Dq4fdS5/eAby/XWPfCvxGkr8cbkv9q6r97ech4D56l5kXHMNgbnx9ximi3Wj9EvBUVf35sPsZlCQjSc5r86+h9zDD94fbVX+q6paqWlZVy+n9P/W3VfU7Q26rL0nOaQ8ukOQc4EpgQT6tZxjMQVUdAaZen/EUcM8p9PqMGSW5G/h74OIkE0luHHZPA/AO4IP0fsvc06Zrht3UACwBHk7yGL1fTnZU1WnxKOZpZjHw7ST/ADwKPFBVfzPknqblo6WSJM8MJEmGgSQJw0CShGEgScIwkCRhGEiSMAwkScD/B3iUiRdmsawFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVVZuGLx-ADW",
        "outputId": "ba438cfd-b069-4247-84bb-bd061b79493e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  'annotations/npy/remove_wake/'\n",
        "signals_path = 'signals/npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "random.shuffle(annotations_npy_list)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "trainDataset_count = 30\n",
        "testDataset_count = len(annotations_npy_list)-trainDataset_count\n",
        "\n",
        "print(trainDataset_count)\n",
        "print(testDataset_count)\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4001EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4192EV-Hypnogram.npy']\n",
            "['SC4181EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy']\n",
            "30\n",
            "9\n",
            "[12.37256261  7.42683691 45.03282853 14.88996668 20.10623907  0.1715662 ]\n",
            "[ 7.2247565   6.81790162 51.16508445 14.67143386 20.00986315  0.11096042]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAKH9Mb0_NF3"
      },
      "source": [
        "signals_path = 'signals/npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "save_train_path = 'signals/npy//Fpz-Cz/remove_wake/train/'\n",
        "save_test_path = 'signals/npy/Fpz-Cz/remove_wake/test/'\n",
        "\n",
        "os.makedirs(save_train_path,exist_ok=True)\n",
        "os.makedirs(save_test_path,exist_ok=True)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_train_path+filename)\n",
        "    \n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_test_path+filename)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4EYtlUH-e9f",
        "outputId": "a6c20f12-3c79-41e4-8d05-74710d29eaa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "def search_correct_annotations_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename\n",
        "\n",
        "train_path =  'signals/npy/Fpz-Cz/remove_wake/train/'\n",
        "test_path = 'signals/npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_path = 'annotations/npy/remove_wake/'\n",
        "\n",
        "train_list = search_signals_npy(train_path)\n",
        "test_list = search_signals_npy(test_path)\n",
        "\n",
        "print(train_list)\n",
        "print(test_list)\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in train_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in test_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4101EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy']\n",
            "['SC4191EP-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy']\n",
            "[11.28578865  7.29828214 46.32743363 14.84383134 20.08589276  0.15877147]\n",
            "[10.33601697  6.73962301 48.59952755 13.12732006 20.97092995  0.22658246]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBwqVERu_aw-"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pylab as plt\n",
        "from  torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSQaEXQr_h8m"
      },
      "source": [
        "def data_preprocessing_torch(signals): # 하나의 데이터셋에 대한 data_preprocessing (using torch)\n",
        "    signals = (signals - signals.mean(dim=1).unsqueeze(1))/signals.std(dim=1).unsqueeze(1)\n",
        "\n",
        "    return signals\n",
        "\n",
        "def data_preprocessing_oneToOne_torch(signals,min,max,max_value):\n",
        "    signals_std = (signals + max_value) / (2*max_value)\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def data_preprocessing_minmax_torch(signals,min,max):\n",
        "    signals_std = (signals - signals.min(dim=1).unsqueeze(1)) / (\n",
        "            signals.max(dim=1).unsqueeze(1) - signals.min(dim=1).unsqueeze(1))\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def get_dataset_one_channel_norm_withoutCut(dirname,annotations_dir,data_path,use_noise=True,epsilon=0.5,noise_scale=2e-6,preprocessing=True,norm_methods='Standard'):\n",
        "    # npy read!\n",
        "    path = dirname + data_path\n",
        "    signals = np.load(path)\n",
        "\n",
        "    signals = torch.from_numpy(signals).float().to(device)\n",
        "\n",
        "    if use_noise:\n",
        "        if np.random.rand() < epsilon:\n",
        "            # noise = np.random.normal(loc=0,scale=noise_scale,size=signals.shape)\n",
        "            # signals = signals + noise\n",
        "            noise = torch.normal(mean=0., std=noise_scale, size=signals.shape).to(device)\n",
        "            signals = signals + noise\n",
        "\n",
        "    if preprocessing:\n",
        "        if norm_methods == 'Standard':\n",
        "            signals = data_preprocessing_torch(signals)\n",
        "        elif norm_methods == 'minmax':\n",
        "            signals = data_preprocessing_minmax_torch(signals,0,1)\n",
        "        elif norm_methods == 'oneToOne':\n",
        "            signals = data_preprocessing_oneToOne_torch(signals,-1,1,1e-4)\n",
        "    label = get_annotations(annotations_dir, data_path)\n",
        "    return signals, label\n",
        "\n",
        "# model conv layer weight init function\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:         # Conv weight init\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "        \n",
        "def suffle_dataset_list(dataset_list): # 데이터 셔플\n",
        "    random.shuffle(dataset_list)\n",
        "    return dataset_list\n",
        "\n",
        "\n",
        "# npy파일을 통해 label을 가져오는 함수\n",
        "def get_annotations(label_dir,file_name):\n",
        "    label_path = label_dir + file_name\n",
        "    label = np.load(label_path)\n",
        "    return label\n",
        "\n",
        "def signals_expand_torch_one_channel(signals): # 2차원 데이터를 3차원으로 변환 (8,N) -> (batch,8,6000) 형태로\n",
        "    signals = signals.expand(1,1,-1)\n",
        "    #print(signals.shape)\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    signals = signals.reshape(-1, 3000, 1) # 형태 변환\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    return signals"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx4M7Rfq_lwo"
      },
      "source": [
        "class DeepSleepNet_Classification(nn.Module):  # input channel = 8channel / output = 5\n",
        "    def __init__(self,in_channel=1,out_channel=6,layer=[64,128,128,128],sample_rate = 100):\n",
        "        super(DeepSleepNet_Classification, self).__init__()\n",
        "\n",
        "        \n",
        "        self.fc1 = nn.Linear(3000,1024)\n",
        "        self.fc2 = nn.Linear(1024,1024)\n",
        "        self.fc3 = nn.Linear(1024,512)\n",
        "        self.fc4 = nn.Linear(512, 256)\n",
        "        self.fc5 = nn.Linear(256, out_channel)\n",
        "\n",
        "        self.ReLU = nn.ReLU()\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "        # print(\"feature_extract_2d.shape : \", feature_extract_2d.shape)\n",
        "        # 여기서 문제 발생 weight의 경우에는 [64 , 32 , 100] 이지만 input 이 2차원 [32, 750]이라 문제 발생!\n",
        "        out = torch.flatten(input, 1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.fc4(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.fc5(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYZ6YSZHArdJ",
        "outputId": "27c78b19-d81d-4cfb-a14c-3673e7734060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "model = DeepSleepNet_Classification(in_channel=1,out_channel=6)\n",
        "summary(model.cuda(),(1,3000))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 1024]       3,073,024\n",
            "              ReLU-2                 [-1, 1024]               0\n",
            "            Linear-3                 [-1, 1024]       1,049,600\n",
            "              ReLU-4                 [-1, 1024]               0\n",
            "            Linear-5                  [-1, 512]         524,800\n",
            "              ReLU-6                  [-1, 512]               0\n",
            "            Linear-7                  [-1, 256]         131,328\n",
            "              ReLU-8                  [-1, 256]               0\n",
            "            Linear-9                    [-1, 6]           1,542\n",
            "================================================================\n",
            "Total params: 4,780,294\n",
            "Trainable params: 4,780,294\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.04\n",
            "Params size (MB): 18.24\n",
            "Estimated Total Size (MB): 18.29\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li3ftAC1Az0Q"
      },
      "source": [
        "def search_npy_list(dirname):  # 매개변수 dir에서 모든 npy파일을 찾고 fold에 따른 dataset 나누기\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for _, file in enumerate(filenames) if file.endswith(\".npy\")]\n",
        "    return filenames\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQH_VRRFBGF1"
      },
      "source": [
        "def train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, test_signal_dir,annotations_dir\n",
        "                               ,epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                               layer_filters=[64,128,256,512],first_conv=[200,40,100],optim='Adam',lf='CE',\n",
        "                               epsilon=0.7,noise_scale=2e-6,min_value=-1e-4,max_value=1e-4,preprocessing=True,\n",
        "                               norm_methods='Standard',use_noise=True,loss_type='softmax'):\n",
        "    # Adam optimizer param\n",
        "    b1 = 0.5\n",
        "    b2 = 0.999\n",
        "\n",
        "    beta = 0.001\n",
        "\n",
        "    check_file = open(logging_filename, 'w')  # logging file\n",
        "\n",
        "    print('Preproceesing  : ',preprocessing)\n",
        "    print('min/max value : %f/%f'%(min_value,max_value))\n",
        "    print('noise scale : ',noise_scale)\n",
        "    print('loss function : ',lf)\n",
        "    print('epsilon : ',epsilon)\n",
        "    print('norm methods : ',norm_methods)\n",
        "\n",
        "    print('logging file name : ', logging_filename)\n",
        "    print('save file name : ', save_filename)\n",
        "    print('layer filters : ',layer_filters)\n",
        "    print('fisrt_conv info : ',first_conv)\n",
        "    print('loss type : ',loss_type)\n",
        "    print('training data oversampling noise : ',use_noise)\n",
        "    best_accuracy = 0.\n",
        "    best_epoch = 0\n",
        "\n",
        "    train_dataset_list = search_npy_list(train_signal_dir)\n",
        "    test_dataset_list = search_npy_list(test_signal_dir)\n",
        "\n",
        "    train_dataset_len = len(train_dataset_list)\n",
        "    test_dataset_len = len(test_dataset_list)\n",
        "\n",
        "    print('train_dataset length : ', len(train_dataset_list))\n",
        "    print(train_dataset_list)\n",
        "\n",
        "    print('test_dataset length : ',test_dataset_len)\n",
        "    print(test_dataset_list)\n",
        "\n",
        "\n",
        "    model = DeepSleepNet_Classification()\n",
        "\n",
        "    #model = resnet18_200hz(in_channel=1,layer_filters=layer_filters,first_conv=first_conv,use_batchnorm=True,num_classes=5)\n",
        "\n",
        "    model.apply(weights_init) # weight init\n",
        "\n",
        "    cuda = torch.cuda.is_available()\n",
        "\n",
        "    if cuda:\n",
        "        print('can use CUDA!!!')\n",
        "        model = model.cuda()\n",
        "    #summary(model,[1,6000])\n",
        "    print('torch.cuda.device_count() : ', torch.cuda.device_count())\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print('Multi GPU Activation !!!')\n",
        "        #model = nn.DataParallel(model)\n",
        "\n",
        "    # loss funcition\n",
        "    if lf == 'CE':\n",
        "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "    elif lf == 'CEW':\n",
        "        samples_per_cls = [27,15,41,5,11]\n",
        "        no_of_classes = 5\n",
        "        effective_num = 1.0 - np.power(beta,samples_per_cls)\n",
        "        #print(effective_num)\n",
        "        weights = (1.0 - beta) / np.array(effective_num)\n",
        "        #print(weights)\n",
        "        weights = weights / np.sum(weights) * no_of_classes\n",
        "        weights = torch.tensor(weights).float()\n",
        "        weights = weights.to(device)\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=weights).to(device)\n",
        "    elif lf == 'FL':\n",
        "        loss_fn = FocalLoss(gamma=2).to(device)\n",
        "    elif lf == 'CBL':\n",
        "        loss_fn = CB_loss(samples_per_cls=[27,15,41,5,11],no_of_classes=5,loss_type=loss_type,beta=0.9999,gamma=2.0)\n",
        "    #loss_fn = FocalLoss(gamma=2).to(device)\n",
        "\n",
        "    # optimizer ADAM (SGD의 경우에는 정상적으로 학습이 진행되지 않았음)\n",
        "    if optim == 'Adam':\n",
        "        print('Optimizer : Adam')\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "    elif optim == 'RMS':\n",
        "        print('Optimizer : RMSprop')\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "    elif optim == 'SGD':\n",
        "        print('Optimizer : SGD')\n",
        "        optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=gamma, patience=10,\n",
        "                                                           min_lr=1e-6)\n",
        "    #stride = 40 일 때, batch_size = 20이면 16GB정도의 메모리 사용\n",
        "    batch_size = 5\n",
        "    norm_square = 2\n",
        "\n",
        "    train_batch_size = math.ceil(train_dataset_len / batch_size)\n",
        "    print('train_batch_size : ',train_batch_size)\n",
        "\n",
        "    test_batch_size = test_dataset_len\n",
        "\n",
        "    best_accuracy = 0.\n",
        "    stop_count = 0\n",
        "    for epoch in range(epochs):\n",
        "        train_dataset = suffle_dataset_list(train_dataset_list) # 매 epoch마다 train_dataset shuffle !\n",
        "        count = 0  # check batch\n",
        "        train_total_loss = 0.0\n",
        "        train_total_count = 0\n",
        "        train_total_data = 0\n",
        "\n",
        "        val_total_loss = 0.0\n",
        "        val_total_count = 0\n",
        "        val_total_data = 0\n",
        "\n",
        "        test_total_loss = 0.0\n",
        "        test_total_count = 0\n",
        "        test_total_data = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "\n",
        "        output_str = 'current_lr : %f\\n'%(optimizer.state_dict()['param_groups'][0]['lr'])\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "        for index, file_name in enumerate(train_dataset):\n",
        "            #print('index : ',index)\n",
        "            if index % batch_size == 0:\n",
        "                batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "            else:\n",
        "                new_signal, new_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "                batch_signal = torch.cat((batch_signal, new_signal),dim=1)\n",
        "                batch_label = np.concatenate((batch_label, new_label))\n",
        "            count += 1\n",
        "            if count == batch_size or index == len(train_dataset) - 1:  # batch 학습 시작!\n",
        "                batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                #batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                # batch_signal = torch.from_numpy(batch_signal).float().to(device)\n",
        "                batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "                optimizer.zero_grad()\n",
        "                # print(batch_signal.shape)\n",
        "                # print(batch_signal)\n",
        "                pred = model(batch_signal)\n",
        "                norm = 0\n",
        "\n",
        "                for parameter in model.parameters():\n",
        "                    norm += torch.norm(parameter, p=norm_square)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label) + beta * norm\n",
        "                #print('loss : ',loss.item())\n",
        "                # loss = loss_fn(pred, batch_label)\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                train_total_loss += loss.item()\n",
        "\n",
        "                train_total_count += check_count\n",
        "                train_total_data += len(batch_signal)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "                count = 0\n",
        "\n",
        "        train_total_loss /= train_batch_size\n",
        "        train_accuracy = train_total_count / train_total_data * 100\n",
        "\n",
        "        output_str = 'train dataset : %d/%d epochs spend time : %.4f sec / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, train_total_loss,\n",
        "                        train_total_count, train_total_data, train_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "    \n",
        "\n",
        "        #check test dataset\n",
        "        start_time = time.time()\n",
        "        for file_name in test_dataset_list:\n",
        "            batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(test_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=False,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "            batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "\n",
        "            batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred = model(batch_signal)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label)\n",
        "\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                test_total_loss += loss.item()\n",
        "                test_total_count += check_count\n",
        "                test_total_data += len(batch_signal)\n",
        "\n",
        "                # 사용하지 않는 변수 제거\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "        test_total_loss /= test_batch_size\n",
        "        test_accuracy = test_total_count / test_total_data * 100\n",
        "\n",
        "\n",
        "\n",
        "        output_str = 'test dataset : %d/%d epochs spend time : %.4f sec  / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, test_total_loss,\n",
        "                        test_total_count, test_total_data, test_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "        scheduler.step(float(test_total_loss))\n",
        "        #scheduler.step()\n",
        "\n",
        "        if epoch == 0:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_epoch = epoch\n",
        "            save_file = save_filename\n",
        "            #save_file = save_path + 'best_SleepEEGNet_CNN_channel%d.pth'%channel\n",
        "            torch.save(model.state_dict(),save_file)\n",
        "            stop_count = 0\n",
        "        else:\n",
        "            if best_accuracy < test_accuracy:\n",
        "                best_accuracy = test_accuracy\n",
        "                best_epoch = epoch\n",
        "                save_file = save_filename\n",
        "                torch.save(model.state_dict(), save_file)\n",
        "                stop_count = 0\n",
        "            else:\n",
        "                stop_count += 1\n",
        "        if stop_count > 30:\n",
        "            print('Early Stopping')\n",
        "            break\n",
        "\n",
        "        output_str = 'best epoch : %d/%d / val accuracy : %f%%\\n' \\\n",
        "                     % (best_epoch+1, epochs, best_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        print('=' * 30)\n",
        "\n",
        "\n",
        "    output_str = 'best epoch : %d/%d / accuracy : %f%%\\n' \\\n",
        "                 % (best_epoch+1, epochs, best_accuracy)\n",
        "    sys.stdout.write(output_str)\n",
        "    check_file.write(output_str)\n",
        "    print('=' * 30)\n",
        "\n",
        "    check_file.close()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-qN1b_NBMC1",
        "outputId": "c2e8e9c5-016b-4245-e0f6-f8b7ed2e84ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "save_filename = './train.pth'\n",
        "logging_filename = './logging.txt'\n",
        "train_signal_dir = 'signals/npy/Fpz-Cz/remove_wake/train/'\n",
        "test_signal_dir = 'signals/npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_dir = 'annotations/npy/remove_wake/'\n",
        "train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, \n",
        "                           test_signal_dir,annotations_dir,\n",
        "                           epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                           layer_filters=[64,128,256,512],first_conv=[200,40,100],\n",
        "                           optim='Adam',lf='CE',epsilon=0,noise_scale=0,\n",
        "                           min_value=-0,max_value=0,preprocessing=True,\n",
        "                           norm_methods='Standard',use_noise=False,loss_type='softmax')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preproceesing  :  True\n",
            "min/max value : 0.000000/0.000000\n",
            "noise scale :  0\n",
            "loss function :  CE\n",
            "epsilon :  0\n",
            "norm methods :  Standard\n",
            "logging file name :  ./logging.txt\n",
            "save file name :  ./train.pth\n",
            "layer filters :  [64, 128, 256, 512]\n",
            "fisrt_conv info :  [200, 40, 100]\n",
            "loss type :  softmax\n",
            "training data oversampling noise :  False\n",
            "train_dataset length :  39\n",
            "['SC4101EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy']\n",
            "test_dataset length :  21\n",
            "['SC4191EP-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy']\n",
            "can use CUDA!!!\n",
            "torch.cuda.device_count() :  1\n",
            "Optimizer : Adam\n",
            "train_batch_size :  8\n",
            "current_lr : 0.001000\n",
            "train dataset : 1/2000 epochs spend time : 5.0966 sec / total_loss : 1.6512 correct : 15943/38420 -> 41.4966%\n",
            "test dataset : 1/2000 epochs spend time : 4.9856 sec  / total_loss : 1.4015 correct : 10081/20743 -> 48.5995%\n",
            "best epoch : 1/2000 / val accuracy : 48.599528%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 2/2000 epochs spend time : 1.7563 sec / total_loss : 1.4651 correct : 17665/38420 -> 45.9787%\n",
            "test dataset : 2/2000 epochs spend time : 0.8263 sec  / total_loss : 1.3705 correct : 10400/20743 -> 50.1374%\n",
            "best epoch : 2/2000 / val accuracy : 50.137396%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 3/2000 epochs spend time : 1.8444 sec / total_loss : 1.4207 correct : 17252/38420 -> 44.9037%\n",
            "test dataset : 3/2000 epochs spend time : 0.8367 sec  / total_loss : 1.2854 correct : 10117/20743 -> 48.7731%\n",
            "best epoch : 2/2000 / val accuracy : 50.137396%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 4/2000 epochs spend time : 1.6856 sec / total_loss : 1.3451 correct : 17842/38420 -> 46.4394%\n",
            "test dataset : 4/2000 epochs spend time : 0.8559 sec  / total_loss : 1.3808 correct : 10265/20743 -> 49.4866%\n",
            "best epoch : 2/2000 / val accuracy : 50.137396%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 5/2000 epochs spend time : 1.7058 sec / total_loss : 1.3659 correct : 18126/38420 -> 47.1786%\n",
            "test dataset : 5/2000 epochs spend time : 0.8711 sec  / total_loss : 1.1475 correct : 11086/20743 -> 53.4445%\n",
            "best epoch : 5/2000 / val accuracy : 53.444536%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 6/2000 epochs spend time : 1.7632 sec / total_loss : 1.1923 correct : 20976/38420 -> 54.5966%\n",
            "test dataset : 6/2000 epochs spend time : 0.8127 sec  / total_loss : 1.9450 correct : 4488/20743 -> 21.6362%\n",
            "best epoch : 5/2000 / val accuracy : 53.444536%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 7/2000 epochs spend time : 1.6985 sec / total_loss : 1.4708 correct : 16575/38420 -> 43.1416%\n",
            "test dataset : 7/2000 epochs spend time : 0.8350 sec  / total_loss : 1.0694 correct : 12371/20743 -> 59.6394%\n",
            "best epoch : 7/2000 / val accuracy : 59.639396%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 8/2000 epochs spend time : 1.7117 sec / total_loss : 1.1487 correct : 21448/38420 -> 55.8251%\n",
            "test dataset : 8/2000 epochs spend time : 0.8485 sec  / total_loss : 1.0431 correct : 11435/20743 -> 55.1270%\n",
            "best epoch : 7/2000 / val accuracy : 59.639396%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 9/2000 epochs spend time : 1.6890 sec / total_loss : 1.0741 correct : 22833/38420 -> 59.4300%\n",
            "test dataset : 9/2000 epochs spend time : 0.8211 sec  / total_loss : 0.8260 correct : 13777/20743 -> 66.4176%\n",
            "best epoch : 9/2000 / val accuracy : 66.417587%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 10/2000 epochs spend time : 1.7227 sec / total_loss : 1.0415 correct : 23363/38420 -> 60.8095%\n",
            "test dataset : 10/2000 epochs spend time : 0.8150 sec  / total_loss : 0.7983 correct : 14349/20743 -> 69.1751%\n",
            "best epoch : 10/2000 / val accuracy : 69.175143%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 11/2000 epochs spend time : 1.7373 sec / total_loss : 0.9352 correct : 24855/38420 -> 64.6929%\n",
            "test dataset : 11/2000 epochs spend time : 0.8129 sec  / total_loss : 0.9656 correct : 11852/20743 -> 57.1373%\n",
            "best epoch : 10/2000 / val accuracy : 69.175143%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 12/2000 epochs spend time : 1.6905 sec / total_loss : 1.1078 correct : 24478/38420 -> 63.7116%\n",
            "test dataset : 12/2000 epochs spend time : 0.8331 sec  / total_loss : 1.1339 correct : 11063/20743 -> 53.3337%\n",
            "best epoch : 10/2000 / val accuracy : 69.175143%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 13/2000 epochs spend time : 1.7218 sec / total_loss : 1.2035 correct : 22669/38420 -> 59.0031%\n",
            "test dataset : 13/2000 epochs spend time : 0.8935 sec  / total_loss : 1.6657 correct : 7947/20743 -> 38.3117%\n",
            "best epoch : 10/2000 / val accuracy : 69.175143%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 14/2000 epochs spend time : 1.7296 sec / total_loss : 1.0905 correct : 22084/38420 -> 57.4805%\n",
            "test dataset : 14/2000 epochs spend time : 0.8113 sec  / total_loss : 0.7149 correct : 14880/20743 -> 71.7350%\n",
            "best epoch : 14/2000 / val accuracy : 71.735043%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 15/2000 epochs spend time : 1.7106 sec / total_loss : 0.8483 correct : 26373/38420 -> 68.6439%\n",
            "test dataset : 15/2000 epochs spend time : 0.8153 sec  / total_loss : 0.5912 correct : 15936/20743 -> 76.8259%\n",
            "best epoch : 15/2000 / val accuracy : 76.825917%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 16/2000 epochs spend time : 1.6963 sec / total_loss : 0.7429 correct : 27859/38420 -> 72.5117%\n",
            "test dataset : 16/2000 epochs spend time : 0.8345 sec  / total_loss : 0.5552 correct : 16068/20743 -> 77.4623%\n",
            "best epoch : 16/2000 / val accuracy : 77.462276%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 17/2000 epochs spend time : 1.7129 sec / total_loss : 0.9800 correct : 24492/38420 -> 63.7480%\n",
            "test dataset : 17/2000 epochs spend time : 0.8167 sec  / total_loss : 0.6511 correct : 15802/20743 -> 76.1799%\n",
            "best epoch : 16/2000 / val accuracy : 77.462276%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 18/2000 epochs spend time : 1.6518 sec / total_loss : 0.7104 correct : 28610/38420 -> 74.4664%\n",
            "test dataset : 18/2000 epochs spend time : 0.8529 sec  / total_loss : 0.8173 correct : 13842/20743 -> 66.7309%\n",
            "best epoch : 16/2000 / val accuracy : 77.462276%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 19/2000 epochs spend time : 1.6883 sec / total_loss : 0.7659 correct : 27868/38420 -> 72.5351%\n",
            "test dataset : 19/2000 epochs spend time : 0.8351 sec  / total_loss : 0.4762 correct : 17292/20743 -> 83.3631%\n",
            "best epoch : 19/2000 / val accuracy : 83.363062%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 20/2000 epochs spend time : 1.7014 sec / total_loss : 0.8040 correct : 28034/38420 -> 72.9672%\n",
            "test dataset : 20/2000 epochs spend time : 0.8273 sec  / total_loss : 0.9870 correct : 12866/20743 -> 62.0257%\n",
            "best epoch : 19/2000 / val accuracy : 83.363062%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 21/2000 epochs spend time : 1.6996 sec / total_loss : 0.8593 correct : 26292/38420 -> 68.4331%\n",
            "test dataset : 21/2000 epochs spend time : 0.8160 sec  / total_loss : 0.6172 correct : 16344/20743 -> 78.7928%\n",
            "best epoch : 19/2000 / val accuracy : 83.363062%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 22/2000 epochs spend time : 1.6699 sec / total_loss : 0.6176 correct : 30210/38420 -> 78.6309%\n",
            "test dataset : 22/2000 epochs spend time : 0.8917 sec  / total_loss : 0.3552 correct : 17989/20743 -> 86.7232%\n",
            "best epoch : 22/2000 / val accuracy : 86.723232%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 23/2000 epochs spend time : 1.7302 sec / total_loss : 0.5851 correct : 30774/38420 -> 80.0989%\n",
            "test dataset : 23/2000 epochs spend time : 0.8117 sec  / total_loss : 0.3597 correct : 17993/20743 -> 86.7425%\n",
            "best epoch : 23/2000 / val accuracy : 86.742516%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 24/2000 epochs spend time : 1.6834 sec / total_loss : 0.7571 correct : 28611/38420 -> 74.4690%\n",
            "test dataset : 24/2000 epochs spend time : 0.8152 sec  / total_loss : 0.6326 correct : 15713/20743 -> 75.7509%\n",
            "best epoch : 23/2000 / val accuracy : 86.742516%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 25/2000 epochs spend time : 1.7176 sec / total_loss : 0.5488 correct : 31384/38420 -> 81.6866%\n",
            "test dataset : 25/2000 epochs spend time : 0.8185 sec  / total_loss : 0.3212 correct : 18408/20743 -> 88.7432%\n",
            "best epoch : 25/2000 / val accuracy : 88.743190%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 26/2000 epochs spend time : 1.7112 sec / total_loss : 0.4364 correct : 33129/38420 -> 86.2285%\n",
            "test dataset : 26/2000 epochs spend time : 0.8234 sec  / total_loss : 0.2624 correct : 18632/20743 -> 89.8231%\n",
            "best epoch : 26/2000 / val accuracy : 89.823073%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 27/2000 epochs spend time : 1.7259 sec / total_loss : 0.3913 correct : 33800/38420 -> 87.9750%\n",
            "test dataset : 27/2000 epochs spend time : 0.8204 sec  / total_loss : 0.4763 correct : 17655/20743 -> 85.1131%\n",
            "best epoch : 26/2000 / val accuracy : 89.823073%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 28/2000 epochs spend time : 1.6728 sec / total_loss : 0.5810 correct : 31326/38420 -> 81.5357%\n",
            "test dataset : 28/2000 epochs spend time : 0.8313 sec  / total_loss : 0.2832 correct : 18874/20743 -> 90.9897%\n",
            "best epoch : 28/2000 / val accuracy : 90.989731%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 29/2000 epochs spend time : 1.7393 sec / total_loss : 0.3734 correct : 34304/38420 -> 89.2868%\n",
            "test dataset : 29/2000 epochs spend time : 0.8346 sec  / total_loss : 0.1684 correct : 19552/20743 -> 94.2583%\n",
            "best epoch : 29/2000 / val accuracy : 94.258304%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 30/2000 epochs spend time : 1.7198 sec / total_loss : 0.3214 correct : 35142/38420 -> 91.4680%\n",
            "test dataset : 30/2000 epochs spend time : 0.8070 sec  / total_loss : 0.3365 correct : 18000/20743 -> 86.7763%\n",
            "best epoch : 29/2000 / val accuracy : 94.258304%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 31/2000 epochs spend time : 1.7381 sec / total_loss : 0.3670 correct : 34414/38420 -> 89.5731%\n",
            "test dataset : 31/2000 epochs spend time : 0.8379 sec  / total_loss : 0.1400 correct : 19794/20743 -> 95.4250%\n",
            "best epoch : 31/2000 / val accuracy : 95.424963%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 32/2000 epochs spend time : 1.7217 sec / total_loss : 0.2142 correct : 36588/38420 -> 95.2317%\n",
            "test dataset : 32/2000 epochs spend time : 0.8203 sec  / total_loss : 0.1535 correct : 19675/20743 -> 94.8513%\n",
            "best epoch : 31/2000 / val accuracy : 95.424963%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 33/2000 epochs spend time : 1.7097 sec / total_loss : 0.3988 correct : 33974/38420 -> 88.4279%\n",
            "test dataset : 33/2000 epochs spend time : 0.8266 sec  / total_loss : 0.3800 correct : 17833/20743 -> 85.9712%\n",
            "best epoch : 31/2000 / val accuracy : 95.424963%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 34/2000 epochs spend time : 1.7500 sec / total_loss : 0.2670 correct : 36066/38420 -> 93.8730%\n",
            "test dataset : 34/2000 epochs spend time : 0.8626 sec  / total_loss : 0.1535 correct : 19685/20743 -> 94.8995%\n",
            "best epoch : 31/2000 / val accuracy : 95.424963%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 35/2000 epochs spend time : 1.7969 sec / total_loss : 0.4698 correct : 33241/38420 -> 86.5200%\n",
            "test dataset : 35/2000 epochs spend time : 0.8373 sec  / total_loss : 0.2950 correct : 18572/20743 -> 89.5338%\n",
            "best epoch : 31/2000 / val accuracy : 95.424963%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 36/2000 epochs spend time : 1.6855 sec / total_loss : 0.2523 correct : 36176/38420 -> 94.1593%\n",
            "test dataset : 36/2000 epochs spend time : 0.8186 sec  / total_loss : 0.0690 correct : 20335/20743 -> 98.0331%\n",
            "best epoch : 36/2000 / val accuracy : 98.033071%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 37/2000 epochs spend time : 1.7148 sec / total_loss : 0.1533 correct : 37608/38420 -> 97.8865%\n",
            "test dataset : 37/2000 epochs spend time : 0.8342 sec  / total_loss : 0.0425 correct : 20500/20743 -> 98.8285%\n",
            "best epoch : 37/2000 / val accuracy : 98.828520%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 38/2000 epochs spend time : 1.6727 sec / total_loss : 0.1680 correct : 37350/38420 -> 97.2150%\n",
            "test dataset : 38/2000 epochs spend time : 0.8586 sec  / total_loss : 0.3412 correct : 18450/20743 -> 88.9457%\n",
            "best epoch : 37/2000 / val accuracy : 98.828520%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 39/2000 epochs spend time : 1.6614 sec / total_loss : 1.0802 correct : 27051/38420 -> 70.4086%\n",
            "test dataset : 39/2000 epochs spend time : 0.8202 sec  / total_loss : 0.5953 correct : 16479/20743 -> 79.4437%\n",
            "best epoch : 37/2000 / val accuracy : 98.828520%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 40/2000 epochs spend time : 1.7063 sec / total_loss : 0.5078 correct : 32419/38420 -> 84.3805%\n",
            "test dataset : 40/2000 epochs spend time : 0.8920 sec  / total_loss : 0.1467 correct : 19829/20743 -> 95.5937%\n",
            "best epoch : 37/2000 / val accuracy : 98.828520%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 41/2000 epochs spend time : 1.6926 sec / total_loss : 0.2417 correct : 36428/38420 -> 94.8152%\n",
            "test dataset : 41/2000 epochs spend time : 0.8269 sec  / total_loss : 0.1683 correct : 19445/20743 -> 93.7425%\n",
            "best epoch : 37/2000 / val accuracy : 98.828520%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 42/2000 epochs spend time : 1.6868 sec / total_loss : 0.1866 correct : 37113/38420 -> 96.5981%\n",
            "test dataset : 42/2000 epochs spend time : 0.8101 sec  / total_loss : 0.0627 correct : 20351/20743 -> 98.1102%\n",
            "best epoch : 37/2000 / val accuracy : 98.828520%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 43/2000 epochs spend time : 1.6731 sec / total_loss : 0.1402 correct : 37708/38420 -> 98.1468%\n",
            "test dataset : 43/2000 epochs spend time : 0.8262 sec  / total_loss : 0.0306 correct : 20597/20743 -> 99.2961%\n",
            "best epoch : 43/2000 / val accuracy : 99.296148%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 44/2000 epochs spend time : 1.7116 sec / total_loss : 0.1071 correct : 38185/38420 -> 99.3883%\n",
            "test dataset : 44/2000 epochs spend time : 0.8266 sec  / total_loss : 0.0210 correct : 20622/20743 -> 99.4167%\n",
            "best epoch : 44/2000 / val accuracy : 99.416671%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 45/2000 epochs spend time : 1.6974 sec / total_loss : 0.1013 correct : 38246/38420 -> 99.5471%\n",
            "test dataset : 45/2000 epochs spend time : 0.8187 sec  / total_loss : 0.0135 correct : 20686/20743 -> 99.7252%\n",
            "best epoch : 45/2000 / val accuracy : 99.725209%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 46/2000 epochs spend time : 1.7075 sec / total_loss : 0.0958 correct : 38292/38420 -> 99.6668%\n",
            "test dataset : 46/2000 epochs spend time : 0.8611 sec  / total_loss : 0.0199 correct : 20622/20743 -> 99.4167%\n",
            "best epoch : 45/2000 / val accuracy : 99.725209%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 47/2000 epochs spend time : 1.6708 sec / total_loss : 0.0980 correct : 38255/38420 -> 99.5705%\n",
            "test dataset : 47/2000 epochs spend time : 0.8369 sec  / total_loss : 0.0107 correct : 20696/20743 -> 99.7734%\n",
            "best epoch : 47/2000 / val accuracy : 99.773418%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 48/2000 epochs spend time : 1.6830 sec / total_loss : 0.0990 correct : 38260/38420 -> 99.5836%\n",
            "test dataset : 48/2000 epochs spend time : 0.8231 sec  / total_loss : 0.0302 correct : 20583/20743 -> 99.2287%\n",
            "best epoch : 47/2000 / val accuracy : 99.773418%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 49/2000 epochs spend time : 1.6513 sec / total_loss : 0.1755 correct : 37540/38420 -> 97.7095%\n",
            "test dataset : 49/2000 epochs spend time : 0.8165 sec  / total_loss : 0.7244 correct : 17798/20743 -> 85.8024%\n",
            "best epoch : 47/2000 / val accuracy : 99.773418%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 50/2000 epochs spend time : 1.6685 sec / total_loss : 1.0663 correct : 28765/38420 -> 74.8699%\n",
            "test dataset : 50/2000 epochs spend time : 0.8446 sec  / total_loss : 1.8702 correct : 8925/20743 -> 43.0266%\n",
            "best epoch : 47/2000 / val accuracy : 99.773418%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 51/2000 epochs spend time : 1.6741 sec / total_loss : 0.7647 correct : 29417/38420 -> 76.5669%\n",
            "test dataset : 51/2000 epochs spend time : 0.8753 sec  / total_loss : 0.2328 correct : 19201/20743 -> 92.5662%\n",
            "best epoch : 47/2000 / val accuracy : 99.773418%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 52/2000 epochs spend time : 1.6846 sec / total_loss : 0.2806 correct : 35716/38420 -> 92.9620%\n",
            "test dataset : 52/2000 epochs spend time : 0.8564 sec  / total_loss : 0.0711 correct : 20406/20743 -> 98.3754%\n",
            "best epoch : 47/2000 / val accuracy : 99.773418%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 53/2000 epochs spend time : 1.6995 sec / total_loss : 0.1337 correct : 37973/38420 -> 98.8365%\n",
            "test dataset : 53/2000 epochs spend time : 0.8531 sec  / total_loss : 0.0206 correct : 20661/20743 -> 99.6047%\n",
            "best epoch : 47/2000 / val accuracy : 99.773418%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 54/2000 epochs spend time : 1.6785 sec / total_loss : 0.1128 correct : 38121/38420 -> 99.2218%\n",
            "test dataset : 54/2000 epochs spend time : 0.8603 sec  / total_loss : 0.0154 correct : 20675/20743 -> 99.6722%\n",
            "best epoch : 47/2000 / val accuracy : 99.773418%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 55/2000 epochs spend time : 1.6510 sec / total_loss : 0.1047 correct : 38200/38420 -> 99.4274%\n",
            "test dataset : 55/2000 epochs spend time : 0.8436 sec  / total_loss : 0.0090 correct : 20711/20743 -> 99.8457%\n",
            "best epoch : 55/2000 / val accuracy : 99.845731%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 56/2000 epochs spend time : 1.7041 sec / total_loss : 0.0913 correct : 38377/38420 -> 99.8881%\n",
            "test dataset : 56/2000 epochs spend time : 0.8609 sec  / total_loss : 0.0031 correct : 20738/20743 -> 99.9759%\n",
            "best epoch : 56/2000 / val accuracy : 99.975895%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 57/2000 epochs spend time : 1.6958 sec / total_loss : 0.0875 correct : 38412/38420 -> 99.9792%\n",
            "test dataset : 57/2000 epochs spend time : 0.8513 sec  / total_loss : 0.0019 correct : 20741/20743 -> 99.9904%\n",
            "best epoch : 57/2000 / val accuracy : 99.990358%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 58/2000 epochs spend time : 1.7060 sec / total_loss : 0.0863 correct : 38416/38420 -> 99.9896%\n",
            "test dataset : 58/2000 epochs spend time : 0.8514 sec  / total_loss : 0.0014 correct : 20742/20743 -> 99.9952%\n",
            "best epoch : 58/2000 / val accuracy : 99.995179%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 59/2000 epochs spend time : 1.7016 sec / total_loss : 0.0858 correct : 38418/38420 -> 99.9948%\n",
            "test dataset : 59/2000 epochs spend time : 0.8176 sec  / total_loss : 0.0011 correct : 20742/20743 -> 99.9952%\n",
            "best epoch : 58/2000 / val accuracy : 99.995179%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 60/2000 epochs spend time : 1.6954 sec / total_loss : 0.0852 correct : 38418/38420 -> 99.9948%\n",
            "test dataset : 60/2000 epochs spend time : 0.8699 sec  / total_loss : 0.0009 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 61/2000 epochs spend time : 1.6942 sec / total_loss : 0.0848 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 61/2000 epochs spend time : 0.8270 sec  / total_loss : 0.0008 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 62/2000 epochs spend time : 1.7102 sec / total_loss : 0.0845 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 62/2000 epochs spend time : 0.8451 sec  / total_loss : 0.0007 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 63/2000 epochs spend time : 1.6827 sec / total_loss : 0.0842 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 63/2000 epochs spend time : 0.8439 sec  / total_loss : 0.0006 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 64/2000 epochs spend time : 1.6951 sec / total_loss : 0.0840 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 64/2000 epochs spend time : 0.8623 sec  / total_loss : 0.0005 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 65/2000 epochs spend time : 1.6956 sec / total_loss : 0.0838 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 65/2000 epochs spend time : 0.8193 sec  / total_loss : 0.0005 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 66/2000 epochs spend time : 1.6708 sec / total_loss : 0.0835 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 66/2000 epochs spend time : 0.8566 sec  / total_loss : 0.0005 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 67/2000 epochs spend time : 1.6756 sec / total_loss : 0.0833 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 67/2000 epochs spend time : 0.8276 sec  / total_loss : 0.0005 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 68/2000 epochs spend time : 1.7077 sec / total_loss : 0.0831 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 68/2000 epochs spend time : 0.8664 sec  / total_loss : 0.0004 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 69/2000 epochs spend time : 1.6708 sec / total_loss : 0.0829 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 69/2000 epochs spend time : 0.8352 sec  / total_loss : 0.0004 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 70/2000 epochs spend time : 1.6774 sec / total_loss : 0.0827 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 70/2000 epochs spend time : 0.8577 sec  / total_loss : 0.0004 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 71/2000 epochs spend time : 1.6640 sec / total_loss : 0.0825 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 71/2000 epochs spend time : 0.8298 sec  / total_loss : 0.0004 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 72/2000 epochs spend time : 1.6944 sec / total_loss : 0.0823 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 72/2000 epochs spend time : 0.8423 sec  / total_loss : 0.0004 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 73/2000 epochs spend time : 1.6704 sec / total_loss : 0.0821 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 73/2000 epochs spend time : 0.8277 sec  / total_loss : 0.0004 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 74/2000 epochs spend time : 1.6599 sec / total_loss : 0.0819 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 74/2000 epochs spend time : 0.8421 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 75/2000 epochs spend time : 1.6548 sec / total_loss : 0.0817 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 75/2000 epochs spend time : 0.8257 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 76/2000 epochs spend time : 1.6701 sec / total_loss : 0.0815 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 76/2000 epochs spend time : 0.8134 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 77/2000 epochs spend time : 1.6900 sec / total_loss : 0.0813 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 77/2000 epochs spend time : 0.8077 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 78/2000 epochs spend time : 1.6495 sec / total_loss : 0.0811 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 78/2000 epochs spend time : 0.8260 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 79/2000 epochs spend time : 1.6787 sec / total_loss : 0.0809 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 79/2000 epochs spend time : 0.8524 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 80/2000 epochs spend time : 1.6746 sec / total_loss : 0.0807 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 80/2000 epochs spend time : 0.8134 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 81/2000 epochs spend time : 1.6854 sec / total_loss : 0.0805 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 81/2000 epochs spend time : 0.8203 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 82/2000 epochs spend time : 1.6327 sec / total_loss : 0.0804 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 82/2000 epochs spend time : 0.8183 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 83/2000 epochs spend time : 1.6826 sec / total_loss : 0.0802 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 83/2000 epochs spend time : 0.8159 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 84/2000 epochs spend time : 1.6807 sec / total_loss : 0.0800 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 84/2000 epochs spend time : 0.8289 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 85/2000 epochs spend time : 1.6819 sec / total_loss : 0.0798 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 85/2000 epochs spend time : 0.7952 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 86/2000 epochs spend time : 1.6531 sec / total_loss : 0.0796 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 86/2000 epochs spend time : 0.8206 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 87/2000 epochs spend time : 1.6930 sec / total_loss : 0.0794 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 87/2000 epochs spend time : 0.8171 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 88/2000 epochs spend time : 1.6563 sec / total_loss : 0.0792 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 88/2000 epochs spend time : 0.8286 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 89/2000 epochs spend time : 1.6930 sec / total_loss : 0.0790 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 89/2000 epochs spend time : 0.8109 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 90/2000 epochs spend time : 1.6703 sec / total_loss : 0.0788 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 90/2000 epochs spend time : 0.8324 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "best epoch : 60/2000 / val accuracy : 100.000000%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 91/2000 epochs spend time : 1.6761 sec / total_loss : 0.0787 correct : 38420/38420 -> 100.0000%\n",
            "test dataset : 91/2000 epochs spend time : 0.8269 sec  / total_loss : 0.0003 correct : 20743/20743 -> 100.0000%\n",
            "Early Stopping\n",
            "best epoch : 60/2000 / accuracy : 100.000000%\n",
            "==============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwSTx6YU7kq-",
        "outputId": "868befc1-b1c6-4247-e593-a1f5a51d9a05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \u001b[0m\u001b[01;34m기타_문서\u001b[0m/\n",
            " \u001b[01;34m가족\u001b[0m/\n",
            " \u001b[01;34m자산\u001b[0m/\n",
            " \u001b[01;34m000.연구\u001b[0m/\n",
            " \u001b[01;34m001.프로젝트-과제\u001b[0m/\n",
            " \u001b[01;34m002.교육\u001b[0m/\n",
            " \u001b[01;34m003.학생처장\u001b[0m/\n",
            " 20201005_학생처_초안.hwp\n",
            " 20201005_학생처.pdf\n",
            " \u001b[01;34mannotations\u001b[0m/\n",
            "\u001b[01;34m'Colab Notebooks'\u001b[0m/\n",
            " Deep-Learning-with-PyTorch.pdf\n",
            " download_physionet.sh\n",
            " images.jpeg\n",
            "'[한림 백년대계]Intramural League.hwp'\n",
            " 다운로드.jpeg\n",
            " logging.txt\n",
            " \u001b[01;34mSAVED_MODEL\u001b[0m/\n",
            " \u001b[01;34msignals\u001b[0m/\n",
            " \u001b[01;34mSleepCapstone\u001b[0m/\n",
            " \u001b[01;34mSystem_on_Chip_연구실\u001b[0m/\n",
            " train.pth\n",
            "'Understanding 1D Convolutional Neural Networks Using Multiclass T.pdf'\n",
            "'이정근 홈페.xlsx'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCE73qoRBUC2"
      },
      "source": [
        "SAVE_PATH = './SAVED_MODEL/'\n",
        "torch.save(model, SAVE_PATH+ 'model.pt')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEKaeux07zwb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}