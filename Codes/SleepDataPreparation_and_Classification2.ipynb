{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SleepDataPreparation_and_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/SleepCapstone/blob/master/Codes/SleepDataPreparation_and_Classification2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAGJRSb3xOT1"
      },
      "source": [
        "한림대학교 소프트웨어 융합대학\n",
        "빅데이터 캡스톤 프로젝트\n",
        "\n",
        "딥러닝에 기반한 수면 분류 모델 개발"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SaQVdPHgfmx",
        "outputId": "0070eded-fb28-4130-aff1-53fcec2659b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohoEZomKw0cz",
        "outputId": "9a285f1c-13f8-40ed-aa5d-6356c1b9fd83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd drive/My\\ Drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvna-5vRzmTQ",
        "outputId": "4a1eb9f4-38ca-45cc-b528-30c9ef8755ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "# GPU 가용성 체크\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Oct 14 23:54:21 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF2HFogi4Nzq",
        "outputId": "19c469bb-54d3-44b3-97ee-03fe48fb13b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "!pip install pyedflib"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyedflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/1d/f10ea5017a47398fda885c3c313973cf032c19fd6eb773d8e5b816ac3efc/pyEDFlib-0.1.19.tar.gz (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 20.8MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 6.1MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 7.0MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 8.2MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 6.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 7.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 71kB 8.1MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 8.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 102kB 8.3MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 122kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 133kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▊                            | 143kB 8.3MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 8.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 163kB 8.3MB/s eta 0:00:01\r\u001b[K     |████▌                           | 174kB 8.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 184kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 194kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 204kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 215kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 225kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 235kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 245kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 256kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 266kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 276kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 286kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 296kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 307kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 317kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 327kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 337kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 348kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 358kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 368kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 378kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 389kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 399kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 409kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 419kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 430kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 440kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 450kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 460kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 471kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 481kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 491kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 501kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 512kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 522kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 532kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 542kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 552kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 563kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 573kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 583kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 593kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 604kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 614kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 624kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 634kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 645kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 655kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 665kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 675kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 686kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 696kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 706kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 716kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 727kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 737kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 747kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 757kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 768kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 778kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 788kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 798kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 808kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 819kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 829kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 839kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 849kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 860kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 870kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 880kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 890kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 901kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 911kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 921kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 931kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 942kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 952kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 962kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 972kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 983kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 993kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.0MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.0MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.0MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.0MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 8.3MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from pyedflib) (1.18.5)\n",
            "Building wheels for collected packages: pyedflib\n",
            "  Building wheel for pyedflib (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyedflib: filename=pyEDFlib-0.1.19-cp36-cp36m-linux_x86_64.whl size=925693 sha256=966718e1cd97e08779a302d6231b46a049edc2a21fc69ee9bdd206746708d24b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/b7/24/a565e4f7471617165f1b040651b52d87ad1885aaf32e02d4f9\n",
            "Successfully built pyedflib\n",
            "Installing collected packages: pyedflib\n",
            "Successfully installed pyedflib-0.1.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1zNHBNc4beq"
      },
      "source": [
        "import numpy as np\n",
        "from pyedflib import highlevel\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n9vmRl3x-1e"
      },
      "source": [
        "def search_signals_npy(dirname):\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for file in filenames if file.endswith(\".npy\")]\n",
        "    return filenames\n",
        "\n",
        "def search_correct_signals_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vSGY6sO9vgC",
        "outputId": "32b48311-61b4-4abf-d179-58d248b3b3f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "\n",
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  'annotations/npy/remove_wake/'\n",
        "signals_path = 'signals/npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "total_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list:\n",
        "    label = np.load(path + filename)\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    \n",
        "    signals = np.load(signals_path+signals_filename)\n",
        "    \n",
        "    \n",
        "    #print('remove start index : %d / remove end index : %d'%(remove_start_index,remove_end_index))\n",
        "    #print(np.bincount(label,minlength=6))\n",
        "    if len(label) !=len(signals[0])//30//fs:\n",
        "        print('file is fault!!!')\n",
        "    for i in range(6):\n",
        "        total_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "print(total_label)\n",
        "\n",
        "x = np.arange(len(total_label))\n",
        "\n",
        "plt.bar(x,total_label,width=0.7)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4336  2804 17799  5703  7717    61]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 6 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVVZuGLx-ADW",
        "outputId": "1739b2bc-b832-491a-ab12-39cb2ac65ac6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  'annotations/npy/remove_wake/'\n",
        "signals_path = 'signals/npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "random.shuffle(annotations_npy_list)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "trainDataset_count = 30\n",
        "testDataset_count = len(annotations_npy_list)-trainDataset_count\n",
        "\n",
        "print(trainDataset_count)\n",
        "print(testDataset_count)\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4001EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4192EV-Hypnogram.npy']\n",
            "['SC4001EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy']\n",
            "30\n",
            "9\n",
            "[10.05598104  7.46299413 46.66689563 15.82580623 19.83377408  0.15454889]\n",
            "[15.13490272  6.78275825 45.26496829 11.77039665 20.87498656  0.17198753]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAKH9Mb0_NF3"
      },
      "source": [
        "signals_path = 'signals/npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "save_train_path = 'signals/npy//Fpz-Cz/remove_wake/train/'\n",
        "save_test_path = 'signals/npy/Fpz-Cz/remove_wake/test/'\n",
        "\n",
        "os.makedirs(save_train_path,exist_ok=True)\n",
        "os.makedirs(save_test_path,exist_ok=True)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_train_path+filename)\n",
        "    \n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_test_path+filename)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4EYtlUH-e9f",
        "outputId": "f194f6ce-13dd-4e81-b3ec-02b0633c547c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "def search_correct_annotations_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename\n",
        "\n",
        "train_path =  'signals/npy/Fpz-Cz/remove_wake/train/'\n",
        "test_path = 'signals/npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_path = 'annotations/npy/remove_wake/'\n",
        "\n",
        "train_list = search_signals_npy(train_path)\n",
        "test_list = search_signals_npy(test_path)\n",
        "\n",
        "print(train_list)\n",
        "print(test_list)\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in train_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in test_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4082EP-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4031EC-Hypnogram.npy']\n",
            "['SC4182EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy']\n",
            "[11.02473498  7.14063604 47.00212014 14.87491166 19.81342756  0.14416961]\n",
            "[11.09453059  6.61779018 47.57672095 12.43106469 22.02037241  0.25952118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBwqVERu_aw-"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pylab as plt\n",
        "from  torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSQaEXQr_h8m"
      },
      "source": [
        "def data_preprocessing_torch(signals): # 하나의 데이터셋에 대한 data_preprocessing (using torch)\n",
        "    signals = (signals - signals.mean(dim=1).unsqueeze(1))/signals.std(dim=1).unsqueeze(1)\n",
        "\n",
        "    return signals\n",
        "\n",
        "def data_preprocessing_oneToOne_torch(signals,min,max,max_value):\n",
        "    signals_std = (signals + max_value) / (2*max_value)\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def data_preprocessing_minmax_torch(signals,min,max):\n",
        "    signals_std = (signals - signals.min(dim=1).unsqueeze(1)) / (\n",
        "            signals.max(dim=1).unsqueeze(1) - signals.min(dim=1).unsqueeze(1))\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def get_dataset_one_channel_norm_withoutCut(dirname,annotations_dir,data_path,use_noise=True,epsilon=0.5,noise_scale=2e-6,preprocessing=True,norm_methods='Standard'):\n",
        "    # npy read!\n",
        "    path = dirname + data_path\n",
        "    signals = np.load(path)\n",
        "\n",
        "    signals = torch.from_numpy(signals).float().to(device)\n",
        "\n",
        "    if use_noise:\n",
        "        if np.random.rand() < epsilon:\n",
        "            # noise = np.random.normal(loc=0,scale=noise_scale,size=signals.shape)\n",
        "            # signals = signals + noise\n",
        "            noise = torch.normal(mean=0., std=noise_scale, size=signals.shape).to(device)\n",
        "            signals = signals + noise\n",
        "\n",
        "    if preprocessing:\n",
        "        if norm_methods == 'Standard':\n",
        "            signals = data_preprocessing_torch(signals)\n",
        "        elif norm_methods == 'minmax':\n",
        "            signals = data_preprocessing_minmax_torch(signals,0,1)\n",
        "        elif norm_methods == 'oneToOne':\n",
        "            signals = data_preprocessing_oneToOne_torch(signals,-1,1,1e-4)\n",
        "    label = get_annotations(annotations_dir, data_path)\n",
        "    return signals, label\n",
        "\n",
        "# model conv layer weight init function\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:         # Conv weight init\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "        \n",
        "def suffle_dataset_list(dataset_list): # 데이터 셔플\n",
        "    random.shuffle(dataset_list)\n",
        "    return dataset_list\n",
        "\n",
        "\n",
        "# npy파일을 통해 label을 가져오는 함수\n",
        "def get_annotations(label_dir,file_name):\n",
        "    label_path = label_dir + file_name\n",
        "    label = np.load(label_path)\n",
        "    return label\n",
        "\n",
        "def signals_expand_torch_one_channel(signals): # 2차원 데이터를 3차원으로 변환 (8,N) -> (batch,8,6000) 형태로\n",
        "    signals = signals.expand(1,1,-1)\n",
        "    #print(signals.shape)\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    signals = signals.reshape(-1, 3000, 1) # 형태 변환\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    return signals"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx4M7Rfq_lwo"
      },
      "source": [
        "class DeepSleepNet_Classification(nn.Module):  # input channel = 8channel / output = 5\n",
        "    def __init__(self,in_channel=1,out_channel=6,layer=[64,128,128,128],sample_rate = 100):\n",
        "        super(DeepSleepNet_Classification, self).__init__()\n",
        "\n",
        "        \n",
        "        self.fc1 = nn.Linear(3000,1024)\n",
        "        self.fc2 = nn.Linear(1024,1024)\n",
        "        self.fc3 = nn.Linear(1024,512)\n",
        "        self.fc4 = nn.Linear(512, 256)\n",
        "        self.fc5 = nn.Linear(256, out_channel)\n",
        "\n",
        "        self.ReLU = nn.ReLU()\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "        # print(\"feature_extract_2d.shape : \", feature_extract_2d.shape)\n",
        "        # 여기서 문제 발생 weight의 경우에는 [64 , 32 , 100] 이지만 input 이 2차원 [32, 750]이라 문제 발생!\n",
        "        out = torch.flatten(input, 1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.fc4(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.fc5(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYZ6YSZHArdJ",
        "outputId": "92aec5a2-cea4-40ad-87fe-7fa0a1526f77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "model = DeepSleepNet_Classification(in_channel=1,out_channel=6)\n",
        "summary(model.cuda(),(1,3000))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 1024]       3,073,024\n",
            "              ReLU-2                 [-1, 1024]               0\n",
            "            Linear-3                 [-1, 1024]       1,049,600\n",
            "              ReLU-4                 [-1, 1024]               0\n",
            "            Linear-5                  [-1, 512]         524,800\n",
            "              ReLU-6                  [-1, 512]               0\n",
            "            Linear-7                  [-1, 256]         131,328\n",
            "              ReLU-8                  [-1, 256]               0\n",
            "            Linear-9                    [-1, 6]           1,542\n",
            "================================================================\n",
            "Total params: 4,780,294\n",
            "Trainable params: 4,780,294\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.04\n",
            "Params size (MB): 18.24\n",
            "Estimated Total Size (MB): 18.29\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li3ftAC1Az0Q"
      },
      "source": [
        "def search_npy_list(dirname):  # 매개변수 dir에서 모든 npy파일을 찾고 fold에 따른 dataset 나누기\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for _, file in enumerate(filenames) if file.endswith(\".npy\")]\n",
        "    return filenames\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQH_VRRFBGF1"
      },
      "source": [
        "def train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, test_signal_dir,annotations_dir\n",
        "                               ,epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                               layer_filters=[64,128,256,512],first_conv=[200,40,100],optim='Adam',lf='CE',\n",
        "                               epsilon=0.7,noise_scale=2e-6,min_value=-1e-4,max_value=1e-4,preprocessing=True,\n",
        "                               norm_methods='Standard',use_noise=True,loss_type='softmax'):\n",
        "    # Adam optimizer param\n",
        "    b1 = 0.5\n",
        "    b2 = 0.999\n",
        "\n",
        "    beta = 0.001\n",
        "\n",
        "    check_file = open(logging_filename, 'w')  # logging file\n",
        "\n",
        "    print('Preproceesing  : ',preprocessing)\n",
        "    print('min/max value : %f/%f'%(min_value,max_value))\n",
        "    print('noise scale : ',noise_scale)\n",
        "    print('loss function : ',lf)\n",
        "    print('epsilon : ',epsilon)\n",
        "    print('norm methods : ',norm_methods)\n",
        "\n",
        "    print('logging file name : ', logging_filename)\n",
        "    print('save file name : ', save_filename)\n",
        "    print('layer filters : ',layer_filters)\n",
        "    print('fisrt_conv info : ',first_conv)\n",
        "    print('loss type : ',loss_type)\n",
        "    print('training data oversampling noise : ',use_noise)\n",
        "    best_accuracy = 0.\n",
        "    best_epoch = 0\n",
        "\n",
        "    train_dataset_list = search_npy_list(train_signal_dir)\n",
        "    test_dataset_list = search_npy_list(test_signal_dir)\n",
        "\n",
        "    train_dataset_len = len(train_dataset_list)\n",
        "    test_dataset_len = len(test_dataset_list)\n",
        "\n",
        "    print('train_dataset length : ', len(train_dataset_list))\n",
        "    print(train_dataset_list)\n",
        "\n",
        "    print('test_dataset length : ',test_dataset_len)\n",
        "    print(test_dataset_list)\n",
        "\n",
        "\n",
        "    model = DeepSleepNet_Classification()\n",
        "\n",
        "    #model = resnet18_200hz(in_channel=1,layer_filters=layer_filters,first_conv=first_conv,use_batchnorm=True,num_classes=5)\n",
        "\n",
        "    model.apply(weights_init) # weight init\n",
        "\n",
        "    cuda = torch.cuda.is_available()\n",
        "\n",
        "    if cuda:\n",
        "        print('can use CUDA!!!')\n",
        "        model = model.cuda()\n",
        "    #summary(model,[1,6000])\n",
        "    print('torch.cuda.device_count() : ', torch.cuda.device_count())\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print('Multi GPU Activation !!!')\n",
        "        #model = nn.DataParallel(model)\n",
        "\n",
        "    # loss funcition\n",
        "    if lf == 'CE':\n",
        "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "    elif lf == 'CEW':\n",
        "        samples_per_cls = [27,15,41,5,11]\n",
        "        no_of_classes = 5\n",
        "        effective_num = 1.0 - np.power(beta,samples_per_cls)\n",
        "        #print(effective_num)\n",
        "        weights = (1.0 - beta) / np.array(effective_num)\n",
        "        #print(weights)\n",
        "        weights = weights / np.sum(weights) * no_of_classes\n",
        "        weights = torch.tensor(weights).float()\n",
        "        weights = weights.to(device)\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=weights).to(device)\n",
        "    elif lf == 'FL':\n",
        "        loss_fn = FocalLoss(gamma=2).to(device)\n",
        "    elif lf == 'CBL':\n",
        "        loss_fn = CB_loss(samples_per_cls=[27,15,41,5,11],no_of_classes=5,loss_type=loss_type,beta=0.9999,gamma=2.0)\n",
        "    #loss_fn = FocalLoss(gamma=2).to(device)\n",
        "\n",
        "    # optimizer ADAM (SGD의 경우에는 정상적으로 학습이 진행되지 않았음)\n",
        "    if optim == 'Adam':\n",
        "        print('Optimizer : Adam')\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "    elif optim == 'RMS':\n",
        "        print('Optimizer : RMSprop')\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "    elif optim == 'SGD':\n",
        "        print('Optimizer : SGD')\n",
        "        optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=gamma, patience=10,\n",
        "                                                           min_lr=1e-6)\n",
        "    #stride = 40 일 때, batch_size = 20이면 16GB정도의 메모리 사용\n",
        "    batch_size = 5\n",
        "    norm_square = 2\n",
        "\n",
        "    train_batch_size = math.ceil(train_dataset_len / batch_size)\n",
        "    print('train_batch_size : ',train_batch_size)\n",
        "\n",
        "    test_batch_size = test_dataset_len\n",
        "\n",
        "    best_accuracy = 0.\n",
        "    stop_count = 0\n",
        "    for epoch in range(epochs):\n",
        "        train_dataset = suffle_dataset_list(train_dataset_list) # 매 epoch마다 train_dataset shuffle !\n",
        "        count = 0  # check batch\n",
        "        train_total_loss = 0.0\n",
        "        train_total_count = 0\n",
        "        train_total_data = 0\n",
        "\n",
        "        val_total_loss = 0.0\n",
        "        val_total_count = 0\n",
        "        val_total_data = 0\n",
        "\n",
        "        test_total_loss = 0.0\n",
        "        test_total_count = 0\n",
        "        test_total_data = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "\n",
        "        output_str = 'current_lr : %f\\n'%(optimizer.state_dict()['param_groups'][0]['lr'])\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "        for index, file_name in enumerate(train_dataset):\n",
        "            #print('index : ',index)\n",
        "            if index % batch_size == 0:\n",
        "                batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "            else:\n",
        "                new_signal, new_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "                batch_signal = torch.cat((batch_signal, new_signal),dim=1)\n",
        "                batch_label = np.concatenate((batch_label, new_label))\n",
        "            count += 1\n",
        "            if count == batch_size or index == len(train_dataset) - 1:  # batch 학습 시작!\n",
        "                batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                #batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                # batch_signal = torch.from_numpy(batch_signal).float().to(device)\n",
        "                batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "                optimizer.zero_grad()\n",
        "                # print(batch_signal.shape)\n",
        "                # print(batch_signal)\n",
        "                pred = model(batch_signal)\n",
        "                norm = 0\n",
        "\n",
        "                for parameter in model.parameters():\n",
        "                    norm += torch.norm(parameter, p=norm_square)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label) + beta * norm\n",
        "                #print('loss : ',loss.item())\n",
        "                # loss = loss_fn(pred, batch_label)\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                train_total_loss += loss.item()\n",
        "\n",
        "                train_total_count += check_count\n",
        "                train_total_data += len(batch_signal)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "                count = 0\n",
        "\n",
        "        train_total_loss /= train_batch_size\n",
        "        train_accuracy = train_total_count / train_total_data * 100\n",
        "\n",
        "        output_str = 'train dataset : %d/%d epochs spend time : %.4f sec / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, train_total_loss,\n",
        "                        train_total_count, train_total_data, train_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "    \n",
        "\n",
        "        #check test dataset\n",
        "        start_time = time.time()\n",
        "        for file_name in test_dataset_list:\n",
        "            batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(test_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=False,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "            batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "\n",
        "            batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred = model(batch_signal)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label)\n",
        "\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                test_total_loss += loss.item()\n",
        "                test_total_count += check_count\n",
        "                test_total_data += len(batch_signal)\n",
        "\n",
        "                # 사용하지 않는 변수 제거\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "        test_total_loss /= test_batch_size\n",
        "        test_accuracy = test_total_count / test_total_data * 100\n",
        "\n",
        "\n",
        "\n",
        "        output_str = 'test dataset : %d/%d epochs spend time : %.4f sec  / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, test_total_loss,\n",
        "                        test_total_count, test_total_data, test_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "        scheduler.step(float(test_total_loss))\n",
        "        #scheduler.step()\n",
        "\n",
        "        if epoch == 0:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_epoch = epoch\n",
        "            save_file = save_filename\n",
        "            #save_file = save_path + 'best_SleepEEGNet_CNN_channel%d.pth'%channel\n",
        "            torch.save(model.state_dict(),save_file)\n",
        "            stop_count = 0\n",
        "        else:\n",
        "            if best_accuracy < test_accuracy:\n",
        "                best_accuracy = test_accuracy\n",
        "                best_epoch = epoch\n",
        "                save_file = save_filename\n",
        "                torch.save(model.state_dict(), save_file)\n",
        "                stop_count = 0\n",
        "            else:\n",
        "                stop_count += 1\n",
        "        if stop_count > 30:\n",
        "            print('Early Stopping')\n",
        "            break\n",
        "\n",
        "        output_str = 'best epoch : %d/%d / val accuracy : %f%%\\n' \\\n",
        "                     % (best_epoch+1, epochs, best_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        print('=' * 30)\n",
        "\n",
        "\n",
        "    output_str = 'best epoch : %d/%d / accuracy : %f%%\\n' \\\n",
        "                 % (best_epoch+1, epochs, best_accuracy)\n",
        "    sys.stdout.write(output_str)\n",
        "    check_file.write(output_str)\n",
        "    print('=' * 30)\n",
        "\n",
        "    check_file.close()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-qN1b_NBMC1",
        "outputId": "70dc4ea0-2492-49c8-d98e-84fc801fdb3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "save_filename = './train.pth'\n",
        "logging_filename = './logging.txt'\n",
        "train_signal_dir = 'signals/npy/Fpz-Cz/remove_wake/train/'\n",
        "test_signal_dir = 'signals/npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_dir = 'annotations/npy/remove_wake/'\n",
        "train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, \n",
        "                           test_signal_dir,annotations_dir,\n",
        "                           epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                           layer_filters=[64,128,256,512],first_conv=[200,40,100],\n",
        "                           optim='Adam',lf='CE',epsilon=0,noise_scale=0,\n",
        "                           min_value=-0,max_value=0,preprocessing=True,\n",
        "                           norm_methods='Standard',use_noise=False,loss_type='softmax')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preproceesing  :  True\n",
            "min/max value : 0.000000/0.000000\n",
            "noise scale :  0\n",
            "loss function :  CE\n",
            "epsilon :  0\n",
            "norm methods :  Standard\n",
            "logging file name :  ./logging.txt\n",
            "save file name :  ./train.pth\n",
            "layer filters :  [64, 128, 256, 512]\n",
            "fisrt_conv info :  [200, 40, 100]\n",
            "loss type :  softmax\n",
            "training data oversampling noise :  False\n",
            "train_dataset length :  36\n",
            "['SC4082EP-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4031EC-Hypnogram.npy']\n",
            "test_dataset length :  15\n",
            "['SC4182EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy']\n",
            "can use CUDA!!!\n",
            "torch.cuda.device_count() :  1\n",
            "Optimizer : Adam\n",
            "train_batch_size :  8\n",
            "current_lr : 0.001000\n",
            "train dataset : 1/2000 epochs spend time : 1.7205 sec / total_loss : 1.6525 correct : 11337/35375 -> 32.0481%\n",
            "test dataset : 1/2000 epochs spend time : 0.6554 sec  / total_loss : 1.4488 correct : 7333/15413 -> 47.5767%\n",
            "best epoch : 1/2000 / val accuracy : 47.576721%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 2/2000 epochs spend time : 1.7473 sec / total_loss : 1.4756 correct : 15694/35375 -> 44.3647%\n",
            "test dataset : 2/2000 epochs spend time : 0.6514 sec  / total_loss : 1.3689 correct : 7340/15413 -> 47.6221%\n",
            "best epoch : 2/2000 / val accuracy : 47.622137%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 3/2000 epochs spend time : 1.8025 sec / total_loss : 1.4089 correct : 15699/35375 -> 44.3788%\n",
            "test dataset : 3/2000 epochs spend time : 0.6503 sec  / total_loss : 1.5167 correct : 3890/15413 -> 25.2384%\n",
            "best epoch : 2/2000 / val accuracy : 47.622137%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 4/2000 epochs spend time : 1.7274 sec / total_loss : 1.3695 correct : 16434/35375 -> 46.4565%\n",
            "test dataset : 4/2000 epochs spend time : 0.6535 sec  / total_loss : 1.2642 correct : 7087/15413 -> 45.9807%\n",
            "best epoch : 2/2000 / val accuracy : 47.622137%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 5/2000 epochs spend time : 1.7561 sec / total_loss : 1.3013 correct : 17810/35375 -> 50.3463%\n",
            "test dataset : 5/2000 epochs spend time : 0.6322 sec  / total_loss : 1.5376 correct : 3880/15413 -> 25.1736%\n",
            "best epoch : 2/2000 / val accuracy : 47.622137%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 6/2000 epochs spend time : 1.6961 sec / total_loss : 1.3209 correct : 16961/35375 -> 47.9463%\n",
            "test dataset : 6/2000 epochs spend time : 0.6449 sec  / total_loss : 1.3474 correct : 5449/15413 -> 35.3533%\n",
            "best epoch : 2/2000 / val accuracy : 47.622137%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 7/2000 epochs spend time : 1.7145 sec / total_loss : 1.2143 correct : 19283/35375 -> 54.5102%\n",
            "test dataset : 7/2000 epochs spend time : 0.6505 sec  / total_loss : 1.0556 correct : 8350/15413 -> 54.1750%\n",
            "best epoch : 7/2000 / val accuracy : 54.175047%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 8/2000 epochs spend time : 1.7398 sec / total_loss : 1.1508 correct : 19012/35375 -> 53.7442%\n",
            "test dataset : 8/2000 epochs spend time : 0.6437 sec  / total_loss : 1.1853 correct : 7426/15413 -> 48.1801%\n",
            "best epoch : 7/2000 / val accuracy : 54.175047%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 9/2000 epochs spend time : 1.7342 sec / total_loss : 1.0872 correct : 20640/35375 -> 58.3463%\n",
            "test dataset : 9/2000 epochs spend time : 0.6336 sec  / total_loss : 1.0765 correct : 8778/15413 -> 56.9519%\n",
            "best epoch : 9/2000 / val accuracy : 56.951924%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 10/2000 epochs spend time : 1.7320 sec / total_loss : 1.1655 correct : 19635/35375 -> 55.5053%\n",
            "test dataset : 10/2000 epochs spend time : 0.6492 sec  / total_loss : 0.9766 correct : 9628/15413 -> 62.4667%\n",
            "best epoch : 10/2000 / val accuracy : 62.466749%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 11/2000 epochs spend time : 1.7706 sec / total_loss : 1.0911 correct : 22082/35375 -> 62.4226%\n",
            "test dataset : 11/2000 epochs spend time : 0.6434 sec  / total_loss : 1.2784 correct : 8737/15413 -> 56.6859%\n",
            "best epoch : 10/2000 / val accuracy : 62.466749%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 12/2000 epochs spend time : 1.7234 sec / total_loss : 1.1936 correct : 19860/35375 -> 56.1413%\n",
            "test dataset : 12/2000 epochs spend time : 0.6849 sec  / total_loss : 1.0466 correct : 8659/15413 -> 56.1798%\n",
            "best epoch : 10/2000 / val accuracy : 62.466749%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 13/2000 epochs spend time : 1.7495 sec / total_loss : 1.0488 correct : 21147/35375 -> 59.7795%\n",
            "test dataset : 13/2000 epochs spend time : 0.6577 sec  / total_loss : 1.1074 correct : 8363/15413 -> 54.2594%\n",
            "best epoch : 10/2000 / val accuracy : 62.466749%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 14/2000 epochs spend time : 1.7219 sec / total_loss : 0.9743 correct : 22134/35375 -> 62.5696%\n",
            "test dataset : 14/2000 epochs spend time : 0.6342 sec  / total_loss : 0.7996 correct : 10531/15413 -> 68.3254%\n",
            "best epoch : 14/2000 / val accuracy : 68.325440%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 15/2000 epochs spend time : 1.7759 sec / total_loss : 0.9155 correct : 23212/35375 -> 65.6170%\n",
            "test dataset : 15/2000 epochs spend time : 0.6474 sec  / total_loss : 0.9645 correct : 9362/15413 -> 60.7409%\n",
            "best epoch : 14/2000 / val accuracy : 68.325440%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 16/2000 epochs spend time : 1.6953 sec / total_loss : 0.8332 correct : 25085/35375 -> 70.9117%\n",
            "test dataset : 16/2000 epochs spend time : 0.6457 sec  / total_loss : 1.4502 correct : 6794/15413 -> 44.0797%\n",
            "best epoch : 14/2000 / val accuracy : 68.325440%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 17/2000 epochs spend time : 1.7788 sec / total_loss : 0.9165 correct : 23349/35375 -> 66.0042%\n",
            "test dataset : 17/2000 epochs spend time : 0.6593 sec  / total_loss : 1.7274 correct : 7009/15413 -> 45.4746%\n",
            "best epoch : 14/2000 / val accuracy : 68.325440%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 18/2000 epochs spend time : 1.7587 sec / total_loss : 1.5014 correct : 16301/35375 -> 46.0806%\n",
            "test dataset : 18/2000 epochs spend time : 0.6605 sec  / total_loss : 0.9883 correct : 9424/15413 -> 61.1432%\n",
            "best epoch : 14/2000 / val accuracy : 68.325440%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 19/2000 epochs spend time : 1.7089 sec / total_loss : 0.9251 correct : 23479/35375 -> 66.3717%\n",
            "test dataset : 19/2000 epochs spend time : 0.6530 sec  / total_loss : 0.8914 correct : 10188/15413 -> 66.1000%\n",
            "best epoch : 14/2000 / val accuracy : 68.325440%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 20/2000 epochs spend time : 1.6846 sec / total_loss : 0.7886 correct : 24907/35375 -> 70.4085%\n",
            "test dataset : 20/2000 epochs spend time : 0.6364 sec  / total_loss : 0.7477 correct : 11337/15413 -> 73.5548%\n",
            "best epoch : 20/2000 / val accuracy : 73.554791%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 21/2000 epochs spend time : 1.7520 sec / total_loss : 0.7862 correct : 24893/35375 -> 70.3689%\n",
            "test dataset : 21/2000 epochs spend time : 0.6447 sec  / total_loss : 0.8330 correct : 10479/15413 -> 67.9881%\n",
            "best epoch : 20/2000 / val accuracy : 73.554791%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 22/2000 epochs spend time : 1.7157 sec / total_loss : 0.6037 correct : 27930/35375 -> 78.9541%\n",
            "test dataset : 22/2000 epochs spend time : 0.6350 sec  / total_loss : 0.7721 correct : 11418/15413 -> 74.0803%\n",
            "best epoch : 22/2000 / val accuracy : 74.080322%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 23/2000 epochs spend time : 1.7474 sec / total_loss : 0.5627 correct : 28559/35375 -> 80.7322%\n",
            "test dataset : 23/2000 epochs spend time : 0.6568 sec  / total_loss : 0.7160 correct : 11593/15413 -> 75.2157%\n",
            "best epoch : 23/2000 / val accuracy : 75.215727%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 24/2000 epochs spend time : 1.7375 sec / total_loss : 0.5104 correct : 29243/35375 -> 82.6657%\n",
            "test dataset : 24/2000 epochs spend time : 0.6290 sec  / total_loss : 1.1183 correct : 10550/15413 -> 68.4487%\n",
            "best epoch : 23/2000 / val accuracy : 75.215727%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 25/2000 epochs spend time : 1.6819 sec / total_loss : 0.8417 correct : 24843/35375 -> 70.2276%\n",
            "test dataset : 25/2000 epochs spend time : 0.6266 sec  / total_loss : 0.6564 correct : 12027/15413 -> 78.0315%\n",
            "best epoch : 25/2000 / val accuracy : 78.031532%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 26/2000 epochs spend time : 1.7078 sec / total_loss : 0.4459 correct : 30278/35375 -> 85.5915%\n",
            "test dataset : 26/2000 epochs spend time : 0.6423 sec  / total_loss : 0.6660 correct : 12219/15413 -> 79.2772%\n",
            "best epoch : 26/2000 / val accuracy : 79.277234%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 27/2000 epochs spend time : 1.7022 sec / total_loss : 0.5017 correct : 29388/35375 -> 83.0756%\n",
            "test dataset : 27/2000 epochs spend time : 0.6652 sec  / total_loss : 0.5925 correct : 12419/15413 -> 80.5748%\n",
            "best epoch : 27/2000 / val accuracy : 80.574839%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 28/2000 epochs spend time : 1.7434 sec / total_loss : 0.3983 correct : 31255/35375 -> 88.3534%\n",
            "test dataset : 28/2000 epochs spend time : 0.6250 sec  / total_loss : 0.8553 correct : 11960/15413 -> 77.5968%\n",
            "best epoch : 27/2000 / val accuracy : 80.574839%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 29/2000 epochs spend time : 1.6875 sec / total_loss : 0.3539 correct : 31931/35375 -> 90.2643%\n",
            "test dataset : 29/2000 epochs spend time : 0.6454 sec  / total_loss : 0.7398 correct : 12406/15413 -> 80.4905%\n",
            "best epoch : 27/2000 / val accuracy : 80.574839%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 30/2000 epochs spend time : 1.7594 sec / total_loss : 1.2430 correct : 22869/35375 -> 64.6473%\n",
            "test dataset : 30/2000 epochs spend time : 0.6641 sec  / total_loss : 1.0048 correct : 9249/15413 -> 60.0078%\n",
            "best epoch : 27/2000 / val accuracy : 80.574839%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 31/2000 epochs spend time : 1.7282 sec / total_loss : 0.6521 correct : 27477/35375 -> 77.6735%\n",
            "test dataset : 31/2000 epochs spend time : 0.6863 sec  / total_loss : 0.6820 correct : 12121/15413 -> 78.6414%\n",
            "best epoch : 27/2000 / val accuracy : 80.574839%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 32/2000 epochs spend time : 1.7633 sec / total_loss : 0.5960 correct : 28270/35375 -> 79.9152%\n",
            "test dataset : 32/2000 epochs spend time : 0.6642 sec  / total_loss : 0.5832 correct : 12956/15413 -> 84.0589%\n",
            "best epoch : 32/2000 / val accuracy : 84.058911%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 33/2000 epochs spend time : 1.7669 sec / total_loss : 0.2907 correct : 32855/35375 -> 92.8763%\n",
            "test dataset : 33/2000 epochs spend time : 0.6730 sec  / total_loss : 0.7090 correct : 12809/15413 -> 83.1052%\n",
            "best epoch : 32/2000 / val accuracy : 84.058911%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 34/2000 epochs spend time : 1.7718 sec / total_loss : 0.3435 correct : 32182/35375 -> 90.9739%\n",
            "test dataset : 34/2000 epochs spend time : 0.6375 sec  / total_loss : 0.6795 correct : 12673/15413 -> 82.2228%\n",
            "best epoch : 32/2000 / val accuracy : 84.058911%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 35/2000 epochs spend time : 1.7416 sec / total_loss : 0.4505 correct : 32527/35375 -> 91.9491%\n",
            "test dataset : 35/2000 epochs spend time : 0.6319 sec  / total_loss : 0.6815 correct : 12813/15413 -> 83.1311%\n",
            "best epoch : 32/2000 / val accuracy : 84.058911%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 36/2000 epochs spend time : 1.6765 sec / total_loss : 0.5610 correct : 29939/35375 -> 84.6332%\n",
            "test dataset : 36/2000 epochs spend time : 0.6605 sec  / total_loss : 0.8273 correct : 11538/15413 -> 74.8589%\n",
            "best epoch : 32/2000 / val accuracy : 84.058911%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 37/2000 epochs spend time : 1.6892 sec / total_loss : 0.3739 correct : 31979/35375 -> 90.4000%\n",
            "test dataset : 37/2000 epochs spend time : 0.6403 sec  / total_loss : 0.5067 correct : 13468/15413 -> 87.3808%\n",
            "best epoch : 37/2000 / val accuracy : 87.380782%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 38/2000 epochs spend time : 1.7108 sec / total_loss : 0.2154 correct : 33961/35375 -> 96.0028%\n",
            "test dataset : 38/2000 epochs spend time : 0.6802 sec  / total_loss : 0.5572 correct : 13440/15413 -> 87.1991%\n",
            "best epoch : 37/2000 / val accuracy : 87.380782%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 39/2000 epochs spend time : 1.6929 sec / total_loss : 0.1516 correct : 34669/35375 -> 98.0042%\n",
            "test dataset : 39/2000 epochs spend time : 0.6523 sec  / total_loss : 0.7423 correct : 13433/15413 -> 87.1537%\n",
            "best epoch : 37/2000 / val accuracy : 87.380782%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 40/2000 epochs spend time : 1.7310 sec / total_loss : 0.1813 correct : 34233/35375 -> 96.7717%\n",
            "test dataset : 40/2000 epochs spend time : 0.6550 sec  / total_loss : 0.6565 correct : 13557/15413 -> 87.9582%\n",
            "best epoch : 40/2000 / val accuracy : 87.958217%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 41/2000 epochs spend time : 1.7141 sec / total_loss : 0.1378 correct : 34771/35375 -> 98.2926%\n",
            "test dataset : 41/2000 epochs spend time : 0.6334 sec  / total_loss : 0.6921 correct : 13560/15413 -> 87.9777%\n",
            "best epoch : 41/2000 / val accuracy : 87.977681%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 42/2000 epochs spend time : 1.7062 sec / total_loss : 0.1247 correct : 34929/35375 -> 98.7392%\n",
            "test dataset : 42/2000 epochs spend time : 0.6241 sec  / total_loss : 0.8281 correct : 13471/15413 -> 87.4002%\n",
            "best epoch : 41/2000 / val accuracy : 87.977681%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 43/2000 epochs spend time : 1.7286 sec / total_loss : 0.1714 correct : 34397/35375 -> 97.2353%\n",
            "test dataset : 43/2000 epochs spend time : 0.6251 sec  / total_loss : 0.7207 correct : 13424/15413 -> 87.0953%\n",
            "best epoch : 41/2000 / val accuracy : 87.977681%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 44/2000 epochs spend time : 1.6812 sec / total_loss : 0.1382 correct : 34744/35375 -> 98.2163%\n",
            "test dataset : 44/2000 epochs spend time : 0.6436 sec  / total_loss : 0.8687 correct : 13196/15413 -> 85.6160%\n",
            "best epoch : 41/2000 / val accuracy : 87.977681%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 45/2000 epochs spend time : 1.7467 sec / total_loss : 0.9445 correct : 27805/35375 -> 78.6007%\n",
            "test dataset : 45/2000 epochs spend time : 0.6364 sec  / total_loss : 0.7495 correct : 11682/15413 -> 75.7932%\n",
            "best epoch : 41/2000 / val accuracy : 87.977681%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 46/2000 epochs spend time : 1.6738 sec / total_loss : 0.4137 correct : 31651/35375 -> 89.4728%\n",
            "test dataset : 46/2000 epochs spend time : 0.6469 sec  / total_loss : 0.6692 correct : 12507/15413 -> 81.1458%\n",
            "best epoch : 41/2000 / val accuracy : 87.977681%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 47/2000 epochs spend time : 1.6875 sec / total_loss : 0.2894 correct : 32745/35375 -> 92.5654%\n",
            "test dataset : 47/2000 epochs spend time : 0.6435 sec  / total_loss : 0.5206 correct : 13491/15413 -> 87.5300%\n",
            "best epoch : 41/2000 / val accuracy : 87.977681%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 48/2000 epochs spend time : 1.6695 sec / total_loss : 0.1486 correct : 34750/35375 -> 98.2332%\n",
            "test dataset : 48/2000 epochs spend time : 0.6406 sec  / total_loss : 0.6191 correct : 13685/15413 -> 88.7887%\n",
            "best epoch : 48/2000 / val accuracy : 88.788685%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 49/2000 epochs spend time : 1.7507 sec / total_loss : 0.1180 correct : 35074/35375 -> 99.1491%\n",
            "test dataset : 49/2000 epochs spend time : 0.6297 sec  / total_loss : 0.5988 correct : 13844/15413 -> 89.8203%\n",
            "best epoch : 49/2000 / val accuracy : 89.820282%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 50/2000 epochs spend time : 1.6979 sec / total_loss : 0.0997 correct : 35293/35375 -> 99.7682%\n",
            "test dataset : 50/2000 epochs spend time : 0.6279 sec  / total_loss : 0.6617 correct : 13849/15413 -> 89.8527%\n",
            "best epoch : 50/2000 / val accuracy : 89.852722%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 51/2000 epochs spend time : 1.7231 sec / total_loss : 0.0954 correct : 35325/35375 -> 99.8587%\n",
            "test dataset : 51/2000 epochs spend time : 0.6428 sec  / total_loss : 0.6983 correct : 13861/15413 -> 89.9306%\n",
            "best epoch : 51/2000 / val accuracy : 89.930578%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 52/2000 epochs spend time : 1.7103 sec / total_loss : 0.0919 correct : 35348/35375 -> 99.9237%\n",
            "test dataset : 52/2000 epochs spend time : 0.6352 sec  / total_loss : 0.7286 correct : 13866/15413 -> 89.9630%\n",
            "best epoch : 52/2000 / val accuracy : 89.963018%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 53/2000 epochs spend time : 1.7086 sec / total_loss : 0.0918 correct : 35355/35375 -> 99.9435%\n",
            "test dataset : 53/2000 epochs spend time : 0.6664 sec  / total_loss : 0.7439 correct : 13861/15413 -> 89.9306%\n",
            "best epoch : 52/2000 / val accuracy : 89.963018%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 54/2000 epochs spend time : 1.6719 sec / total_loss : 0.0894 correct : 35361/35375 -> 99.9604%\n",
            "test dataset : 54/2000 epochs spend time : 0.6402 sec  / total_loss : 0.7818 correct : 13859/15413 -> 89.9176%\n",
            "best epoch : 52/2000 / val accuracy : 89.963018%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 55/2000 epochs spend time : 1.7459 sec / total_loss : 0.0889 correct : 35365/35375 -> 99.9717%\n",
            "test dataset : 55/2000 epochs spend time : 0.6700 sec  / total_loss : 0.8063 correct : 13854/15413 -> 89.8852%\n",
            "best epoch : 52/2000 / val accuracy : 89.963018%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 56/2000 epochs spend time : 1.6948 sec / total_loss : 0.0875 correct : 35369/35375 -> 99.9830%\n",
            "test dataset : 56/2000 epochs spend time : 0.6298 sec  / total_loss : 0.8127 correct : 13867/15413 -> 89.9695%\n",
            "best epoch : 56/2000 / val accuracy : 89.969506%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 57/2000 epochs spend time : 1.6949 sec / total_loss : 0.0870 correct : 35371/35375 -> 99.9887%\n",
            "test dataset : 57/2000 epochs spend time : 0.6189 sec  / total_loss : 0.8326 correct : 13863/15413 -> 89.9436%\n",
            "best epoch : 56/2000 / val accuracy : 89.969506%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 58/2000 epochs spend time : 1.6806 sec / total_loss : 0.0868 correct : 35371/35375 -> 99.9887%\n",
            "test dataset : 58/2000 epochs spend time : 0.6243 sec  / total_loss : 0.8475 correct : 13871/15413 -> 89.9955%\n",
            "best epoch : 58/2000 / val accuracy : 89.995458%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 59/2000 epochs spend time : 1.7074 sec / total_loss : 0.0864 correct : 35371/35375 -> 99.9887%\n",
            "test dataset : 59/2000 epochs spend time : 0.6431 sec  / total_loss : 0.8647 correct : 13867/15413 -> 89.9695%\n",
            "best epoch : 58/2000 / val accuracy : 89.995458%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 60/2000 epochs spend time : 1.7039 sec / total_loss : 0.0865 correct : 35371/35375 -> 99.9887%\n",
            "test dataset : 60/2000 epochs spend time : 0.6390 sec  / total_loss : 0.8683 correct : 13874/15413 -> 90.0149%\n",
            "best epoch : 60/2000 / val accuracy : 90.014922%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 61/2000 epochs spend time : 1.6900 sec / total_loss : 0.0861 correct : 35372/35375 -> 99.9915%\n",
            "test dataset : 61/2000 epochs spend time : 0.6212 sec  / total_loss : 0.8770 correct : 13871/15413 -> 89.9955%\n",
            "best epoch : 60/2000 / val accuracy : 90.014922%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 62/2000 epochs spend time : 1.6866 sec / total_loss : 0.0860 correct : 35372/35375 -> 99.9915%\n",
            "test dataset : 62/2000 epochs spend time : 0.6330 sec  / total_loss : 0.8827 correct : 13871/15413 -> 89.9955%\n",
            "best epoch : 60/2000 / val accuracy : 90.014922%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 63/2000 epochs spend time : 1.7016 sec / total_loss : 0.0864 correct : 35373/35375 -> 99.9943%\n",
            "test dataset : 63/2000 epochs spend time : 0.6487 sec  / total_loss : 0.8872 correct : 13873/15413 -> 90.0084%\n",
            "best epoch : 60/2000 / val accuracy : 90.014922%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 64/2000 epochs spend time : 1.6928 sec / total_loss : 0.0859 correct : 35374/35375 -> 99.9972%\n",
            "test dataset : 64/2000 epochs spend time : 0.6168 sec  / total_loss : 0.8922 correct : 13870/15413 -> 89.9890%\n",
            "best epoch : 60/2000 / val accuracy : 90.014922%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 65/2000 epochs spend time : 1.6837 sec / total_loss : 0.0858 correct : 35374/35375 -> 99.9972%\n",
            "test dataset : 65/2000 epochs spend time : 0.6522 sec  / total_loss : 0.8974 correct : 13872/15413 -> 90.0019%\n",
            "best epoch : 60/2000 / val accuracy : 90.014922%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 66/2000 epochs spend time : 1.6908 sec / total_loss : 0.0857 correct : 35374/35375 -> 99.9972%\n",
            "test dataset : 66/2000 epochs spend time : 0.6641 sec  / total_loss : 0.9012 correct : 13874/15413 -> 90.0149%\n",
            "best epoch : 60/2000 / val accuracy : 90.014922%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 67/2000 epochs spend time : 1.6791 sec / total_loss : 0.0856 correct : 35374/35375 -> 99.9972%\n",
            "test dataset : 67/2000 epochs spend time : 0.6073 sec  / total_loss : 0.9093 correct : 13876/15413 -> 90.0279%\n",
            "best epoch : 67/2000 / val accuracy : 90.027899%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 68/2000 epochs spend time : 1.6954 sec / total_loss : 0.0855 correct : 35373/35375 -> 99.9943%\n",
            "test dataset : 68/2000 epochs spend time : 0.6501 sec  / total_loss : 0.9148 correct : 13872/15413 -> 90.0019%\n",
            "best epoch : 67/2000 / val accuracy : 90.027899%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 69/2000 epochs spend time : 1.6636 sec / total_loss : 0.0855 correct : 35373/35375 -> 99.9943%\n",
            "test dataset : 69/2000 epochs spend time : 0.6364 sec  / total_loss : 0.9185 correct : 13875/15413 -> 90.0214%\n",
            "best epoch : 67/2000 / val accuracy : 90.027899%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 70/2000 epochs spend time : 1.7358 sec / total_loss : 0.0854 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 70/2000 epochs spend time : 0.6250 sec  / total_loss : 0.9219 correct : 13874/15413 -> 90.0149%\n",
            "best epoch : 67/2000 / val accuracy : 90.027899%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 71/2000 epochs spend time : 1.7237 sec / total_loss : 0.0854 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 71/2000 epochs spend time : 0.6221 sec  / total_loss : 0.9241 correct : 13876/15413 -> 90.0279%\n",
            "best epoch : 67/2000 / val accuracy : 90.027899%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 72/2000 epochs spend time : 1.6800 sec / total_loss : 0.0852 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 72/2000 epochs spend time : 0.6539 sec  / total_loss : 0.9261 correct : 13875/15413 -> 90.0214%\n",
            "best epoch : 67/2000 / val accuracy : 90.027899%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 73/2000 epochs spend time : 1.6930 sec / total_loss : 0.0852 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 73/2000 epochs spend time : 0.6380 sec  / total_loss : 0.9278 correct : 13876/15413 -> 90.0279%\n",
            "best epoch : 67/2000 / val accuracy : 90.027899%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 74/2000 epochs spend time : 1.7068 sec / total_loss : 0.0855 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 74/2000 epochs spend time : 0.6411 sec  / total_loss : 0.9295 correct : 13876/15413 -> 90.0279%\n",
            "best epoch : 67/2000 / val accuracy : 90.027899%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 75/2000 epochs spend time : 1.6982 sec / total_loss : 0.0851 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 75/2000 epochs spend time : 0.6359 sec  / total_loss : 0.9310 correct : 13876/15413 -> 90.0279%\n",
            "best epoch : 67/2000 / val accuracy : 90.027899%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 76/2000 epochs spend time : 1.6653 sec / total_loss : 0.0851 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 76/2000 epochs spend time : 0.6422 sec  / total_loss : 0.9328 correct : 13874/15413 -> 90.0149%\n",
            "best epoch : 67/2000 / val accuracy : 90.027899%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 77/2000 epochs spend time : 1.7257 sec / total_loss : 0.0851 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 77/2000 epochs spend time : 0.6311 sec  / total_loss : 0.9346 correct : 13876/15413 -> 90.0279%\n",
            "best epoch : 67/2000 / val accuracy : 90.027899%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 78/2000 epochs spend time : 1.7181 sec / total_loss : 0.0850 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 78/2000 epochs spend time : 0.6178 sec  / total_loss : 0.9364 correct : 13877/15413 -> 90.0344%\n",
            "best epoch : 78/2000 / val accuracy : 90.034387%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 79/2000 epochs spend time : 1.6826 sec / total_loss : 0.0852 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 79/2000 epochs spend time : 0.6779 sec  / total_loss : 0.9387 correct : 13876/15413 -> 90.0279%\n",
            "best epoch : 78/2000 / val accuracy : 90.034387%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 80/2000 epochs spend time : 1.7007 sec / total_loss : 0.0850 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 80/2000 epochs spend time : 0.6154 sec  / total_loss : 0.9405 correct : 13877/15413 -> 90.0344%\n",
            "best epoch : 78/2000 / val accuracy : 90.034387%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 81/2000 epochs spend time : 1.7606 sec / total_loss : 0.0849 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 81/2000 epochs spend time : 0.6604 sec  / total_loss : 0.9420 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 81/2000 / val accuracy : 90.040875%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 82/2000 epochs spend time : 1.6927 sec / total_loss : 0.0850 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 82/2000 epochs spend time : 0.6445 sec  / total_loss : 0.9431 correct : 13877/15413 -> 90.0344%\n",
            "best epoch : 81/2000 / val accuracy : 90.040875%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 83/2000 epochs spend time : 1.6798 sec / total_loss : 0.0848 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 83/2000 epochs spend time : 0.6130 sec  / total_loss : 0.9442 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 81/2000 / val accuracy : 90.040875%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 84/2000 epochs spend time : 1.7412 sec / total_loss : 0.0849 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 84/2000 epochs spend time : 0.6309 sec  / total_loss : 0.9451 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 81/2000 / val accuracy : 90.040875%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 85/2000 epochs spend time : 1.6512 sec / total_loss : 0.0848 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 85/2000 epochs spend time : 0.6406 sec  / total_loss : 0.9460 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 81/2000 / val accuracy : 90.040875%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 86/2000 epochs spend time : 1.6865 sec / total_loss : 0.0848 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 86/2000 epochs spend time : 0.6203 sec  / total_loss : 0.9465 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 81/2000 / val accuracy : 90.040875%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 87/2000 epochs spend time : 1.6822 sec / total_loss : 0.0848 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 87/2000 epochs spend time : 0.6272 sec  / total_loss : 0.9471 correct : 13877/15413 -> 90.0344%\n",
            "best epoch : 81/2000 / val accuracy : 90.040875%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 88/2000 epochs spend time : 1.6991 sec / total_loss : 0.0848 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 88/2000 epochs spend time : 0.6200 sec  / total_loss : 0.9481 correct : 13877/15413 -> 90.0344%\n",
            "best epoch : 81/2000 / val accuracy : 90.040875%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 89/2000 epochs spend time : 1.6519 sec / total_loss : 0.0848 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 89/2000 epochs spend time : 0.6265 sec  / total_loss : 0.9486 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 81/2000 / val accuracy : 90.040875%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 90/2000 epochs spend time : 1.6724 sec / total_loss : 0.0848 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 90/2000 epochs spend time : 0.6490 sec  / total_loss : 0.9492 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 81/2000 / val accuracy : 90.040875%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 91/2000 epochs spend time : 1.6841 sec / total_loss : 0.0848 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 91/2000 epochs spend time : 0.6222 sec  / total_loss : 0.9504 correct : 13879/15413 -> 90.0474%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 92/2000 epochs spend time : 1.7077 sec / total_loss : 0.0847 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 92/2000 epochs spend time : 0.6613 sec  / total_loss : 0.9514 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 93/2000 epochs spend time : 1.6525 sec / total_loss : 0.0847 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 93/2000 epochs spend time : 0.6211 sec  / total_loss : 0.9517 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 94/2000 epochs spend time : 1.7256 sec / total_loss : 0.0846 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 94/2000 epochs spend time : 0.6113 sec  / total_loss : 0.9522 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 95/2000 epochs spend time : 1.6838 sec / total_loss : 0.0848 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 95/2000 epochs spend time : 0.6190 sec  / total_loss : 0.9525 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 96/2000 epochs spend time : 1.6511 sec / total_loss : 0.0846 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 96/2000 epochs spend time : 0.6517 sec  / total_loss : 0.9529 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 97/2000 epochs spend time : 1.6811 sec / total_loss : 0.0846 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 97/2000 epochs spend time : 0.6229 sec  / total_loss : 0.9532 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 98/2000 epochs spend time : 1.6630 sec / total_loss : 0.0846 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 98/2000 epochs spend time : 0.6295 sec  / total_loss : 0.9538 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 99/2000 epochs spend time : 1.6937 sec / total_loss : 0.0846 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 99/2000 epochs spend time : 0.6240 sec  / total_loss : 0.9544 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 100/2000 epochs spend time : 1.7566 sec / total_loss : 0.0846 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 100/2000 epochs spend time : 0.6235 sec  / total_loss : 0.9549 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 101/2000 epochs spend time : 1.6841 sec / total_loss : 0.0846 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 101/2000 epochs spend time : 0.6229 sec  / total_loss : 0.9553 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 102/2000 epochs spend time : 1.6660 sec / total_loss : 0.0846 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 102/2000 epochs spend time : 0.6166 sec  / total_loss : 0.9556 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 103/2000 epochs spend time : 1.7042 sec / total_loss : 0.0847 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 103/2000 epochs spend time : 0.6809 sec  / total_loss : 0.9561 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000016\n",
            "train dataset : 104/2000 epochs spend time : 1.6425 sec / total_loss : 0.0846 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 104/2000 epochs spend time : 0.6158 sec  / total_loss : 0.9562 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000016\n",
            "train dataset : 105/2000 epochs spend time : 1.6608 sec / total_loss : 0.0846 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 105/2000 epochs spend time : 0.6420 sec  / total_loss : 0.9564 correct : 13878/15413 -> 90.0409%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000016\n",
            "train dataset : 106/2000 epochs spend time : 1.6512 sec / total_loss : 0.0846 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 106/2000 epochs spend time : 0.6132 sec  / total_loss : 0.9567 correct : 13879/15413 -> 90.0474%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000016\n",
            "train dataset : 107/2000 epochs spend time : 1.6548 sec / total_loss : 0.0846 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 107/2000 epochs spend time : 0.6327 sec  / total_loss : 0.9569 correct : 13879/15413 -> 90.0474%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000016\n",
            "train dataset : 108/2000 epochs spend time : 1.6892 sec / total_loss : 0.0846 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 108/2000 epochs spend time : 0.6396 sec  / total_loss : 0.9571 correct : 13879/15413 -> 90.0474%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000016\n",
            "train dataset : 109/2000 epochs spend time : 1.6790 sec / total_loss : 0.0846 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 109/2000 epochs spend time : 0.6423 sec  / total_loss : 0.9574 correct : 13879/15413 -> 90.0474%\n",
            "best epoch : 91/2000 / val accuracy : 90.047363%\n",
            "==============================\n",
            "current_lr : 0.000016\n",
            "train dataset : 110/2000 epochs spend time : 1.6809 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 110/2000 epochs spend time : 0.6198 sec  / total_loss : 0.9576 correct : 13880/15413 -> 90.0539%\n",
            "best epoch : 110/2000 / val accuracy : 90.053851%\n",
            "==============================\n",
            "current_lr : 0.000016\n",
            "train dataset : 111/2000 epochs spend time : 1.7074 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 111/2000 epochs spend time : 0.6175 sec  / total_loss : 0.9577 correct : 13880/15413 -> 90.0539%\n",
            "best epoch : 110/2000 / val accuracy : 90.053851%\n",
            "==============================\n",
            "current_lr : 0.000016\n",
            "train dataset : 112/2000 epochs spend time : 1.6661 sec / total_loss : 0.0846 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 112/2000 epochs spend time : 0.6260 sec  / total_loss : 0.9579 correct : 13880/15413 -> 90.0539%\n",
            "best epoch : 110/2000 / val accuracy : 90.053851%\n",
            "==============================\n",
            "current_lr : 0.000016\n",
            "train dataset : 113/2000 epochs spend time : 1.6955 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 113/2000 epochs spend time : 0.6235 sec  / total_loss : 0.9582 correct : 13879/15413 -> 90.0474%\n",
            "best epoch : 110/2000 / val accuracy : 90.053851%\n",
            "==============================\n",
            "current_lr : 0.000016\n",
            "train dataset : 114/2000 epochs spend time : 1.6626 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 114/2000 epochs spend time : 0.6394 sec  / total_loss : 0.9585 correct : 13879/15413 -> 90.0474%\n",
            "best epoch : 110/2000 / val accuracy : 90.053851%\n",
            "==============================\n",
            "current_lr : 0.000008\n",
            "train dataset : 115/2000 epochs spend time : 1.6559 sec / total_loss : 0.0847 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 115/2000 epochs spend time : 0.6179 sec  / total_loss : 0.9586 correct : 13879/15413 -> 90.0474%\n",
            "best epoch : 110/2000 / val accuracy : 90.053851%\n",
            "==============================\n",
            "current_lr : 0.000008\n",
            "train dataset : 116/2000 epochs spend time : 1.6647 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 116/2000 epochs spend time : 0.6197 sec  / total_loss : 0.9587 correct : 13880/15413 -> 90.0539%\n",
            "best epoch : 110/2000 / val accuracy : 90.053851%\n",
            "==============================\n",
            "current_lr : 0.000008\n",
            "train dataset : 117/2000 epochs spend time : 1.6636 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 117/2000 epochs spend time : 0.6156 sec  / total_loss : 0.9588 correct : 13882/15413 -> 90.0668%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000008\n",
            "train dataset : 118/2000 epochs spend time : 1.6980 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 118/2000 epochs spend time : 0.6322 sec  / total_loss : 0.9588 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000008\n",
            "train dataset : 119/2000 epochs spend time : 1.7136 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 119/2000 epochs spend time : 0.6297 sec  / total_loss : 0.9590 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000008\n",
            "train dataset : 120/2000 epochs spend time : 1.7286 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 120/2000 epochs spend time : 0.6501 sec  / total_loss : 0.9591 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000008\n",
            "train dataset : 121/2000 epochs spend time : 1.7143 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 121/2000 epochs spend time : 0.6230 sec  / total_loss : 0.9592 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000008\n",
            "train dataset : 122/2000 epochs spend time : 1.6866 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 122/2000 epochs spend time : 0.6278 sec  / total_loss : 0.9593 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000008\n",
            "train dataset : 123/2000 epochs spend time : 1.7219 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 123/2000 epochs spend time : 0.6404 sec  / total_loss : 0.9593 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000008\n",
            "train dataset : 124/2000 epochs spend time : 1.6855 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 124/2000 epochs spend time : 0.6166 sec  / total_loss : 0.9594 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000008\n",
            "train dataset : 125/2000 epochs spend time : 1.6919 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 125/2000 epochs spend time : 0.6664 sec  / total_loss : 0.9594 correct : 13880/15413 -> 90.0539%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000004\n",
            "train dataset : 126/2000 epochs spend time : 1.6602 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 126/2000 epochs spend time : 0.6238 sec  / total_loss : 0.9595 correct : 13880/15413 -> 90.0539%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000004\n",
            "train dataset : 127/2000 epochs spend time : 1.6623 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 127/2000 epochs spend time : 0.6261 sec  / total_loss : 0.9596 correct : 13880/15413 -> 90.0539%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000004\n",
            "train dataset : 128/2000 epochs spend time : 1.7316 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 128/2000 epochs spend time : 0.6139 sec  / total_loss : 0.9596 correct : 13880/15413 -> 90.0539%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000004\n",
            "train dataset : 129/2000 epochs spend time : 1.7089 sec / total_loss : 0.0847 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 129/2000 epochs spend time : 0.6278 sec  / total_loss : 0.9597 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000004\n",
            "train dataset : 130/2000 epochs spend time : 1.6828 sec / total_loss : 0.0847 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 130/2000 epochs spend time : 0.6343 sec  / total_loss : 0.9597 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000004\n",
            "train dataset : 131/2000 epochs spend time : 1.6616 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 131/2000 epochs spend time : 0.6330 sec  / total_loss : 0.9598 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000004\n",
            "train dataset : 132/2000 epochs spend time : 1.7087 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 132/2000 epochs spend time : 0.6117 sec  / total_loss : 0.9598 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000004\n",
            "train dataset : 133/2000 epochs spend time : 1.6568 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 133/2000 epochs spend time : 0.6241 sec  / total_loss : 0.9598 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000004\n",
            "train dataset : 134/2000 epochs spend time : 1.6850 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 134/2000 epochs spend time : 0.6347 sec  / total_loss : 0.9599 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000004\n",
            "train dataset : 135/2000 epochs spend time : 1.6587 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 135/2000 epochs spend time : 0.6309 sec  / total_loss : 0.9600 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000004\n",
            "train dataset : 136/2000 epochs spend time : 1.7003 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 136/2000 epochs spend time : 0.6522 sec  / total_loss : 0.9601 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000002\n",
            "train dataset : 137/2000 epochs spend time : 1.6805 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 137/2000 epochs spend time : 0.6311 sec  / total_loss : 0.9601 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000002\n",
            "train dataset : 138/2000 epochs spend time : 1.6537 sec / total_loss : 0.0846 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 138/2000 epochs spend time : 0.6343 sec  / total_loss : 0.9601 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000002\n",
            "train dataset : 139/2000 epochs spend time : 1.6737 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 139/2000 epochs spend time : 0.6152 sec  / total_loss : 0.9602 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000002\n",
            "train dataset : 140/2000 epochs spend time : 1.6702 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 140/2000 epochs spend time : 0.6257 sec  / total_loss : 0.9602 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000002\n",
            "train dataset : 141/2000 epochs spend time : 1.7451 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 141/2000 epochs spend time : 0.6281 sec  / total_loss : 0.9602 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000002\n",
            "train dataset : 142/2000 epochs spend time : 1.6791 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 142/2000 epochs spend time : 0.6259 sec  / total_loss : 0.9602 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000002\n",
            "train dataset : 143/2000 epochs spend time : 1.7006 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 143/2000 epochs spend time : 0.6320 sec  / total_loss : 0.9603 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000002\n",
            "train dataset : 144/2000 epochs spend time : 1.6717 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 144/2000 epochs spend time : 0.6409 sec  / total_loss : 0.9603 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000002\n",
            "train dataset : 145/2000 epochs spend time : 1.6903 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 145/2000 epochs spend time : 0.6297 sec  / total_loss : 0.9604 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000002\n",
            "train dataset : 146/2000 epochs spend time : 1.6793 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 146/2000 epochs spend time : 0.6022 sec  / total_loss : 0.9604 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000002\n",
            "train dataset : 147/2000 epochs spend time : 1.6469 sec / total_loss : 0.0844 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 147/2000 epochs spend time : 0.6465 sec  / total_loss : 0.9604 correct : 13881/15413 -> 90.0603%\n",
            "best epoch : 117/2000 / val accuracy : 90.066827%\n",
            "==============================\n",
            "current_lr : 0.000001\n",
            "train dataset : 148/2000 epochs spend time : 1.6661 sec / total_loss : 0.0845 correct : 35375/35375 -> 100.0000%\n",
            "test dataset : 148/2000 epochs spend time : 0.6346 sec  / total_loss : 0.9604 correct : 13881/15413 -> 90.0603%\n",
            "Early Stopping\n",
            "best epoch : 117/2000 / accuracy : 90.066827%\n",
            "==============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCE73qoRBUC2"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}