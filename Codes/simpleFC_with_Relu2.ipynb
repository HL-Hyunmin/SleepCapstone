{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SleepDataPreparation_and_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/SleepCapstone/blob/master/Codes/simpleFC_with_Relu2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAGJRSb3xOT1"
      },
      "source": [
        "한림대학교 소프트웨어 융합대학\n",
        "빅데이터 캡스톤 프로젝트\n",
        "\n",
        "딥러닝에 기반한 수면 분류 모델 개발"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SaQVdPHgfmx",
        "outputId": "6dec0213-fe3d-4de1-e932-60961096cb2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohoEZomKw0cz",
        "outputId": "84599922-d567-435e-fdd4-8408b3988fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd drive/My\\ Drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvna-5vRzmTQ",
        "outputId": "f41ecb6f-e1c8-4f33-f2ff-9cdacdb3a862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "# GPU 가용성 체크\n",
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Oct 16 11:58:51 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1zNHBNc4beq"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def search_signals_npy(dirname):\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for file in filenames if file.endswith(\".npy\")]\n",
        "    return filenames\n",
        "\n",
        "def search_correct_signals_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vSGY6sO9vgC",
        "outputId": "8e1251bc-aa28-4106-819c-cdf9b19fac2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "\n",
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  'annotations/npy/remove_wake/'\n",
        "signals_path = 'signals/npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "total_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list:\n",
        "    label = np.load(path + filename)\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    \n",
        "    signals = np.load(signals_path+signals_filename)\n",
        "    \n",
        "    #print('remove start index : %d / remove end index : %d'%(remove_start_index,remove_end_index))\n",
        "    #print(np.bincount(label,minlength=6))\n",
        "    if len(label) !=len(signals[0])//30//fs:\n",
        "        print('file is fault!!!')\n",
        "    for i in range(6):\n",
        "        total_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "print(total_label)\n",
        "\n",
        "x = np.arange(len(total_label))\n",
        "\n",
        "plt.bar(x,total_label,width=0.7)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4336  2804 17799  5703  7717    61]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 6 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVVZuGLx-ADW",
        "outputId": "40a1cb7b-f170-4f0c-cc96-3c110d412eb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  'annotations/npy/remove_wake/'\n",
        "signals_path = 'signals/npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "random.shuffle(annotations_npy_list)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "trainDataset_count = 30\n",
        "testDataset_count = len(annotations_npy_list)-trainDataset_count\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4001EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4192EV-Hypnogram.npy']\n",
            "['SC4161EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy']\n",
            "[10.91263731  7.16558875 46.47024604 14.62546624 20.67891729  0.14714437]\n",
            "[12.47145808  7.71990867 45.87365445 15.53767533 18.20158747  0.19571599]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAKH9Mb0_NF3"
      },
      "source": [
        "signals_path = 'signals/npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "save_train_path = 'signals/npy//Fpz-Cz/remove_wake/train/'\n",
        "save_test_path = 'signals/npy/Fpz-Cz/remove_wake/test/'\n",
        "\n",
        "os.makedirs(save_train_path,exist_ok=True)\n",
        "os.makedirs(save_test_path,exist_ok=True)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_train_path+filename)\n",
        "    \n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_test_path+filename)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4EYtlUH-e9f",
        "outputId": "43afd9db-0f61-473f-983e-a3476715d1e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "def search_correct_annotations_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename\n",
        "\n",
        "train_path =  'signals/npy/Fpz-Cz/remove_wake/train/'\n",
        "test_path = 'signals/npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_path = 'annotations/npy/remove_wake/'\n",
        "\n",
        "train_list = search_signals_npy(train_path)\n",
        "test_list = search_signals_npy(test_path)\n",
        "\n",
        "print(train_list)\n",
        "print(test_list)\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in train_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in test_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4161EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4151EC-Hypnogram.npy']\n",
            "['SC4082EP-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy']\n",
            "[10.91263731  7.16558875 46.47024604 14.62546624 20.67891729  0.14714437]\n",
            "[12.47145808  7.71990867 45.87365445 15.53767533 18.20158747  0.19571599]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBwqVERu_aw-"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pylab as plt\n",
        "from  torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSQaEXQr_h8m"
      },
      "source": [
        "def data_preprocessing_torch(signals): # 하나의 데이터셋에 대한 data_preprocessing (using torch)\n",
        "    signals = (signals - signals.mean(dim=1).unsqueeze(1))/signals.std(dim=1).unsqueeze(1)\n",
        "\n",
        "    return signals\n",
        "\n",
        "def data_preprocessing_oneToOne_torch(signals,min,max,max_value):\n",
        "    signals_std = (signals + max_value) / (2*max_value)\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def data_preprocessing_minmax_torch(signals,min,max):\n",
        "    signals_std = (signals - signals.min(dim=1).unsqueeze(1)) / (\n",
        "            signals.max(dim=1).unsqueeze(1) - signals.min(dim=1).unsqueeze(1))\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def get_dataset_one_channel_norm_withoutCut(dirname,annotations_dir,data_path,use_noise=True,epsilon=0.5,noise_scale=2e-6,preprocessing=True,norm_methods='Standard'):\n",
        "    # npy read!\n",
        "    path = dirname + data_path\n",
        "    signals = np.load(path)\n",
        "\n",
        "    signals = torch.from_numpy(signals).float().to(device)\n",
        "\n",
        "    if use_noise:\n",
        "        if np.random.rand() < epsilon:\n",
        "            # noise = np.random.normal(loc=0,scale=noise_scale,size=signals.shape)\n",
        "            # signals = signals + noise\n",
        "            noise = torch.normal(mean=0., std=noise_scale, size=signals.shape).to(device)\n",
        "            signals = signals + noise\n",
        "\n",
        "    if preprocessing:\n",
        "        if norm_methods == 'Standard':\n",
        "            signals = data_preprocessing_torch(signals)\n",
        "        elif norm_methods == 'minmax':\n",
        "            signals = data_preprocessing_minmax_torch(signals,0,1)\n",
        "        elif norm_methods == 'oneToOne':\n",
        "            signals = data_preprocessing_oneToOne_torch(signals,-1,1,1e-4)\n",
        "    label = get_annotations(annotations_dir, data_path)\n",
        "    return signals, label\n",
        "\n",
        "# model conv layer weight init function\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:         # Conv weight init\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "        \n",
        "def suffle_dataset_list(dataset_list): # 데이터 셔플\n",
        "    random.shuffle(dataset_list)\n",
        "    return dataset_list\n",
        "\n",
        "\n",
        "# npy파일을 통해 label을 가져오는 함수\n",
        "def get_annotations(label_dir,file_name):\n",
        "    label_path = label_dir + file_name\n",
        "    label = np.load(label_path)\n",
        "    return label\n",
        "\n",
        "def signals_expand_torch_one_channel(signals): # 2차원 데이터를 3차원으로 변환 (8,N) -> (batch,8,6000) 형태로\n",
        "    signals = signals.expand(1,1,-1)\n",
        "    #print(signals.shape)\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    signals = signals.reshape(-1, 3000, 1) # 형태 변환\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    return signals"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx4M7Rfq_lwo"
      },
      "source": [
        "class DeepSleepNet_Classification(nn.Module):  # input channel = 8channel / output = 5\n",
        "    def __init__(self,in_channel=1,out_channel=6,layer=[64,128,128,128],sample_rate = 100):\n",
        "        super(DeepSleepNet_Classification, self).__init__()\n",
        "\n",
        "        \n",
        "        self.fc1 = nn.Linear(3000,1024)\n",
        "        self.fc2 = nn.Linear(1024,1024)\n",
        "        self.fc3 = nn.Linear(1024,512)\n",
        "        self.fc4 = nn.Linear(512,512)\n",
        "        self.fc5 = nn.Linear(512,256)        \n",
        "        self.fc6 = nn.Linear(256, out_channel)\n",
        "\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = torch.flatten(input, 1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.dropout(out)        \n",
        "        out = self.fc3(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc4(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.dropout(out)        \n",
        "        out = self.fc5(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.dropout(out)        \n",
        "        out = self.fc6(out)        \n",
        "\n",
        "        return out"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYZ6YSZHArdJ",
        "outputId": "732c8f97-16f3-4d63-c303-7cb2eb05649e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "model = DeepSleepNet_Classification(in_channel=1,out_channel=6)\n",
        "summary(model.cuda(),(1,3000))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 1024]       3,073,024\n",
            "              ReLU-2                 [-1, 1024]               0\n",
            "           Dropout-3                 [-1, 1024]               0\n",
            "            Linear-4                 [-1, 1024]       1,049,600\n",
            "              ReLU-5                 [-1, 1024]               0\n",
            "           Dropout-6                 [-1, 1024]               0\n",
            "            Linear-7                  [-1, 512]         524,800\n",
            "              ReLU-8                  [-1, 512]               0\n",
            "           Dropout-9                  [-1, 512]               0\n",
            "           Linear-10                  [-1, 512]         262,656\n",
            "             ReLU-11                  [-1, 512]               0\n",
            "          Dropout-12                  [-1, 512]               0\n",
            "           Linear-13                  [-1, 256]         131,328\n",
            "             ReLU-14                  [-1, 256]               0\n",
            "          Dropout-15                  [-1, 256]               0\n",
            "           Linear-16                    [-1, 6]           1,542\n",
            "================================================================\n",
            "Total params: 5,042,950\n",
            "Trainable params: 5,042,950\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.08\n",
            "Params size (MB): 19.24\n",
            "Estimated Total Size (MB): 19.32\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li3ftAC1Az0Q"
      },
      "source": [
        "def search_npy_list(dirname):  # 매개변수 dir에서 모든 npy파일을 찾고 fold에 따른 dataset 나누기\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for _, file in enumerate(filenames) if file.endswith(\".npy\")]\n",
        "    return filenames\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQH_VRRFBGF1"
      },
      "source": [
        "def train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, test_signal_dir,annotations_dir\n",
        "                               ,epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                               layer_filters=[64,128,256,512],first_conv=[200,40,100],optim='Adam',lf='CE',\n",
        "                               epsilon=0.7,noise_scale=2e-6,min_value=-1e-4,max_value=1e-4,preprocessing=True,\n",
        "                               norm_methods='Standard',use_noise=True,loss_type='softmax'):\n",
        "    # Adam optimizer param\n",
        "    b1 = 0.5\n",
        "    b2 = 0.999\n",
        "\n",
        "    beta = 0.001\n",
        "\n",
        "    check_file = open(logging_filename, 'w')  # logging file\n",
        "\n",
        "    print('Preproceesing  : ',preprocessing)\n",
        "    print('min/max value : %f/%f'%(min_value,max_value))\n",
        "    print('noise scale : ',noise_scale)\n",
        "    print('loss function : ',lf)\n",
        "    print('epsilon : ',epsilon)\n",
        "    print('norm methods : ',norm_methods)\n",
        "\n",
        "    print('logging file name : ', logging_filename)\n",
        "    print('save file name : ', save_filename)\n",
        "    print('layer filters : ',layer_filters)\n",
        "    print('fisrt_conv info : ',first_conv)\n",
        "    print('loss type : ',loss_type)\n",
        "    print('training data oversampling noise : ',use_noise)\n",
        "    best_accuracy = 0.\n",
        "    best_epoch = 0\n",
        "\n",
        "    train_dataset_list = search_npy_list(train_signal_dir)\n",
        "    test_dataset_list = search_npy_list(test_signal_dir)\n",
        "\n",
        "    train_dataset_len = len(train_dataset_list)\n",
        "    test_dataset_len = len(test_dataset_list)\n",
        "\n",
        "    print('train_dataset length : ', len(train_dataset_list))\n",
        "    print(train_dataset_list)\n",
        "\n",
        "    print('test_dataset length : ',test_dataset_len)\n",
        "    print(test_dataset_list)\n",
        "\n",
        "\n",
        "    model = DeepSleepNet_Classification()\n",
        "\n",
        "    #model = resnet18_200hz(in_channel=1,layer_filters=layer_filters,first_conv=first_conv,use_batchnorm=True,num_classes=5)\n",
        "\n",
        "    model.apply(weights_init) # weight init\n",
        "\n",
        "    cuda = torch.cuda.is_available()\n",
        "\n",
        "    if cuda:\n",
        "        print('can use CUDA!!!')\n",
        "        model = model.cuda()\n",
        "    #summary(model,[1,6000])\n",
        "    print('torch.cuda.device_count() : ', torch.cuda.device_count())\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print('Multi GPU Activation !!!')\n",
        "        #model = nn.DataParallel(model)\n",
        "\n",
        "    # loss funcition\n",
        "    if lf == 'CE':\n",
        "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "    elif lf == 'CEW':\n",
        "        samples_per_cls = [27,15,41,5,11]\n",
        "        no_of_classes = 5\n",
        "        effective_num = 1.0 - np.power(beta,samples_per_cls)\n",
        "        #print(effective_num)\n",
        "        weights = (1.0 - beta) / np.array(effective_num)\n",
        "        #print(weights)\n",
        "        weights = weights / np.sum(weights) * no_of_classes\n",
        "        weights = torch.tensor(weights).float()\n",
        "        weights = weights.to(device)\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=weights).to(device)\n",
        "    elif lf == 'FL':\n",
        "        loss_fn = FocalLoss(gamma=2).to(device)\n",
        "    elif lf == 'CBL':\n",
        "        loss_fn = CB_loss(samples_per_cls=[27,15,41,5,11],no_of_classes=5,loss_type=loss_type,beta=0.9999,gamma=2.0)\n",
        "    #loss_fn = FocalLoss(gamma=2).to(device)\n",
        "\n",
        "    # optimizer ADAM (SGD의 경우에는 정상적으로 학습이 진행되지 않았음)\n",
        "    if optim == 'Adam':\n",
        "        print('Optimizer : Adam')\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "    elif optim == 'RMS':\n",
        "        print('Optimizer : RMSprop')\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "    elif optim == 'SGD':\n",
        "        print('Optimizer : SGD')\n",
        "        optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=gamma, patience=10,\n",
        "                                                           min_lr=1e-6)\n",
        "    #stride = 40 일 때, batch_size = 20이면 16GB정도의 메모리 사용\n",
        "    batch_size = 5\n",
        "    norm_square = 2\n",
        "\n",
        "    train_batch_size = math.ceil(train_dataset_len / batch_size)\n",
        "    print('train_batch_size : ',train_batch_size)\n",
        "\n",
        "    test_batch_size = test_dataset_len\n",
        "\n",
        "    best_accuracy = 0.\n",
        "    stop_count = 0\n",
        "    for epoch in range(epochs):\n",
        "        train_dataset = suffle_dataset_list(train_dataset_list) # 매 epoch마다 train_dataset shuffle !\n",
        "        count = 0  # check batch\n",
        "        train_total_loss = 0.0\n",
        "        train_total_count = 0\n",
        "        train_total_data = 0\n",
        "\n",
        "        val_total_loss = 0.0\n",
        "        val_total_count = 0\n",
        "        val_total_data = 0\n",
        "\n",
        "        test_total_loss = 0.0\n",
        "        test_total_count = 0\n",
        "        test_total_data = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "\n",
        "        output_str = 'current_lr : %f\\n'%(optimizer.state_dict()['param_groups'][0]['lr'])\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "        for index, file_name in enumerate(train_dataset):\n",
        "            #print('index : ',index)\n",
        "            if index % batch_size == 0:\n",
        "                batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "            else:\n",
        "                new_signal, new_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "                batch_signal = torch.cat((batch_signal, new_signal),dim=1)\n",
        "                batch_label = np.concatenate((batch_label, new_label))\n",
        "            count += 1\n",
        "            if count == batch_size or index == len(train_dataset) - 1:  # batch 학습 시작!\n",
        "                batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                #batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                # batch_signal = torch.from_numpy(batch_signal).float().to(device)\n",
        "                batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "                optimizer.zero_grad()\n",
        "                # print(batch_signal.shape)\n",
        "                # print(batch_signal)\n",
        "                pred = model(batch_signal)\n",
        "                norm = 0\n",
        "\n",
        "                for parameter in model.parameters():\n",
        "                    norm += torch.norm(parameter, p=norm_square)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label) + beta * norm\n",
        "                #print('loss : ',loss.item())\n",
        "                # loss = loss_fn(pred, batch_label)\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                train_total_loss += loss.item()\n",
        "\n",
        "                train_total_count += check_count\n",
        "                train_total_data += len(batch_signal)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "                count = 0\n",
        "\n",
        "        train_total_loss /= train_batch_size\n",
        "        train_accuracy = train_total_count / train_total_data * 100\n",
        "\n",
        "        output_str = 'train dataset : %d/%d epochs spend time : %.4f sec / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, train_total_loss,\n",
        "                        train_total_count, train_total_data, train_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "    \n",
        "\n",
        "        #check test dataset\n",
        "        start_time = time.time()\n",
        "        for file_name in test_dataset_list:\n",
        "            batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(test_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=False,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "            batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "\n",
        "            batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                pred = model(batch_signal)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label)\n",
        "\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                test_total_loss += loss.item()\n",
        "                test_total_count += check_count\n",
        "                test_total_data += len(batch_signal)\n",
        "\n",
        "                # 사용하지 않는 변수 제거\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "        test_total_loss /= test_batch_size\n",
        "        test_accuracy = test_total_count / test_total_data * 100\n",
        "\n",
        "\n",
        "\n",
        "        output_str = 'test dataset : %d/%d epochs spend time : %.4f sec  / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, test_total_loss,\n",
        "                        test_total_count, test_total_data, test_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "        scheduler.step(float(test_total_loss))\n",
        "        #scheduler.step()\n",
        "\n",
        "        if epoch == 0:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_epoch = epoch\n",
        "            save_file = save_filename\n",
        "            #save_file = save_path + 'best_SleepEEGNet_CNN_channel%d.pth'%channel\n",
        "            torch.save(model.state_dict(),save_file)\n",
        "            stop_count = 0\n",
        "        else:\n",
        "            if best_accuracy < test_accuracy:\n",
        "                best_accuracy = test_accuracy\n",
        "                best_epoch = epoch\n",
        "                save_file = save_filename\n",
        "                torch.save(model.state_dict(), save_file)\n",
        "                stop_count = 0\n",
        "            else:\n",
        "                stop_count += 1\n",
        "        if stop_count > 30:\n",
        "            print('Early Stopping')\n",
        "            break\n",
        "\n",
        "        output_str = 'best epoch : %d/%d / val accuracy : %f%%\\n' \\\n",
        "                     % (best_epoch+1, epochs, best_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        print('=' * 30)\n",
        "\n",
        "\n",
        "    output_str = 'best epoch : %d/%d / accuracy : %f%%\\n' \\\n",
        "                 % (best_epoch+1, epochs, best_accuracy)\n",
        "    sys.stdout.write(output_str)\n",
        "    check_file.write(output_str)\n",
        "    print('=' * 30)\n",
        "\n",
        "    check_file.close()"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-qN1b_NBMC1",
        "outputId": "418548ad-aa75-4984-8d4b-c2b98a7598f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "save_filename = './train.pth'\n",
        "logging_filename = './logging.txt'\n",
        "train_signal_dir = 'signals/npy/Fpz-Cz/remove_wake/train/'\n",
        "test_signal_dir = 'signals/npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_dir = 'annotations/npy/remove_wake/'\n",
        "train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, \n",
        "                           test_signal_dir,annotations_dir,\n",
        "                           epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                           layer_filters=[64,128,256,512],first_conv=[200,40,100],\n",
        "                           optim='Adam',lf='CE',epsilon=0,noise_scale=0,\n",
        "                           min_value=-0,max_value=0,preprocessing=True,\n",
        "                           norm_methods='Standard',use_noise=False,loss_type='softmax')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preproceesing  :  True\n",
            "min/max value : 0.000000/0.000000\n",
            "noise scale :  0\n",
            "loss function :  CE\n",
            "epsilon :  0\n",
            "norm methods :  Standard\n",
            "logging file name :  ./logging.txt\n",
            "save file name :  ./train.pth\n",
            "layer filters :  [64, 128, 256, 512]\n",
            "fisrt_conv info :  [200, 40, 100]\n",
            "loss type :  softmax\n",
            "training data oversampling noise :  False\n",
            "train_dataset length :  30\n",
            "['SC4161EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4151EC-Hypnogram.npy']\n",
            "test_dataset length :  9\n",
            "['SC4082EP-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy']\n",
            "can use CUDA!!!\n",
            "torch.cuda.device_count() :  1\n",
            "Optimizer : Adam\n",
            "train_batch_size :  6\n",
            "current_lr : 0.001000\n",
            "train dataset : 1/2000 epochs spend time : 2.0298 sec / total_loss : 1.6977 correct : 12493/29223 -> 42.7506%\n",
            "test dataset : 1/2000 epochs spend time : 0.5149 sec  / total_loss : 1.5121 correct : 1429/9197 -> 15.5377%\n",
            "best epoch : 1/2000 / val accuracy : 15.537675%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 2/2000 epochs spend time : 2.0184 sec / total_loss : 1.5656 correct : 11042/29223 -> 37.7853%\n",
            "test dataset : 2/2000 epochs spend time : 0.5314 sec  / total_loss : 1.5216 correct : 4219/9197 -> 45.8737%\n",
            "best epoch : 2/2000 / val accuracy : 45.873654%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 3/2000 epochs spend time : 1.9391 sec / total_loss : 1.5266 correct : 13325/29223 -> 45.5976%\n",
            "test dataset : 3/2000 epochs spend time : 0.5088 sec  / total_loss : 1.4278 correct : 4489/9197 -> 48.8094%\n",
            "best epoch : 3/2000 / val accuracy : 48.809394%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 4/2000 epochs spend time : 1.9173 sec / total_loss : 1.4834 correct : 13276/29223 -> 45.4300%\n",
            "test dataset : 4/2000 epochs spend time : 0.5804 sec  / total_loss : 1.3859 correct : 4335/9197 -> 47.1349%\n",
            "best epoch : 3/2000 / val accuracy : 48.809394%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 5/2000 epochs spend time : 1.8853 sec / total_loss : 1.4228 correct : 13569/29223 -> 46.4326%\n",
            "test dataset : 5/2000 epochs spend time : 0.5231 sec  / total_loss : 1.3249 correct : 4124/9197 -> 44.8407%\n",
            "best epoch : 3/2000 / val accuracy : 48.809394%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 6/2000 epochs spend time : 1.8958 sec / total_loss : 1.5079 correct : 12255/29223 -> 41.9361%\n",
            "test dataset : 6/2000 epochs spend time : 0.5806 sec  / total_loss : 1.4554 correct : 4219/9197 -> 45.8737%\n",
            "best epoch : 3/2000 / val accuracy : 48.809394%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 7/2000 epochs spend time : 1.8706 sec / total_loss : 1.4741 correct : 13368/29223 -> 45.7448%\n",
            "test dataset : 7/2000 epochs spend time : 0.4938 sec  / total_loss : 1.3340 correct : 4641/9197 -> 50.4621%\n",
            "best epoch : 7/2000 / val accuracy : 50.462107%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 8/2000 epochs spend time : 1.8366 sec / total_loss : 1.4831 correct : 13173/29223 -> 45.0775%\n",
            "test dataset : 8/2000 epochs spend time : 0.5287 sec  / total_loss : 1.4709 correct : 4219/9197 -> 45.8737%\n",
            "best epoch : 7/2000 / val accuracy : 50.462107%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 9/2000 epochs spend time : 1.8454 sec / total_loss : 1.5370 correct : 13581/29223 -> 46.4737%\n",
            "test dataset : 9/2000 epochs spend time : 0.4930 sec  / total_loss : 1.3788 correct : 4219/9197 -> 45.8737%\n",
            "best epoch : 7/2000 / val accuracy : 50.462107%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 10/2000 epochs spend time : 1.8698 sec / total_loss : 1.4823 correct : 12819/29223 -> 43.8661%\n",
            "test dataset : 10/2000 epochs spend time : 0.5108 sec  / total_loss : 1.3603 correct : 4726/9197 -> 51.3863%\n",
            "best epoch : 10/2000 / val accuracy : 51.386322%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 11/2000 epochs spend time : 1.8769 sec / total_loss : 1.4037 correct : 14264/29223 -> 48.8109%\n",
            "test dataset : 11/2000 epochs spend time : 0.4870 sec  / total_loss : 1.3222 correct : 4109/9197 -> 44.6776%\n",
            "best epoch : 10/2000 / val accuracy : 51.386322%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 12/2000 epochs spend time : 1.8191 sec / total_loss : 1.2719 correct : 14521/29223 -> 49.6903%\n",
            "test dataset : 12/2000 epochs spend time : 0.4910 sec  / total_loss : 1.4156 correct : 3207/9197 -> 34.8701%\n",
            "best epoch : 10/2000 / val accuracy : 51.386322%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 13/2000 epochs spend time : 1.8658 sec / total_loss : 1.2468 correct : 14119/29223 -> 48.3147%\n",
            "test dataset : 13/2000 epochs spend time : 0.5066 sec  / total_loss : 1.2466 correct : 4428/9197 -> 48.1461%\n",
            "best epoch : 10/2000 / val accuracy : 51.386322%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 14/2000 epochs spend time : 1.8498 sec / total_loss : 1.1302 correct : 16803/29223 -> 57.4992%\n",
            "test dataset : 14/2000 epochs spend time : 0.4857 sec  / total_loss : 1.2442 correct : 4538/9197 -> 49.3422%\n",
            "best epoch : 10/2000 / val accuracy : 51.386322%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 15/2000 epochs spend time : 1.8163 sec / total_loss : 1.1915 correct : 15789/29223 -> 54.0294%\n",
            "test dataset : 15/2000 epochs spend time : 0.4767 sec  / total_loss : 1.2958 correct : 4689/9197 -> 50.9840%\n",
            "best epoch : 10/2000 / val accuracy : 51.386322%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 16/2000 epochs spend time : 1.8213 sec / total_loss : 1.1121 correct : 17104/29223 -> 58.5292%\n",
            "test dataset : 16/2000 epochs spend time : 0.4960 sec  / total_loss : 1.2165 correct : 4436/9197 -> 48.2331%\n",
            "best epoch : 10/2000 / val accuracy : 51.386322%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 17/2000 epochs spend time : 1.8625 sec / total_loss : 1.0016 correct : 18429/29223 -> 63.0633%\n",
            "test dataset : 17/2000 epochs spend time : 0.4957 sec  / total_loss : 1.1408 correct : 5053/9197 -> 54.9418%\n",
            "best epoch : 17/2000 / val accuracy : 54.941829%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 18/2000 epochs spend time : 1.8427 sec / total_loss : 0.9403 correct : 18947/29223 -> 64.8359%\n",
            "test dataset : 18/2000 epochs spend time : 0.5008 sec  / total_loss : 1.2448 correct : 4941/9197 -> 53.7240%\n",
            "best epoch : 17/2000 / val accuracy : 54.941829%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 19/2000 epochs spend time : 1.8181 sec / total_loss : 0.9977 correct : 18328/29223 -> 62.7177%\n",
            "test dataset : 19/2000 epochs spend time : 0.5293 sec  / total_loss : 1.1462 correct : 4948/9197 -> 53.8002%\n",
            "best epoch : 17/2000 / val accuracy : 54.941829%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 20/2000 epochs spend time : 1.9173 sec / total_loss : 0.9274 correct : 18929/29223 -> 64.7743%\n",
            "test dataset : 20/2000 epochs spend time : 0.4942 sec  / total_loss : 1.1786 correct : 4689/9197 -> 50.9840%\n",
            "best epoch : 17/2000 / val accuracy : 54.941829%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 21/2000 epochs spend time : 1.8383 sec / total_loss : 0.8584 correct : 20165/29223 -> 69.0039%\n",
            "test dataset : 21/2000 epochs spend time : 0.4952 sec  / total_loss : 1.2501 correct : 5062/9197 -> 55.0397%\n",
            "best epoch : 21/2000 / val accuracy : 55.039687%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 22/2000 epochs spend time : 1.8079 sec / total_loss : 0.8767 correct : 19644/29223 -> 67.2210%\n",
            "test dataset : 22/2000 epochs spend time : 0.4904 sec  / total_loss : 1.1747 correct : 4934/9197 -> 53.6479%\n",
            "best epoch : 21/2000 / val accuracy : 55.039687%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 23/2000 epochs spend time : 1.8316 sec / total_loss : 0.8147 correct : 20813/29223 -> 71.2213%\n",
            "test dataset : 23/2000 epochs spend time : 0.4936 sec  / total_loss : 1.2842 correct : 4979/9197 -> 54.1372%\n",
            "best epoch : 21/2000 / val accuracy : 55.039687%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 24/2000 epochs spend time : 1.9316 sec / total_loss : 0.7751 correct : 21176/29223 -> 72.4635%\n",
            "test dataset : 24/2000 epochs spend time : 0.5071 sec  / total_loss : 2.3896 correct : 3849/9197 -> 41.8506%\n",
            "best epoch : 21/2000 / val accuracy : 55.039687%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 25/2000 epochs spend time : 1.8631 sec / total_loss : 1.1974 correct : 16719/29223 -> 57.2118%\n",
            "test dataset : 25/2000 epochs spend time : 0.5035 sec  / total_loss : 1.2010 correct : 4780/9197 -> 51.9735%\n",
            "best epoch : 21/2000 / val accuracy : 55.039687%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 26/2000 epochs spend time : 1.8779 sec / total_loss : 0.8460 correct : 20273/29223 -> 69.3734%\n",
            "test dataset : 26/2000 epochs spend time : 0.4871 sec  / total_loss : 1.3776 correct : 4912/9197 -> 53.4087%\n",
            "best epoch : 21/2000 / val accuracy : 55.039687%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 27/2000 epochs spend time : 1.8752 sec / total_loss : 0.7098 correct : 21928/29223 -> 75.0368%\n",
            "test dataset : 27/2000 epochs spend time : 0.4898 sec  / total_loss : 1.4880 correct : 5095/9197 -> 55.3985%\n",
            "best epoch : 27/2000 / val accuracy : 55.398500%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 28/2000 epochs spend time : 1.8859 sec / total_loss : 0.7790 correct : 21375/29223 -> 73.1444%\n",
            "test dataset : 28/2000 epochs spend time : 0.4903 sec  / total_loss : 1.2524 correct : 5075/9197 -> 55.1810%\n",
            "best epoch : 27/2000 / val accuracy : 55.398500%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 29/2000 epochs spend time : 1.8516 sec / total_loss : 0.6398 correct : 22949/29223 -> 78.5306%\n",
            "test dataset : 29/2000 epochs spend time : 0.4815 sec  / total_loss : 1.3625 correct : 4784/9197 -> 52.0170%\n",
            "best epoch : 27/2000 / val accuracy : 55.398500%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 30/2000 epochs spend time : 1.8242 sec / total_loss : 0.5576 correct : 23736/29223 -> 81.2237%\n",
            "test dataset : 30/2000 epochs spend time : 0.4859 sec  / total_loss : 1.7689 correct : 5133/9197 -> 55.8117%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 31/2000 epochs spend time : 1.8091 sec / total_loss : 0.5097 correct : 24419/29223 -> 83.5609%\n",
            "test dataset : 31/2000 epochs spend time : 0.4975 sec  / total_loss : 1.5611 correct : 4992/9197 -> 54.2786%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 32/2000 epochs spend time : 1.8399 sec / total_loss : 0.4489 correct : 25261/29223 -> 86.4422%\n",
            "test dataset : 32/2000 epochs spend time : 0.4835 sec  / total_loss : 1.6540 correct : 4934/9197 -> 53.6479%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 33/2000 epochs spend time : 1.7696 sec / total_loss : 0.4389 correct : 25495/29223 -> 87.2429%\n",
            "test dataset : 33/2000 epochs spend time : 0.5388 sec  / total_loss : 1.8224 correct : 4951/9197 -> 53.8328%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 34/2000 epochs spend time : 1.8026 sec / total_loss : 0.4293 correct : 25505/29223 -> 87.2771%\n",
            "test dataset : 34/2000 epochs spend time : 0.5079 sec  / total_loss : 1.9523 correct : 5004/9197 -> 54.4090%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 35/2000 epochs spend time : 1.7876 sec / total_loss : 0.4000 correct : 25878/29223 -> 88.5535%\n",
            "test dataset : 35/2000 epochs spend time : 0.5140 sec  / total_loss : 1.8444 correct : 4998/9197 -> 54.3438%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 36/2000 epochs spend time : 1.7568 sec / total_loss : 0.3783 correct : 26084/29223 -> 89.2585%\n",
            "test dataset : 36/2000 epochs spend time : 0.4848 sec  / total_loss : 2.0762 correct : 5003/9197 -> 54.3982%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 37/2000 epochs spend time : 1.8100 sec / total_loss : 0.3832 correct : 26109/29223 -> 89.3440%\n",
            "test dataset : 37/2000 epochs spend time : 0.4836 sec  / total_loss : 2.1082 correct : 5066/9197 -> 55.0832%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 38/2000 epochs spend time : 1.8186 sec / total_loss : 0.3595 correct : 26369/29223 -> 90.2337%\n",
            "test dataset : 38/2000 epochs spend time : 0.4705 sec  / total_loss : 1.9821 correct : 4952/9197 -> 53.8436%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 39/2000 epochs spend time : 1.8120 sec / total_loss : 0.3328 correct : 26646/29223 -> 91.1816%\n",
            "test dataset : 39/2000 epochs spend time : 0.4966 sec  / total_loss : 2.0005 correct : 4876/9197 -> 53.0173%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 40/2000 epochs spend time : 1.7794 sec / total_loss : 0.3128 correct : 26912/29223 -> 92.0918%\n",
            "test dataset : 40/2000 epochs spend time : 0.4784 sec  / total_loss : 2.1471 correct : 5014/9197 -> 54.5178%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 41/2000 epochs spend time : 1.8041 sec / total_loss : 0.2775 correct : 27269/29223 -> 93.3135%\n",
            "test dataset : 41/2000 epochs spend time : 0.4772 sec  / total_loss : 2.1551 correct : 4963/9197 -> 53.9632%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 42/2000 epochs spend time : 1.8195 sec / total_loss : 0.2684 correct : 27385/29223 -> 93.7104%\n",
            "test dataset : 42/2000 epochs spend time : 0.4802 sec  / total_loss : 2.3113 correct : 4975/9197 -> 54.0937%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 43/2000 epochs spend time : 1.8230 sec / total_loss : 0.2623 correct : 27444/29223 -> 93.9123%\n",
            "test dataset : 43/2000 epochs spend time : 0.5176 sec  / total_loss : 2.3185 correct : 4932/9197 -> 53.6262%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 44/2000 epochs spend time : 1.8305 sec / total_loss : 0.2495 correct : 27601/29223 -> 94.4496%\n",
            "test dataset : 44/2000 epochs spend time : 0.4911 sec  / total_loss : 2.3152 correct : 4963/9197 -> 53.9632%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 45/2000 epochs spend time : 1.8415 sec / total_loss : 0.2484 correct : 27605/29223 -> 94.4633%\n",
            "test dataset : 45/2000 epochs spend time : 0.5144 sec  / total_loss : 2.3736 correct : 4982/9197 -> 54.1698%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 46/2000 epochs spend time : 1.8046 sec / total_loss : 0.2503 correct : 27571/29223 -> 94.3469%\n",
            "test dataset : 46/2000 epochs spend time : 0.4958 sec  / total_loss : 2.3392 correct : 4999/9197 -> 54.3547%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 47/2000 epochs spend time : 1.7768 sec / total_loss : 0.2334 correct : 27775/29223 -> 95.0450%\n",
            "test dataset : 47/2000 epochs spend time : 0.4838 sec  / total_loss : 2.4112 correct : 4959/9197 -> 53.9198%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 48/2000 epochs spend time : 1.8402 sec / total_loss : 0.2305 correct : 27786/29223 -> 95.0826%\n",
            "test dataset : 48/2000 epochs spend time : 0.4804 sec  / total_loss : 2.4828 correct : 4993/9197 -> 54.2894%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 49/2000 epochs spend time : 1.7828 sec / total_loss : 0.2276 correct : 27836/29223 -> 95.2537%\n",
            "test dataset : 49/2000 epochs spend time : 0.4856 sec  / total_loss : 2.4286 correct : 4995/9197 -> 54.3112%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 50/2000 epochs spend time : 1.8099 sec / total_loss : 0.2224 correct : 27860/29223 -> 95.3359%\n",
            "test dataset : 50/2000 epochs spend time : 0.4955 sec  / total_loss : 2.4906 correct : 4935/9197 -> 53.6588%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 51/2000 epochs spend time : 1.8285 sec / total_loss : 0.2157 correct : 27982/29223 -> 95.7533%\n",
            "test dataset : 51/2000 epochs spend time : 0.5036 sec  / total_loss : 2.4983 correct : 4963/9197 -> 53.9632%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 52/2000 epochs spend time : 1.8557 sec / total_loss : 0.2060 correct : 28035/29223 -> 95.9347%\n",
            "test dataset : 52/2000 epochs spend time : 0.4746 sec  / total_loss : 2.5521 correct : 4934/9197 -> 53.6479%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 53/2000 epochs spend time : 1.7928 sec / total_loss : 0.2041 correct : 28089/29223 -> 96.1195%\n",
            "test dataset : 53/2000 epochs spend time : 0.5079 sec  / total_loss : 2.5708 correct : 4972/9197 -> 54.0611%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 54/2000 epochs spend time : 1.8974 sec / total_loss : 0.1936 correct : 28152/29223 -> 96.3351%\n",
            "test dataset : 54/2000 epochs spend time : 0.6046 sec  / total_loss : 2.6074 correct : 4958/9197 -> 53.9089%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 55/2000 epochs spend time : 2.1126 sec / total_loss : 0.1938 correct : 28157/29223 -> 96.3522%\n",
            "test dataset : 55/2000 epochs spend time : 0.5388 sec  / total_loss : 2.6416 correct : 4970/9197 -> 54.0394%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 56/2000 epochs spend time : 1.9117 sec / total_loss : 0.1945 correct : 28193/29223 -> 96.4754%\n",
            "test dataset : 56/2000 epochs spend time : 0.5372 sec  / total_loss : 2.6277 correct : 4969/9197 -> 54.0285%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 57/2000 epochs spend time : 1.8833 sec / total_loss : 0.1962 correct : 28173/29223 -> 96.4069%\n",
            "test dataset : 57/2000 epochs spend time : 0.5229 sec  / total_loss : 2.6293 correct : 4965/9197 -> 53.9850%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 58/2000 epochs spend time : 1.8900 sec / total_loss : 0.1931 correct : 28175/29223 -> 96.4138%\n",
            "test dataset : 58/2000 epochs spend time : 0.5407 sec  / total_loss : 2.6619 correct : 4969/9197 -> 54.0285%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 59/2000 epochs spend time : 1.7841 sec / total_loss : 0.1883 correct : 28233/29223 -> 96.6123%\n",
            "test dataset : 59/2000 epochs spend time : 0.4904 sec  / total_loss : 2.6541 correct : 4980/9197 -> 54.1481%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 60/2000 epochs spend time : 1.7576 sec / total_loss : 0.1827 correct : 28304/29223 -> 96.8552%\n",
            "test dataset : 60/2000 epochs spend time : 0.5072 sec  / total_loss : 2.7070 correct : 4983/9197 -> 54.1807%\n",
            "best epoch : 30/2000 / val accuracy : 55.811678%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 61/2000 epochs spend time : 1.7839 sec / total_loss : 0.1828 correct : 28279/29223 -> 96.7697%\n",
            "test dataset : 61/2000 epochs spend time : 0.4829 sec  / total_loss : 2.7043 correct : 4993/9197 -> 54.2894%\n",
            "Early Stopping\n",
            "best epoch : 30/2000 / accuracy : 55.811678%\n",
            "==============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCE73qoRBUC2"
      },
      "source": [
        ""
      ],
      "execution_count": 52,
      "outputs": []
    }
  ]
}
