{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SleepDataPreparation_and_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/SleepCapstone/blob/master/Codes/SleepNet_with_Convolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAGJRSb3xOT1"
      },
      "source": [
        "한림대학교 소프트웨어 융합대학\n",
        "빅데이터 캡스톤 프로젝트\n",
        "\n",
        "딥러닝에 기반한 수면 분류 모델 개발"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SaQVdPHgfmx",
        "outputId": "1f0512ad-863e-4473-f2b4-1752b5479308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohoEZomKw0cz",
        "outputId": "dfd2a2b5-0ed1-4ae2-edb1-2f624faac9f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "cd drive/My\\ Drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvna-5vRzmTQ",
        "outputId": "fae058ee-c394-4f44-bd7b-7dfa2802e5aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "source": [
        "# GPU 가용성 체크\n",
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Oct 18 06:29:25 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1zNHBNc4beq"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def search_signals_npy(dirname):\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for file in filenames if file.endswith(\".npy\")]\n",
        "    return filenames\n",
        "\n",
        "def search_correct_signals_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vSGY6sO9vgC",
        "outputId": "b7cd9f3e-394e-4177-dde7-86fda2eb1dd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "\n",
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  'annotations/npy/remove_wake/'\n",
        "signals_path = 'signals/npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "total_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list:\n",
        "    label = np.load(path + filename)\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    \n",
        "    signals = np.load(signals_path+signals_filename)\n",
        "    \n",
        "    #print('remove start index : %d / remove end index : %d'%(remove_start_index,remove_end_index))\n",
        "    #print(np.bincount(label,minlength=6))\n",
        "    if len(label) !=len(signals[0])//30//fs:\n",
        "        print('file is fault!!!')\n",
        "    for i in range(6):\n",
        "        total_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "print(total_label)\n",
        "\n",
        "x = np.arange(len(total_label))\n",
        "\n",
        "plt.bar(x,total_label,width=0.7)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4336  2804 17799  5703  7717    61]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 6 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVVZuGLx-ADW",
        "outputId": "f7f13cda-ef4c-4b6b-b82e-6dcd591c4154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  'annotations/npy/remove_wake/'\n",
        "signals_path = 'signals/npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "random.shuffle(annotations_npy_list)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "trainDataset_count = 30\n",
        "testDataset_count = len(annotations_npy_list)-trainDataset_count\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4001EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4192EV-Hypnogram.npy']\n",
            "['SC4102EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy']\n",
            "[12.0170408   7.53399967 46.47550385 13.78010814 20.04587908  0.14746846]\n",
            "[ 8.4629981   6.3883618  45.75585073 18.95003163 20.24035421  0.20240354]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAKH9Mb0_NF3"
      },
      "source": [
        "signals_path = 'signals/npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "save_train_path = 'signals/npy//Fpz-Cz/remove_wake/train/'\n",
        "save_test_path = 'signals/npy/Fpz-Cz/remove_wake/test/'\n",
        "\n",
        "os.makedirs(save_train_path,exist_ok=True)\n",
        "os.makedirs(save_test_path,exist_ok=True)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_train_path+filename)\n",
        "    \n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_test_path+filename)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4EYtlUH-e9f",
        "outputId": "8f873791-015e-4471-c3f8-27b55bd656e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "def search_correct_annotations_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename\n",
        "\n",
        "train_path =  'signals/npy/Fpz-Cz/remove_wake/train/'\n",
        "test_path = 'signals/npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_path = 'annotations/npy/remove_wake/'\n",
        "\n",
        "train_list = search_signals_npy(train_path)\n",
        "test_list = search_signals_npy(test_path)\n",
        "\n",
        "print(train_list)\n",
        "print(test_list)\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in train_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in test_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4102EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4131EC-Hypnogram.npy']\n",
            "['SC4082EP-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy']\n",
            "[12.0170408   7.53399967 46.47550385 13.78010814 20.04587908  0.14746846]\n",
            "[ 8.4629981   6.3883618  45.75585073 18.95003163 20.24035421  0.20240354]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBwqVERu_aw-"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pylab as plt\n",
        "from  torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSQaEXQr_h8m"
      },
      "source": [
        "def data_preprocessing_torch(signals): # 하나의 데이터셋에 대한 data_preprocessing (using torch)\n",
        "    signals = (signals - signals.mean(dim=1).unsqueeze(1))/signals.std(dim=1).unsqueeze(1)\n",
        "\n",
        "    return signals\n",
        "\n",
        "def data_preprocessing_oneToOne_torch(signals,min,max,max_value):\n",
        "    signals_std = (signals + max_value) / (2*max_value)\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def data_preprocessing_minmax_torch(signals,min,max):\n",
        "    signals_std = (signals - signals.min(dim=1).unsqueeze(1)) / (\n",
        "            signals.max(dim=1).unsqueeze(1) - signals.min(dim=1).unsqueeze(1))\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def get_dataset_one_channel_norm_withoutCut(dirname,annotations_dir,data_path,use_noise=True,epsilon=0.5,noise_scale=2e-6,preprocessing=True,norm_methods='Standard'):\n",
        "    # npy read!\n",
        "    path = dirname + data_path\n",
        "    signals = np.load(path)\n",
        "\n",
        "    signals = torch.from_numpy(signals).float().to(device)\n",
        "\n",
        "    if use_noise:\n",
        "        if np.random.rand() < epsilon:\n",
        "            # noise = np.random.normal(loc=0,scale=noise_scale,size=signals.shape)\n",
        "            # signals = signals + noise\n",
        "            noise = torch.normal(mean=0., std=noise_scale, size=signals.shape).to(device)\n",
        "            signals = signals + noise\n",
        "\n",
        "    if preprocessing:\n",
        "        if norm_methods == 'Standard':\n",
        "            signals = data_preprocessing_torch(signals)\n",
        "        elif norm_methods == 'minmax':\n",
        "            signals = data_preprocessing_minmax_torch(signals,0,1)\n",
        "        elif norm_methods == 'oneToOne':\n",
        "            signals = data_preprocessing_oneToOne_torch(signals,-1,1,1e-4)\n",
        "    label = get_annotations(annotations_dir, data_path)\n",
        "    return signals, label\n",
        "\n",
        "# model conv layer weight init function\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:         # Conv weight init\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "        \n",
        "def suffle_dataset_list(dataset_list): # 데이터 셔플\n",
        "    random.shuffle(dataset_list)\n",
        "    return dataset_list\n",
        "\n",
        "\n",
        "# npy파일을 통해 label을 가져오는 함수\n",
        "def get_annotations(label_dir,file_name):\n",
        "    label_path = label_dir + file_name\n",
        "    label = np.load(label_path)\n",
        "    return label\n",
        "\n",
        "def signals_expand_torch_one_channel(signals): # 2차원 데이터를 3차원으로 변환 (8,N) -> (batch,8,6000) 형태로\n",
        "    signals = signals.expand(1,1,-1)\n",
        "    #print(signals.shape)\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    signals = signals.reshape(-1, 3000, 1) # 형태 변환\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    return signals"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx4M7Rfq_lwo"
      },
      "source": [
        "class DeepSleepNet_Classification(nn.Module):  # input channel = 8channel / output = 5\n",
        "    def __init__(self,in_channel=1,out_channel=6,layer=[64,128,128,128],sample_rate = 100):\n",
        "        super(DeepSleepNet_Classification, self).__init__()\n",
        "\n",
        "        self.conv1d_1 = nn.Conv1d(1,16, kernel_size=300, stride=50)\n",
        "        self.conv1d_2 = nn.Conv1d(16, 32, kernel_size=10, stride=5)\n",
        "        self.fc1 = nn.Linear(320,160)\n",
        "        self.fc2 = nn.Linear(160,out_channel) \n",
        "\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.conv1d_1(input)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.conv1d_2(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.ReLU(out)\n",
        "        #out = self.dropout(out)\n",
        "        #out = self.fc2(out)\n",
        "        #out = self.ReLU(out)\n",
        "        #out = self.dropout(out)        \n",
        "        #out = self.fc3(out)\n",
        "        #out = self.ReLU(out)\n",
        "        #out = self.dropout(out)\n",
        "        #out = self.fc4(out)\n",
        "        #out = self.ReLU(out)\n",
        "        #out = self.dropout(out)        \n",
        "        #out = self.fc5(out)\n",
        "        #out = self.ReLU(out)\n",
        "        #out = self.dropout(out)        \n",
        "        #out = self.fc6(out)        \n",
        "\n",
        "        return out"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYZ6YSZHArdJ",
        "outputId": "3824ea9a-ed43-4b64-cf0a-a39930aa03dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "model = DeepSleepNet_Classification(in_channel=1,out_channel=6)\n",
        "summary(model.cuda(),(1,3000))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv1d-1               [-1, 16, 55]           4,816\n",
            "              ReLU-2               [-1, 16, 55]               0\n",
            "            Conv1d-3               [-1, 32, 10]           5,152\n",
            "              ReLU-4               [-1, 32, 10]               0\n",
            "            Linear-5                  [-1, 160]          51,360\n",
            "              ReLU-6                  [-1, 160]               0\n",
            "            Linear-7                    [-1, 6]             966\n",
            "              ReLU-8                    [-1, 6]               0\n",
            "================================================================\n",
            "Total params: 62,294\n",
            "Trainable params: 62,294\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.02\n",
            "Params size (MB): 0.24\n",
            "Estimated Total Size (MB): 0.27\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li3ftAC1Az0Q"
      },
      "source": [
        "def search_npy_list(dirname):  # 매개변수 dir에서 모든 npy파일을 찾고 fold에 따른 dataset 나누기\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for _, file in enumerate(filenames) if file.endswith(\".npy\")]\n",
        "    return filenames\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQH_VRRFBGF1"
      },
      "source": [
        "def train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, test_signal_dir,annotations_dir\n",
        "                               ,epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                               layer_filters=[64,128,256,512],first_conv=[200,40,100],optim='Adam',lf='CE',\n",
        "                               epsilon=0.7,noise_scale=2e-6,min_value=-1e-4,max_value=1e-4,preprocessing=True,\n",
        "                               norm_methods='Standard',use_noise=True,loss_type='softmax'):\n",
        "    # Adam optimizer param\n",
        "    b1 = 0.5\n",
        "    b2 = 0.999\n",
        "\n",
        "    beta = 0.001\n",
        "\n",
        "    check_file = open(logging_filename, 'w')  # logging file\n",
        "\n",
        "    print('Preproceesing  : ',preprocessing)\n",
        "    print('min/max value : %f/%f'%(min_value,max_value))\n",
        "    print('noise scale : ',noise_scale)\n",
        "    print('loss function : ',lf)\n",
        "    print('epsilon : ',epsilon)\n",
        "    print('norm methods : ',norm_methods)\n",
        "\n",
        "    print('logging file name : ', logging_filename)\n",
        "    print('save file name : ', save_filename)\n",
        "    print('layer filters : ',layer_filters)\n",
        "    print('fisrt_conv info : ',first_conv)\n",
        "    print('loss type : ',loss_type)\n",
        "    print('training data oversampling noise : ',use_noise)\n",
        "    best_accuracy = 0.\n",
        "    best_epoch = 0\n",
        "\n",
        "    train_dataset_list = search_npy_list(train_signal_dir)\n",
        "    test_dataset_list = search_npy_list(test_signal_dir)\n",
        "\n",
        "    train_dataset_len = len(train_dataset_list)\n",
        "    test_dataset_len = len(test_dataset_list)\n",
        "\n",
        "    print('train_dataset length : ', len(train_dataset_list))\n",
        "    print(train_dataset_list)\n",
        "\n",
        "    print('test_dataset length : ',test_dataset_len)\n",
        "    print(test_dataset_list)\n",
        "\n",
        "\n",
        "    model = DeepSleepNet_Classification()\n",
        "\n",
        "    #model = resnet18_200hz(in_channel=1,layer_filters=layer_filters,first_conv=first_conv,use_batchnorm=True,num_classes=5)\n",
        "\n",
        "    model.apply(weights_init) # weight init\n",
        "\n",
        "    cuda = torch.cuda.is_available()\n",
        "\n",
        "    if cuda:\n",
        "        print('can use CUDA!!!')\n",
        "        model = model.cuda()\n",
        "    #summary(model,[1,6000])\n",
        "    print('torch.cuda.device_count() : ', torch.cuda.device_count())\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print('Multi GPU Activation !!!')\n",
        "        #model = nn.DataParallel(model)\n",
        "\n",
        "    # loss funcition\n",
        "    if lf == 'CE':\n",
        "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "    elif lf == 'CEW':\n",
        "        samples_per_cls = [27,15,41,5,11]\n",
        "        no_of_classes = 5\n",
        "        effective_num = 1.0 - np.power(beta,samples_per_cls)\n",
        "        #print(effective_num)\n",
        "        weights = (1.0 - beta) / np.array(effective_num)\n",
        "        #print(weights)\n",
        "        weights = weights / np.sum(weights) * no_of_classes\n",
        "        weights = torch.tensor(weights).float()\n",
        "        weights = weights.to(device)\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=weights).to(device)\n",
        "    elif lf == 'FL':\n",
        "        loss_fn = FocalLoss(gamma=2).to(device)\n",
        "    elif lf == 'CBL':\n",
        "        loss_fn = CB_loss(samples_per_cls=[27,15,41,5,11],no_of_classes=5,loss_type=loss_type,beta=0.9999,gamma=2.0)\n",
        "    #loss_fn = FocalLoss(gamma=2).to(device)\n",
        "\n",
        "    # optimizer ADAM (SGD의 경우에는 정상적으로 학습이 진행되지 않았음)\n",
        "    if optim == 'Adam':\n",
        "        print('Optimizer : Adam')\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "    elif optim == 'RMS':\n",
        "        print('Optimizer : RMSprop')\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "    elif optim == 'SGD':\n",
        "        print('Optimizer : SGD')\n",
        "        optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=gamma, patience=10,\n",
        "                                                           min_lr=1e-6)\n",
        "    #stride = 40 일 때, batch_size = 20이면 16GB정도의 메모리 사용\n",
        "    batch_size = 5\n",
        "    norm_square = 2\n",
        "\n",
        "    train_batch_size = math.ceil(train_dataset_len / batch_size)\n",
        "    print('train_batch_size : ',train_batch_size)\n",
        "\n",
        "    test_batch_size = test_dataset_len\n",
        "\n",
        "    best_accuracy = 0.\n",
        "    stop_count = 0\n",
        "    for epoch in range(epochs):\n",
        "        train_dataset = suffle_dataset_list(train_dataset_list) # 매 epoch마다 train_dataset shuffle !\n",
        "        count = 0  # check batch\n",
        "        train_total_loss = 0.0\n",
        "        train_total_count = 0\n",
        "        train_total_data = 0\n",
        "\n",
        "        val_total_loss = 0.0\n",
        "        val_total_count = 0\n",
        "        val_total_data = 0\n",
        "\n",
        "        test_total_loss = 0.0\n",
        "        test_total_count = 0\n",
        "        test_total_data = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "\n",
        "        output_str = 'current_lr : %f\\n'%(optimizer.state_dict()['param_groups'][0]['lr'])\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "        for index, file_name in enumerate(train_dataset):\n",
        "            #print('index : ',index)\n",
        "            if index % batch_size == 0:\n",
        "                batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "            else:\n",
        "                new_signal, new_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "                batch_signal = torch.cat((batch_signal, new_signal),dim=1)\n",
        "                batch_label = np.concatenate((batch_label, new_label))\n",
        "            count += 1\n",
        "            if count == batch_size or index == len(train_dataset) - 1:  # batch 학습 시작!\n",
        "                batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                #batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                # batch_signal = torch.from_numpy(batch_signal).float().to(device)\n",
        "                batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "                optimizer.zero_grad()\n",
        "                # print(batch_signal.shape)\n",
        "                # print(batch_signal)\n",
        "                pred = model(batch_signal)\n",
        "                norm = 0\n",
        "\n",
        "                for parameter in model.parameters():\n",
        "                    norm += torch.norm(parameter, p=norm_square)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label) + beta * norm\n",
        "                #print('loss : ',loss.item())\n",
        "                # loss = loss_fn(pred, batch_label)\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                train_total_loss += loss.item()\n",
        "\n",
        "                train_total_count += check_count\n",
        "                train_total_data += len(batch_signal)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "                count = 0\n",
        "\n",
        "        train_total_loss /= train_batch_size\n",
        "        train_accuracy = train_total_count / train_total_data * 100\n",
        "\n",
        "        output_str = 'train dataset : %d/%d epochs spend time : %.4f sec / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, train_total_loss,\n",
        "                        train_total_count, train_total_data, train_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "    \n",
        "\n",
        "        #check test dataset\n",
        "        start_time = time.time()\n",
        "        for file_name in test_dataset_list:\n",
        "            batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(test_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=False,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "            batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "\n",
        "            batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                pred = model(batch_signal)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label)\n",
        "\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                test_total_loss += loss.item()\n",
        "                test_total_count += check_count\n",
        "                test_total_data += len(batch_signal)\n",
        "\n",
        "                # 사용하지 않는 변수 제거\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "        test_total_loss /= test_batch_size\n",
        "        test_accuracy = test_total_count / test_total_data * 100\n",
        "\n",
        "\n",
        "\n",
        "        output_str = 'test dataset : %d/%d epochs spend time : %.4f sec  / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, test_total_loss,\n",
        "                        test_total_count, test_total_data, test_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "        scheduler.step(float(test_total_loss))\n",
        "        #scheduler.step()\n",
        "\n",
        "        if epoch == 0:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_epoch = epoch\n",
        "            save_file = save_filename\n",
        "            #save_file = save_path + 'best_SleepEEGNet_CNN_channel%d.pth'%channel\n",
        "            torch.save(model.state_dict(),save_file)\n",
        "            stop_count = 0\n",
        "        else:\n",
        "            if best_accuracy < test_accuracy:\n",
        "                best_accuracy = test_accuracy\n",
        "                best_epoch = epoch\n",
        "                save_file = save_filename\n",
        "                torch.save(model.state_dict(), save_file)\n",
        "                stop_count = 0\n",
        "            else:\n",
        "                stop_count += 1\n",
        "        if stop_count > 30:\n",
        "            print('Early Stopping')\n",
        "            break\n",
        "\n",
        "        output_str = 'best epoch : %d/%d / val accuracy : %f%%\\n' \\\n",
        "                     % (best_epoch+1, epochs, best_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        print('=' * 30)\n",
        "\n",
        "\n",
        "    output_str = 'best epoch : %d/%d / accuracy : %f%%\\n' \\\n",
        "                 % (best_epoch+1, epochs, best_accuracy)\n",
        "    sys.stdout.write(output_str)\n",
        "    check_file.write(output_str)\n",
        "    print('=' * 30)\n",
        "\n",
        "    check_file.close()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-qN1b_NBMC1",
        "outputId": "8a19c256-143d-4a2c-f0c0-1d82140d1f2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "save_filename = './train.pth'\n",
        "logging_filename = './logging.txt'\n",
        "train_signal_dir = 'signals/npy/Fpz-Cz/remove_wake/train/'\n",
        "test_signal_dir = 'signals/npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_dir = 'annotations/npy/remove_wake/'\n",
        "train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, \n",
        "                           test_signal_dir,annotations_dir,\n",
        "                           epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                           layer_filters=[64,128,256,512],first_conv=[200,40,100],\n",
        "                           optim='Adam',lf='CE',epsilon=0,noise_scale=0,\n",
        "                           min_value=-0,max_value=0,preprocessing=True,\n",
        "                           norm_methods='Standard',use_noise=False,loss_type='softmax')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preproceesing  :  True\n",
            "min/max value : 0.000000/0.000000\n",
            "noise scale :  0\n",
            "loss function :  CE\n",
            "epsilon :  0\n",
            "norm methods :  Standard\n",
            "logging file name :  ./logging.txt\n",
            "save file name :  ./train.pth\n",
            "layer filters :  [64, 128, 256, 512]\n",
            "fisrt_conv info :  [200, 40, 100]\n",
            "loss type :  softmax\n",
            "training data oversampling noise :  False\n",
            "train_dataset length :  30\n",
            "['SC4102EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4131EC-Hypnogram.npy']\n",
            "test_dataset length :  9\n",
            "['SC4082EP-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy']\n",
            "can use CUDA!!!\n",
            "torch.cuda.device_count() :  1\n",
            "Optimizer : Adam\n",
            "train_batch_size :  6\n",
            "current_lr : 0.001000\n",
            "train dataset : 1/2000 epochs spend time : 1.4851 sec / total_loss : 1.7999 correct : 3998/30515 -> 13.1018%\n",
            "test dataset : 1/2000 epochs spend time : 0.2740 sec  / total_loss : 1.7307 correct : 1498/7905 -> 18.9500%\n",
            "best epoch : 1/2000 / val accuracy : 18.950032%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 2/2000 epochs spend time : 1.4633 sec / total_loss : 1.7740 correct : 5232/30515 -> 17.1457%\n",
            "test dataset : 2/2000 epochs spend time : 0.2801 sec  / total_loss : 1.6481 correct : 3400/7905 -> 43.0108%\n",
            "best epoch : 2/2000 / val accuracy : 43.010753%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 3/2000 epochs spend time : 1.4573 sec / total_loss : 1.5646 correct : 12107/30515 -> 39.6756%\n",
            "test dataset : 3/2000 epochs spend time : 0.2959 sec  / total_loss : 1.5312 correct : 3617/7905 -> 45.7559%\n",
            "best epoch : 3/2000 / val accuracy : 45.755851%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 4/2000 epochs spend time : 1.4529 sec / total_loss : 1.4586 correct : 15087/30515 -> 49.4413%\n",
            "test dataset : 4/2000 epochs spend time : 0.2988 sec  / total_loss : 1.3494 correct : 4140/7905 -> 52.3719%\n",
            "best epoch : 4/2000 / val accuracy : 52.371917%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 5/2000 epochs spend time : 1.5172 sec / total_loss : 1.3986 correct : 15134/30515 -> 49.5953%\n",
            "test dataset : 5/2000 epochs spend time : 0.2897 sec  / total_loss : 1.3022 correct : 4312/7905 -> 54.5478%\n",
            "best epoch : 5/2000 / val accuracy : 54.547755%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 6/2000 epochs spend time : 1.5334 sec / total_loss : 1.3571 correct : 15948/30515 -> 52.2628%\n",
            "test dataset : 6/2000 epochs spend time : 0.2933 sec  / total_loss : 1.3828 correct : 3871/7905 -> 48.9690%\n",
            "best epoch : 5/2000 / val accuracy : 54.547755%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 7/2000 epochs spend time : 1.5214 sec / total_loss : 1.3203 correct : 16037/30515 -> 52.5545%\n",
            "test dataset : 7/2000 epochs spend time : 0.2896 sec  / total_loss : 1.2681 correct : 4603/7905 -> 58.2290%\n",
            "best epoch : 7/2000 / val accuracy : 58.228969%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 8/2000 epochs spend time : 1.4899 sec / total_loss : 1.3631 correct : 15095/30515 -> 49.4675%\n",
            "test dataset : 8/2000 epochs spend time : 0.2942 sec  / total_loss : 1.3353 correct : 3996/7905 -> 50.5503%\n",
            "best epoch : 7/2000 / val accuracy : 58.228969%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 9/2000 epochs spend time : 1.5184 sec / total_loss : 1.3065 correct : 15845/30515 -> 51.9253%\n",
            "test dataset : 9/2000 epochs spend time : 0.2962 sec  / total_loss : 1.2011 correct : 4625/7905 -> 58.5073%\n",
            "best epoch : 9/2000 / val accuracy : 58.507274%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 10/2000 epochs spend time : 1.5072 sec / total_loss : 1.2472 correct : 16848/30515 -> 55.2122%\n",
            "test dataset : 10/2000 epochs spend time : 0.2895 sec  / total_loss : 1.2419 correct : 4360/7905 -> 55.1550%\n",
            "best epoch : 9/2000 / val accuracy : 58.507274%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 11/2000 epochs spend time : 1.4675 sec / total_loss : 1.2459 correct : 16717/30515 -> 54.7829%\n",
            "test dataset : 11/2000 epochs spend time : 0.2831 sec  / total_loss : 1.1854 correct : 4639/7905 -> 58.6844%\n",
            "best epoch : 11/2000 / val accuracy : 58.684377%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 12/2000 epochs spend time : 1.4739 sec / total_loss : 1.2572 correct : 16522/30515 -> 54.1439%\n",
            "test dataset : 12/2000 epochs spend time : 0.2707 sec  / total_loss : 1.1970 correct : 4572/7905 -> 57.8368%\n",
            "best epoch : 11/2000 / val accuracy : 58.684377%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 13/2000 epochs spend time : 1.4300 sec / total_loss : 1.2187 correct : 16860/30515 -> 55.2515%\n",
            "test dataset : 13/2000 epochs spend time : 0.2744 sec  / total_loss : 1.2446 correct : 4539/7905 -> 57.4194%\n",
            "best epoch : 11/2000 / val accuracy : 58.684377%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 14/2000 epochs spend time : 1.4293 sec / total_loss : 1.2086 correct : 17468/30515 -> 57.2440%\n",
            "test dataset : 14/2000 epochs spend time : 0.2712 sec  / total_loss : 1.1622 correct : 4607/7905 -> 58.2796%\n",
            "best epoch : 11/2000 / val accuracy : 58.684377%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 15/2000 epochs spend time : 1.4893 sec / total_loss : 1.1926 correct : 17408/30515 -> 57.0474%\n",
            "test dataset : 15/2000 epochs spend time : 0.2763 sec  / total_loss : 1.1540 correct : 4732/7905 -> 59.8608%\n",
            "best epoch : 15/2000 / val accuracy : 59.860848%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 16/2000 epochs spend time : 1.4534 sec / total_loss : 1.1731 correct : 17529/30515 -> 57.4439%\n",
            "test dataset : 16/2000 epochs spend time : 0.2806 sec  / total_loss : 1.3189 correct : 4127/7905 -> 52.2075%\n",
            "best epoch : 15/2000 / val accuracy : 59.860848%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 17/2000 epochs spend time : 1.4713 sec / total_loss : 1.2035 correct : 17066/30515 -> 55.9266%\n",
            "test dataset : 17/2000 epochs spend time : 0.2677 sec  / total_loss : 1.2909 correct : 4202/7905 -> 53.1562%\n",
            "best epoch : 15/2000 / val accuracy : 59.860848%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 18/2000 epochs spend time : 1.4727 sec / total_loss : 1.1866 correct : 17428/30515 -> 57.1129%\n",
            "test dataset : 18/2000 epochs spend time : 0.2860 sec  / total_loss : 1.1447 correct : 4729/7905 -> 59.8229%\n",
            "best epoch : 15/2000 / val accuracy : 59.860848%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 19/2000 epochs spend time : 1.4597 sec / total_loss : 1.1563 correct : 17752/30515 -> 58.1747%\n",
            "test dataset : 19/2000 epochs spend time : 0.2715 sec  / total_loss : 1.1384 correct : 4703/7905 -> 59.4940%\n",
            "best epoch : 15/2000 / val accuracy : 59.860848%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 20/2000 epochs spend time : 1.4442 sec / total_loss : 1.1945 correct : 17297/30515 -> 56.6836%\n",
            "test dataset : 20/2000 epochs spend time : 0.2812 sec  / total_loss : 1.1487 correct : 4714/7905 -> 59.6331%\n",
            "best epoch : 15/2000 / val accuracy : 59.860848%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 21/2000 epochs spend time : 1.4621 sec / total_loss : 1.1360 correct : 17996/30515 -> 58.9743%\n",
            "test dataset : 21/2000 epochs spend time : 0.2706 sec  / total_loss : 1.1385 correct : 4790/7905 -> 60.5946%\n",
            "best epoch : 21/2000 / val accuracy : 60.594560%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 22/2000 epochs spend time : 1.4424 sec / total_loss : 1.1780 correct : 17520/30515 -> 57.4144%\n",
            "test dataset : 22/2000 epochs spend time : 0.2881 sec  / total_loss : 1.1212 correct : 4858/7905 -> 61.4548%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 23/2000 epochs spend time : 1.4566 sec / total_loss : 1.1361 correct : 17883/30515 -> 58.6040%\n",
            "test dataset : 23/2000 epochs spend time : 0.2769 sec  / total_loss : 1.1373 correct : 4737/7905 -> 59.9241%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 24/2000 epochs spend time : 1.4711 sec / total_loss : 1.1335 correct : 17832/30515 -> 58.4368%\n",
            "test dataset : 24/2000 epochs spend time : 0.2795 sec  / total_loss : 1.1688 correct : 4539/7905 -> 57.4194%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 25/2000 epochs spend time : 1.4575 sec / total_loss : 1.1166 correct : 17933/30515 -> 58.7678%\n",
            "test dataset : 25/2000 epochs spend time : 0.2755 sec  / total_loss : 1.1405 correct : 4624/7905 -> 58.4946%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 26/2000 epochs spend time : 1.4808 sec / total_loss : 1.1303 correct : 18002/30515 -> 58.9939%\n",
            "test dataset : 26/2000 epochs spend time : 0.2949 sec  / total_loss : 1.1512 correct : 4623/7905 -> 58.4820%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 27/2000 epochs spend time : 1.4509 sec / total_loss : 1.1177 correct : 17970/30515 -> 58.8891%\n",
            "test dataset : 27/2000 epochs spend time : 0.2874 sec  / total_loss : 1.1676 correct : 4482/7905 -> 56.6983%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 28/2000 epochs spend time : 1.4534 sec / total_loss : 1.1317 correct : 17687/30515 -> 57.9617%\n",
            "test dataset : 28/2000 epochs spend time : 0.2756 sec  / total_loss : 1.1304 correct : 4691/7905 -> 59.3422%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 29/2000 epochs spend time : 1.4566 sec / total_loss : 1.1237 correct : 17855/30515 -> 58.5122%\n",
            "test dataset : 29/2000 epochs spend time : 0.3037 sec  / total_loss : 1.1228 correct : 4649/7905 -> 58.8109%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 30/2000 epochs spend time : 1.4446 sec / total_loss : 1.1025 correct : 18189/30515 -> 59.6068%\n",
            "test dataset : 30/2000 epochs spend time : 0.2776 sec  / total_loss : 1.1433 correct : 4572/7905 -> 57.8368%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 31/2000 epochs spend time : 1.4334 sec / total_loss : 1.0872 correct : 18056/30515 -> 59.1709%\n",
            "test dataset : 31/2000 epochs spend time : 0.2786 sec  / total_loss : 1.1374 correct : 4541/7905 -> 57.4447%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 32/2000 epochs spend time : 1.4526 sec / total_loss : 1.0924 correct : 18083/30515 -> 59.2594%\n",
            "test dataset : 32/2000 epochs spend time : 0.2867 sec  / total_loss : 1.1588 correct : 4440/7905 -> 56.1670%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 33/2000 epochs spend time : 1.4469 sec / total_loss : 1.0829 correct : 18212/30515 -> 59.6821%\n",
            "test dataset : 33/2000 epochs spend time : 0.2814 sec  / total_loss : 1.1616 correct : 4415/7905 -> 55.8507%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 34/2000 epochs spend time : 1.4463 sec / total_loss : 1.0708 correct : 18371/30515 -> 60.2032%\n",
            "test dataset : 34/2000 epochs spend time : 0.2770 sec  / total_loss : 1.1615 correct : 4426/7905 -> 55.9899%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 35/2000 epochs spend time : 1.4726 sec / total_loss : 1.0661 correct : 18278/30515 -> 59.8984%\n",
            "test dataset : 35/2000 epochs spend time : 0.2777 sec  / total_loss : 1.1967 correct : 4333/7905 -> 54.8134%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 36/2000 epochs spend time : 1.4564 sec / total_loss : 1.0560 correct : 18530/30515 -> 60.7242%\n",
            "test dataset : 36/2000 epochs spend time : 0.2794 sec  / total_loss : 1.1655 correct : 4371/7905 -> 55.2941%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 37/2000 epochs spend time : 1.4763 sec / total_loss : 1.0561 correct : 18491/30515 -> 60.5964%\n",
            "test dataset : 37/2000 epochs spend time : 0.2827 sec  / total_loss : 1.1453 correct : 4494/7905 -> 56.8501%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 38/2000 epochs spend time : 1.4837 sec / total_loss : 1.0576 correct : 18429/30515 -> 60.3932%\n",
            "test dataset : 38/2000 epochs spend time : 0.2886 sec  / total_loss : 1.1203 correct : 4571/7905 -> 57.8242%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 39/2000 epochs spend time : 1.4602 sec / total_loss : 1.0516 correct : 18449/30515 -> 60.4588%\n",
            "test dataset : 39/2000 epochs spend time : 0.2856 sec  / total_loss : 1.1433 correct : 4479/7905 -> 56.6603%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 40/2000 epochs spend time : 1.4700 sec / total_loss : 1.0613 correct : 18261/30515 -> 59.8427%\n",
            "test dataset : 40/2000 epochs spend time : 0.2843 sec  / total_loss : 1.1681 correct : 4395/7905 -> 55.5977%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 41/2000 epochs spend time : 1.4777 sec / total_loss : 1.0573 correct : 18570/30515 -> 60.8553%\n",
            "test dataset : 41/2000 epochs spend time : 0.2866 sec  / total_loss : 1.1600 correct : 4413/7905 -> 55.8254%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 42/2000 epochs spend time : 1.4391 sec / total_loss : 1.0448 correct : 18524/30515 -> 60.7046%\n",
            "test dataset : 42/2000 epochs spend time : 0.2666 sec  / total_loss : 1.1830 correct : 4339/7905 -> 54.8893%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 43/2000 epochs spend time : 1.4411 sec / total_loss : 1.0474 correct : 18518/30515 -> 60.6849%\n",
            "test dataset : 43/2000 epochs spend time : 0.2749 sec  / total_loss : 1.1517 correct : 4471/7905 -> 56.5591%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 44/2000 epochs spend time : 1.4786 sec / total_loss : 1.0444 correct : 18475/30515 -> 60.5440%\n",
            "test dataset : 44/2000 epochs spend time : 0.2761 sec  / total_loss : 1.1605 correct : 4373/7905 -> 55.3194%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 45/2000 epochs spend time : 1.4619 sec / total_loss : 1.0490 correct : 18537/30515 -> 60.7472%\n",
            "test dataset : 45/2000 epochs spend time : 0.2837 sec  / total_loss : 1.1782 correct : 4311/7905 -> 54.5351%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 46/2000 epochs spend time : 1.4314 sec / total_loss : 1.0462 correct : 18527/30515 -> 60.7144%\n",
            "test dataset : 46/2000 epochs spend time : 0.2773 sec  / total_loss : 1.1577 correct : 4417/7905 -> 55.8760%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 47/2000 epochs spend time : 1.4533 sec / total_loss : 1.0393 correct : 18560/30515 -> 60.8225%\n",
            "test dataset : 47/2000 epochs spend time : 0.2809 sec  / total_loss : 1.1740 correct : 4344/7905 -> 54.9526%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 48/2000 epochs spend time : 1.4454 sec / total_loss : 1.0466 correct : 18336/30515 -> 60.0885%\n",
            "test dataset : 48/2000 epochs spend time : 0.2768 sec  / total_loss : 1.1985 correct : 4223/7905 -> 53.4219%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 49/2000 epochs spend time : 1.4324 sec / total_loss : 1.0660 correct : 18188/30515 -> 59.6035%\n",
            "test dataset : 49/2000 epochs spend time : 0.2722 sec  / total_loss : 1.1297 correct : 4532/7905 -> 57.3308%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 50/2000 epochs spend time : 1.4699 sec / total_loss : 1.0396 correct : 18501/30515 -> 60.6292%\n",
            "test dataset : 50/2000 epochs spend time : 0.2837 sec  / total_loss : 1.1460 correct : 4436/7905 -> 56.1164%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 51/2000 epochs spend time : 1.4549 sec / total_loss : 1.0275 correct : 18639/30515 -> 61.0814%\n",
            "test dataset : 51/2000 epochs spend time : 0.2803 sec  / total_loss : 1.1530 correct : 4426/7905 -> 55.9899%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 52/2000 epochs spend time : 1.4600 sec / total_loss : 1.0287 correct : 18684/30515 -> 61.2289%\n",
            "test dataset : 52/2000 epochs spend time : 0.2818 sec  / total_loss : 1.1337 correct : 4495/7905 -> 56.8627%\n",
            "best epoch : 22/2000 / val accuracy : 61.454775%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 53/2000 epochs spend time : 1.4573 sec / total_loss : 1.0304 correct : 18657/30515 -> 61.1404%\n",
            "test dataset : 53/2000 epochs spend time : 0.2809 sec  / total_loss : 1.1494 correct : 4435/7905 -> 56.1037%\n",
            "Early Stopping\n",
            "best epoch : 22/2000 / accuracy : 61.454775%\n",
            "==============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCE73qoRBUC2"
      },
      "source": [
        ""
      ],
      "execution_count": 63,
      "outputs": []
    }
  ]
}
