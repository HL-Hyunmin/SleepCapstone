{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SleepDataPreparation_and_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/SleepCapstone/blob/master/Codes/simpleFC_with_Relu_56.3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAGJRSb3xOT1"
      },
      "source": [
        "한림대학교 소프트웨어 융합대학\n",
        "빅데이터 캡스톤 프로젝트\n",
        "\n",
        "딥러닝에 기반한 수면 분류 모델 개발"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SaQVdPHgfmx",
        "outputId": "6dec0213-fe3d-4de1-e932-60961096cb2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohoEZomKw0cz",
        "outputId": "84599922-d567-435e-fdd4-8408b3988fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd drive/My\\ Drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvna-5vRzmTQ",
        "outputId": "f41ecb6f-e1c8-4f33-f2ff-9cdacdb3a862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "# GPU 가용성 체크\n",
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Oct 16 11:58:51 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1zNHBNc4beq"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def search_signals_npy(dirname):\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for file in filenames if file.endswith(\".npy\")]\n",
        "    return filenames\n",
        "\n",
        "def search_correct_signals_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vSGY6sO9vgC",
        "outputId": "8e1251bc-aa28-4106-819c-cdf9b19fac2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "\n",
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  'annotations/npy/remove_wake/'\n",
        "signals_path = 'signals/npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "total_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list:\n",
        "    label = np.load(path + filename)\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    \n",
        "    signals = np.load(signals_path+signals_filename)\n",
        "    \n",
        "    #print('remove start index : %d / remove end index : %d'%(remove_start_index,remove_end_index))\n",
        "    #print(np.bincount(label,minlength=6))\n",
        "    if len(label) !=len(signals[0])//30//fs:\n",
        "        print('file is fault!!!')\n",
        "    for i in range(6):\n",
        "        total_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "print(total_label)\n",
        "\n",
        "x = np.arange(len(total_label))\n",
        "\n",
        "plt.bar(x,total_label,width=0.7)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4336  2804 17799  5703  7717    61]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 6 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVVZuGLx-ADW",
        "outputId": "40a1cb7b-f170-4f0c-cc96-3c110d412eb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  'annotations/npy/remove_wake/'\n",
        "signals_path = 'signals/npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "random.shuffle(annotations_npy_list)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "trainDataset_count = 30\n",
        "testDataset_count = len(annotations_npy_list)-trainDataset_count\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4001EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4192EV-Hypnogram.npy']\n",
            "['SC4161EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy']\n",
            "[10.91263731  7.16558875 46.47024604 14.62546624 20.67891729  0.14714437]\n",
            "[12.47145808  7.71990867 45.87365445 15.53767533 18.20158747  0.19571599]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAKH9Mb0_NF3"
      },
      "source": [
        "signals_path = 'signals/npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "save_train_path = 'signals/npy//Fpz-Cz/remove_wake/train/'\n",
        "save_test_path = 'signals/npy/Fpz-Cz/remove_wake/test/'\n",
        "\n",
        "os.makedirs(save_train_path,exist_ok=True)\n",
        "os.makedirs(save_test_path,exist_ok=True)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_train_path+filename)\n",
        "    \n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_test_path+filename)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4EYtlUH-e9f",
        "outputId": "43afd9db-0f61-473f-983e-a3476715d1e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "def search_correct_annotations_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename\n",
        "\n",
        "train_path =  'signals/npy/Fpz-Cz/remove_wake/train/'\n",
        "test_path = 'signals/npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_path = 'annotations/npy/remove_wake/'\n",
        "\n",
        "train_list = search_signals_npy(train_path)\n",
        "test_list = search_signals_npy(test_path)\n",
        "\n",
        "print(train_list)\n",
        "print(test_list)\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in train_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in test_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4161EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4151EC-Hypnogram.npy']\n",
            "['SC4082EP-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy']\n",
            "[10.91263731  7.16558875 46.47024604 14.62546624 20.67891729  0.14714437]\n",
            "[12.47145808  7.71990867 45.87365445 15.53767533 18.20158747  0.19571599]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBwqVERu_aw-"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pylab as plt\n",
        "from  torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSQaEXQr_h8m"
      },
      "source": [
        "def data_preprocessing_torch(signals): # 하나의 데이터셋에 대한 data_preprocessing (using torch)\n",
        "    signals = (signals - signals.mean(dim=1).unsqueeze(1))/signals.std(dim=1).unsqueeze(1)\n",
        "\n",
        "    return signals\n",
        "\n",
        "def data_preprocessing_oneToOne_torch(signals,min,max,max_value):\n",
        "    signals_std = (signals + max_value) / (2*max_value)\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def data_preprocessing_minmax_torch(signals,min,max):\n",
        "    signals_std = (signals - signals.min(dim=1).unsqueeze(1)) / (\n",
        "            signals.max(dim=1).unsqueeze(1) - signals.min(dim=1).unsqueeze(1))\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def get_dataset_one_channel_norm_withoutCut(dirname,annotations_dir,data_path,use_noise=True,epsilon=0.5,noise_scale=2e-6,preprocessing=True,norm_methods='Standard'):\n",
        "    # npy read!\n",
        "    path = dirname + data_path\n",
        "    signals = np.load(path)\n",
        "\n",
        "    signals = torch.from_numpy(signals).float().to(device)\n",
        "\n",
        "    if use_noise:\n",
        "        if np.random.rand() < epsilon:\n",
        "            # noise = np.random.normal(loc=0,scale=noise_scale,size=signals.shape)\n",
        "            # signals = signals + noise\n",
        "            noise = torch.normal(mean=0., std=noise_scale, size=signals.shape).to(device)\n",
        "            signals = signals + noise\n",
        "\n",
        "    if preprocessing:\n",
        "        if norm_methods == 'Standard':\n",
        "            signals = data_preprocessing_torch(signals)\n",
        "        elif norm_methods == 'minmax':\n",
        "            signals = data_preprocessing_minmax_torch(signals,0,1)\n",
        "        elif norm_methods == 'oneToOne':\n",
        "            signals = data_preprocessing_oneToOne_torch(signals,-1,1,1e-4)\n",
        "    label = get_annotations(annotations_dir, data_path)\n",
        "    return signals, label\n",
        "\n",
        "# model conv layer weight init function\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:         # Conv weight init\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "        \n",
        "def suffle_dataset_list(dataset_list): # 데이터 셔플\n",
        "    random.shuffle(dataset_list)\n",
        "    return dataset_list\n",
        "\n",
        "\n",
        "# npy파일을 통해 label을 가져오는 함수\n",
        "def get_annotations(label_dir,file_name):\n",
        "    label_path = label_dir + file_name\n",
        "    label = np.load(label_path)\n",
        "    return label\n",
        "\n",
        "def signals_expand_torch_one_channel(signals): # 2차원 데이터를 3차원으로 변환 (8,N) -> (batch,8,6000) 형태로\n",
        "    signals = signals.expand(1,1,-1)\n",
        "    #print(signals.shape)\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    signals = signals.reshape(-1, 3000, 1) # 형태 변환\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    return signals"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx4M7Rfq_lwo"
      },
      "source": [
        "class DeepSleepNet_Classification(nn.Module):  # input channel = 8channel / output = 5\n",
        "    def __init__(self,in_channel=1,out_channel=6,layer=[64,128,128,128],sample_rate = 100):\n",
        "        super(DeepSleepNet_Classification, self).__init__()\n",
        "\n",
        "        \n",
        "        self.fc1 = nn.Linear(3000,1024)\n",
        "        self.fc2 = nn.Linear(1024,1024)\n",
        "        self.fc3 = nn.Linear(1024,512)\n",
        "        self.fc4 = nn.Linear(512, 256)\n",
        "        self.fc5 = nn.Linear(256, out_channel)\n",
        "\n",
        "        self.ReLU = nn.ReLU()\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "        # print(\"feature_extract_2d.shape : \", feature_extract_2d.shape)\n",
        "        # 여기서 문제 발생 weight의 경우에는 [64 , 32 , 100] 이지만 input 이 2차원 [32, 750]이라 문제 발생!\n",
        "        out = torch.flatten(input, 1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.fc4(out)\n",
        "        out = self.ReLU(out)\n",
        "        out = self.fc5(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYZ6YSZHArdJ",
        "outputId": "41c8e367-ef5b-46b2-d032-7a2f55252f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "model = DeepSleepNet_Classification(in_channel=1,out_channel=6)\n",
        "summary(model.cuda(),(1,3000))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 1024]       3,073,024\n",
            "              ReLU-2                 [-1, 1024]               0\n",
            "            Linear-3                 [-1, 1024]       1,049,600\n",
            "              ReLU-4                 [-1, 1024]               0\n",
            "            Linear-5                  [-1, 512]         524,800\n",
            "              ReLU-6                  [-1, 512]               0\n",
            "            Linear-7                  [-1, 256]         131,328\n",
            "              ReLU-8                  [-1, 256]               0\n",
            "            Linear-9                    [-1, 6]           1,542\n",
            "================================================================\n",
            "Total params: 4,780,294\n",
            "Trainable params: 4,780,294\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.04\n",
            "Params size (MB): 18.24\n",
            "Estimated Total Size (MB): 18.29\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li3ftAC1Az0Q"
      },
      "source": [
        "def search_npy_list(dirname):  # 매개변수 dir에서 모든 npy파일을 찾고 fold에 따른 dataset 나누기\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for _, file in enumerate(filenames) if file.endswith(\".npy\")]\n",
        "    return filenames\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQH_VRRFBGF1"
      },
      "source": [
        "def train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, test_signal_dir,annotations_dir\n",
        "                               ,epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                               layer_filters=[64,128,256,512],first_conv=[200,40,100],optim='Adam',lf='CE',\n",
        "                               epsilon=0.7,noise_scale=2e-6,min_value=-1e-4,max_value=1e-4,preprocessing=True,\n",
        "                               norm_methods='Standard',use_noise=True,loss_type='softmax'):\n",
        "    # Adam optimizer param\n",
        "    b1 = 0.5\n",
        "    b2 = 0.999\n",
        "\n",
        "    beta = 0.001\n",
        "\n",
        "    check_file = open(logging_filename, 'w')  # logging file\n",
        "\n",
        "    print('Preproceesing  : ',preprocessing)\n",
        "    print('min/max value : %f/%f'%(min_value,max_value))\n",
        "    print('noise scale : ',noise_scale)\n",
        "    print('loss function : ',lf)\n",
        "    print('epsilon : ',epsilon)\n",
        "    print('norm methods : ',norm_methods)\n",
        "\n",
        "    print('logging file name : ', logging_filename)\n",
        "    print('save file name : ', save_filename)\n",
        "    print('layer filters : ',layer_filters)\n",
        "    print('fisrt_conv info : ',first_conv)\n",
        "    print('loss type : ',loss_type)\n",
        "    print('training data oversampling noise : ',use_noise)\n",
        "    best_accuracy = 0.\n",
        "    best_epoch = 0\n",
        "\n",
        "    train_dataset_list = search_npy_list(train_signal_dir)\n",
        "    test_dataset_list = search_npy_list(test_signal_dir)\n",
        "\n",
        "    train_dataset_len = len(train_dataset_list)\n",
        "    test_dataset_len = len(test_dataset_list)\n",
        "\n",
        "    print('train_dataset length : ', len(train_dataset_list))\n",
        "    print(train_dataset_list)\n",
        "\n",
        "    print('test_dataset length : ',test_dataset_len)\n",
        "    print(test_dataset_list)\n",
        "\n",
        "\n",
        "    model = DeepSleepNet_Classification()\n",
        "\n",
        "    #model = resnet18_200hz(in_channel=1,layer_filters=layer_filters,first_conv=first_conv,use_batchnorm=True,num_classes=5)\n",
        "\n",
        "    model.apply(weights_init) # weight init\n",
        "\n",
        "    cuda = torch.cuda.is_available()\n",
        "\n",
        "    if cuda:\n",
        "        print('can use CUDA!!!')\n",
        "        model = model.cuda()\n",
        "    #summary(model,[1,6000])\n",
        "    print('torch.cuda.device_count() : ', torch.cuda.device_count())\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print('Multi GPU Activation !!!')\n",
        "        #model = nn.DataParallel(model)\n",
        "\n",
        "    # loss funcition\n",
        "    if lf == 'CE':\n",
        "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "    elif lf == 'CEW':\n",
        "        samples_per_cls = [27,15,41,5,11]\n",
        "        no_of_classes = 5\n",
        "        effective_num = 1.0 - np.power(beta,samples_per_cls)\n",
        "        #print(effective_num)\n",
        "        weights = (1.0 - beta) / np.array(effective_num)\n",
        "        #print(weights)\n",
        "        weights = weights / np.sum(weights) * no_of_classes\n",
        "        weights = torch.tensor(weights).float()\n",
        "        weights = weights.to(device)\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=weights).to(device)\n",
        "    elif lf == 'FL':\n",
        "        loss_fn = FocalLoss(gamma=2).to(device)\n",
        "    elif lf == 'CBL':\n",
        "        loss_fn = CB_loss(samples_per_cls=[27,15,41,5,11],no_of_classes=5,loss_type=loss_type,beta=0.9999,gamma=2.0)\n",
        "    #loss_fn = FocalLoss(gamma=2).to(device)\n",
        "\n",
        "    # optimizer ADAM (SGD의 경우에는 정상적으로 학습이 진행되지 않았음)\n",
        "    if optim == 'Adam':\n",
        "        print('Optimizer : Adam')\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "    elif optim == 'RMS':\n",
        "        print('Optimizer : RMSprop')\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "    elif optim == 'SGD':\n",
        "        print('Optimizer : SGD')\n",
        "        optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=gamma, patience=10,\n",
        "                                                           min_lr=1e-6)\n",
        "    #stride = 40 일 때, batch_size = 20이면 16GB정도의 메모리 사용\n",
        "    batch_size = 5\n",
        "    norm_square = 2\n",
        "\n",
        "    train_batch_size = math.ceil(train_dataset_len / batch_size)\n",
        "    print('train_batch_size : ',train_batch_size)\n",
        "\n",
        "    test_batch_size = test_dataset_len\n",
        "\n",
        "    best_accuracy = 0.\n",
        "    stop_count = 0\n",
        "    for epoch in range(epochs):\n",
        "        train_dataset = suffle_dataset_list(train_dataset_list) # 매 epoch마다 train_dataset shuffle !\n",
        "        count = 0  # check batch\n",
        "        train_total_loss = 0.0\n",
        "        train_total_count = 0\n",
        "        train_total_data = 0\n",
        "\n",
        "        val_total_loss = 0.0\n",
        "        val_total_count = 0\n",
        "        val_total_data = 0\n",
        "\n",
        "        test_total_loss = 0.0\n",
        "        test_total_count = 0\n",
        "        test_total_data = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "\n",
        "        output_str = 'current_lr : %f\\n'%(optimizer.state_dict()['param_groups'][0]['lr'])\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "        for index, file_name in enumerate(train_dataset):\n",
        "            #print('index : ',index)\n",
        "            if index % batch_size == 0:\n",
        "                batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "            else:\n",
        "                new_signal, new_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "                batch_signal = torch.cat((batch_signal, new_signal),dim=1)\n",
        "                batch_label = np.concatenate((batch_label, new_label))\n",
        "            count += 1\n",
        "            if count == batch_size or index == len(train_dataset) - 1:  # batch 학습 시작!\n",
        "                batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                #batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                # batch_signal = torch.from_numpy(batch_signal).float().to(device)\n",
        "                batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "                optimizer.zero_grad()\n",
        "                # print(batch_signal.shape)\n",
        "                # print(batch_signal)\n",
        "                pred = model(batch_signal)\n",
        "                norm = 0\n",
        "\n",
        "                for parameter in model.parameters():\n",
        "                    norm += torch.norm(parameter, p=norm_square)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label) + beta * norm\n",
        "                #print('loss : ',loss.item())\n",
        "                # loss = loss_fn(pred, batch_label)\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                train_total_loss += loss.item()\n",
        "\n",
        "                train_total_count += check_count\n",
        "                train_total_data += len(batch_signal)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "                count = 0\n",
        "\n",
        "        train_total_loss /= train_batch_size\n",
        "        train_accuracy = train_total_count / train_total_data * 100\n",
        "\n",
        "        output_str = 'train dataset : %d/%d epochs spend time : %.4f sec / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, train_total_loss,\n",
        "                        train_total_count, train_total_data, train_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "    \n",
        "\n",
        "        #check test dataset\n",
        "        start_time = time.time()\n",
        "        for file_name in test_dataset_list:\n",
        "            batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(test_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=False,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "            batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "\n",
        "            batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred = model(batch_signal)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label)\n",
        "\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                test_total_loss += loss.item()\n",
        "                test_total_count += check_count\n",
        "                test_total_data += len(batch_signal)\n",
        "\n",
        "                # 사용하지 않는 변수 제거\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "        test_total_loss /= test_batch_size\n",
        "        test_accuracy = test_total_count / test_total_data * 100\n",
        "\n",
        "\n",
        "\n",
        "        output_str = 'test dataset : %d/%d epochs spend time : %.4f sec  / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, test_total_loss,\n",
        "                        test_total_count, test_total_data, test_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "        scheduler.step(float(test_total_loss))\n",
        "        #scheduler.step()\n",
        "\n",
        "        if epoch == 0:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_epoch = epoch\n",
        "            save_file = save_filename\n",
        "            #save_file = save_path + 'best_SleepEEGNet_CNN_channel%d.pth'%channel\n",
        "            torch.save(model.state_dict(),save_file)\n",
        "            stop_count = 0\n",
        "        else:\n",
        "            if best_accuracy < test_accuracy:\n",
        "                best_accuracy = test_accuracy\n",
        "                best_epoch = epoch\n",
        "                save_file = save_filename\n",
        "                torch.save(model.state_dict(), save_file)\n",
        "                stop_count = 0\n",
        "            else:\n",
        "                stop_count += 1\n",
        "        if stop_count > 30:\n",
        "            print('Early Stopping')\n",
        "            break\n",
        "\n",
        "        output_str = 'best epoch : %d/%d / val accuracy : %f%%\\n' \\\n",
        "                     % (best_epoch+1, epochs, best_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        print('=' * 30)\n",
        "\n",
        "\n",
        "    output_str = 'best epoch : %d/%d / accuracy : %f%%\\n' \\\n",
        "                 % (best_epoch+1, epochs, best_accuracy)\n",
        "    sys.stdout.write(output_str)\n",
        "    check_file.write(output_str)\n",
        "    print('=' * 30)\n",
        "\n",
        "    check_file.close()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-qN1b_NBMC1",
        "outputId": "d70578cb-3662-4600-f0e6-49527fe75a0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "save_filename = './train.pth'\n",
        "logging_filename = './logging.txt'\n",
        "train_signal_dir = 'signals/npy/Fpz-Cz/remove_wake/train/'\n",
        "test_signal_dir = 'signals/npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_dir = 'annotations/npy/remove_wake/'\n",
        "train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, \n",
        "                           test_signal_dir,annotations_dir,\n",
        "                           epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                           layer_filters=[64,128,256,512],first_conv=[200,40,100],\n",
        "                           optim='Adam',lf='CE',epsilon=0,noise_scale=0,\n",
        "                           min_value=-0,max_value=0,preprocessing=True,\n",
        "                           norm_methods='Standard',use_noise=False,loss_type='softmax')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preproceesing  :  True\n",
            "min/max value : 0.000000/0.000000\n",
            "noise scale :  0\n",
            "loss function :  CE\n",
            "epsilon :  0\n",
            "norm methods :  Standard\n",
            "logging file name :  ./logging.txt\n",
            "save file name :  ./train.pth\n",
            "layer filters :  [64, 128, 256, 512]\n",
            "fisrt_conv info :  [200, 40, 100]\n",
            "loss type :  softmax\n",
            "training data oversampling noise :  False\n",
            "train_dataset length :  30\n",
            "['SC4161EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4162EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4151EC-Hypnogram.npy']\n",
            "test_dataset length :  9\n",
            "['SC4082EP-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy']\n",
            "can use CUDA!!!\n",
            "torch.cuda.device_count() :  1\n",
            "Optimizer : Adam\n",
            "train_batch_size :  6\n",
            "current_lr : 0.001000\n",
            "train dataset : 1/2000 epochs spend time : 2.0907 sec / total_loss : 1.6675 correct : 10831/29223 -> 37.0633%\n",
            "test dataset : 1/2000 epochs spend time : 0.5566 sec  / total_loss : 1.4433 correct : 4223/9197 -> 45.9171%\n",
            "best epoch : 1/2000 / val accuracy : 45.917147%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 2/2000 epochs spend time : 2.0009 sec / total_loss : 1.4937 correct : 12275/29223 -> 42.0046%\n",
            "test dataset : 2/2000 epochs spend time : 0.5343 sec  / total_loss : 1.3761 correct : 4212/9197 -> 45.7975%\n",
            "best epoch : 1/2000 / val accuracy : 45.917147%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 3/2000 epochs spend time : 1.9412 sec / total_loss : 1.5463 correct : 10281/29223 -> 35.1812%\n",
            "test dataset : 3/2000 epochs spend time : 0.5287 sec  / total_loss : 1.4479 correct : 3562/9197 -> 38.7300%\n",
            "best epoch : 1/2000 / val accuracy : 45.917147%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 4/2000 epochs spend time : 1.9500 sec / total_loss : 1.4521 correct : 13487/29223 -> 46.1520%\n",
            "test dataset : 4/2000 epochs spend time : 0.5339 sec  / total_loss : 1.4514 correct : 4220/9197 -> 45.8845%\n",
            "best epoch : 1/2000 / val accuracy : 45.917147%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 5/2000 epochs spend time : 1.9090 sec / total_loss : 1.3537 correct : 13736/29223 -> 47.0041%\n",
            "test dataset : 5/2000 epochs spend time : 0.5042 sec  / total_loss : 1.4715 correct : 4398/9197 -> 47.8199%\n",
            "best epoch : 5/2000 / val accuracy : 47.819941%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 6/2000 epochs spend time : 1.8551 sec / total_loss : 1.3034 correct : 14261/29223 -> 48.8006%\n",
            "test dataset : 6/2000 epochs spend time : 0.4838 sec  / total_loss : 1.2549 correct : 4199/9197 -> 45.6562%\n",
            "best epoch : 5/2000 / val accuracy : 47.819941%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 7/2000 epochs spend time : 1.8645 sec / total_loss : 1.2881 correct : 14847/29223 -> 50.8059%\n",
            "test dataset : 7/2000 epochs spend time : 0.4884 sec  / total_loss : 1.2413 correct : 4965/9197 -> 53.9850%\n",
            "best epoch : 7/2000 / val accuracy : 53.984995%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 8/2000 epochs spend time : 1.8744 sec / total_loss : 1.1247 correct : 16582/29223 -> 56.7430%\n",
            "test dataset : 8/2000 epochs spend time : 0.5022 sec  / total_loss : 1.3144 correct : 4796/9197 -> 52.1474%\n",
            "best epoch : 7/2000 / val accuracy : 53.984995%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 9/2000 epochs spend time : 1.8602 sec / total_loss : 1.1695 correct : 16239/29223 -> 55.5692%\n",
            "test dataset : 9/2000 epochs spend time : 0.4899 sec  / total_loss : 1.3655 correct : 4432/9197 -> 48.1896%\n",
            "best epoch : 7/2000 / val accuracy : 53.984995%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 10/2000 epochs spend time : 1.9418 sec / total_loss : 1.1405 correct : 16610/29223 -> 56.8388%\n",
            "test dataset : 10/2000 epochs spend time : 0.5119 sec  / total_loss : 1.9100 correct : 2820/9197 -> 30.6622%\n",
            "best epoch : 7/2000 / val accuracy : 53.984995%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 11/2000 epochs spend time : 1.8585 sec / total_loss : 1.1211 correct : 16135/29223 -> 55.2134%\n",
            "test dataset : 11/2000 epochs spend time : 0.4880 sec  / total_loss : 1.1891 correct : 5019/9197 -> 54.5721%\n",
            "best epoch : 11/2000 / val accuracy : 54.572143%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 12/2000 epochs spend time : 1.8574 sec / total_loss : 1.1437 correct : 17309/29223 -> 59.2307%\n",
            "test dataset : 12/2000 epochs spend time : 0.4851 sec  / total_loss : 1.3329 correct : 4923/9197 -> 53.5283%\n",
            "best epoch : 11/2000 / val accuracy : 54.572143%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 13/2000 epochs spend time : 1.7912 sec / total_loss : 0.9336 correct : 19342/29223 -> 66.1876%\n",
            "test dataset : 13/2000 epochs spend time : 0.4995 sec  / total_loss : 1.3812 correct : 4771/9197 -> 51.8756%\n",
            "best epoch : 11/2000 / val accuracy : 54.572143%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 14/2000 epochs spend time : 1.8728 sec / total_loss : 0.9543 correct : 18953/29223 -> 64.8564%\n",
            "test dataset : 14/2000 epochs spend time : 0.5191 sec  / total_loss : 1.1499 correct : 4793/9197 -> 52.1148%\n",
            "best epoch : 11/2000 / val accuracy : 54.572143%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 15/2000 epochs spend time : 1.7481 sec / total_loss : 0.9247 correct : 19179/29223 -> 65.6298%\n",
            "test dataset : 15/2000 epochs spend time : 0.4681 sec  / total_loss : 1.2514 correct : 4869/9197 -> 52.9412%\n",
            "best epoch : 11/2000 / val accuracy : 54.572143%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 16/2000 epochs spend time : 1.7842 sec / total_loss : 0.7453 correct : 21444/29223 -> 73.3806%\n",
            "test dataset : 16/2000 epochs spend time : 0.4652 sec  / total_loss : 1.7781 correct : 5082/9197 -> 55.2571%\n",
            "best epoch : 16/2000 / val accuracy : 55.257149%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 17/2000 epochs spend time : 1.7764 sec / total_loss : 1.2322 correct : 16627/29223 -> 56.8970%\n",
            "test dataset : 17/2000 epochs spend time : 0.4875 sec  / total_loss : 1.3174 correct : 4499/9197 -> 48.9181%\n",
            "best epoch : 16/2000 / val accuracy : 55.257149%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 18/2000 epochs spend time : 1.7594 sec / total_loss : 1.0901 correct : 17063/29223 -> 58.3889%\n",
            "test dataset : 18/2000 epochs spend time : 0.4956 sec  / total_loss : 1.8211 correct : 2641/9197 -> 28.7159%\n",
            "best epoch : 16/2000 / val accuracy : 55.257149%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 19/2000 epochs spend time : 1.8278 sec / total_loss : 1.2312 correct : 15836/29223 -> 54.1902%\n",
            "test dataset : 19/2000 epochs spend time : 0.4875 sec  / total_loss : 1.2486 correct : 5103/9197 -> 55.4855%\n",
            "best epoch : 19/2000 / val accuracy : 55.485484%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 20/2000 epochs spend time : 1.8023 sec / total_loss : 0.9590 correct : 18951/29223 -> 64.8496%\n",
            "test dataset : 20/2000 epochs spend time : 0.4758 sec  / total_loss : 1.1947 correct : 4989/9197 -> 54.2459%\n",
            "best epoch : 19/2000 / val accuracy : 55.485484%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 21/2000 epochs spend time : 1.7796 sec / total_loss : 0.7433 correct : 21765/29223 -> 74.4790%\n",
            "test dataset : 21/2000 epochs spend time : 0.4699 sec  / total_loss : 1.2718 correct : 4536/9197 -> 49.3204%\n",
            "best epoch : 19/2000 / val accuracy : 55.485484%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 22/2000 epochs spend time : 1.9678 sec / total_loss : 0.6721 correct : 22534/29223 -> 77.1105%\n",
            "test dataset : 22/2000 epochs spend time : 0.5133 sec  / total_loss : 1.4635 correct : 5182/9197 -> 56.3445%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 23/2000 epochs spend time : 1.8831 sec / total_loss : 0.5433 correct : 24012/29223 -> 82.1682%\n",
            "test dataset : 23/2000 epochs spend time : 0.5184 sec  / total_loss : 2.6918 correct : 4785/9197 -> 52.0278%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 24/2000 epochs spend time : 1.9227 sec / total_loss : 0.9119 correct : 20201/29223 -> 69.1271%\n",
            "test dataset : 24/2000 epochs spend time : 0.4868 sec  / total_loss : 1.3612 correct : 5089/9197 -> 55.3333%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 25/2000 epochs spend time : 1.8998 sec / total_loss : 0.5246 correct : 24189/29223 -> 82.7738%\n",
            "test dataset : 25/2000 epochs spend time : 0.4937 sec  / total_loss : 1.5616 correct : 4992/9197 -> 54.2786%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 26/2000 epochs spend time : 1.8006 sec / total_loss : 0.3935 correct : 25868/29223 -> 88.5193%\n",
            "test dataset : 26/2000 epochs spend time : 0.5182 sec  / total_loss : 1.7275 correct : 5031/9197 -> 54.7026%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 27/2000 epochs spend time : 1.8068 sec / total_loss : 0.3253 correct : 26642/29223 -> 91.1679%\n",
            "test dataset : 27/2000 epochs spend time : 0.5076 sec  / total_loss : 2.1325 correct : 5012/9197 -> 54.4960%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 28/2000 epochs spend time : 1.7804 sec / total_loss : 0.2832 correct : 26992/29223 -> 92.3656%\n",
            "test dataset : 28/2000 epochs spend time : 0.4827 sec  / total_loss : 2.1379 correct : 4989/9197 -> 54.2459%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 29/2000 epochs spend time : 1.8302 sec / total_loss : 0.2232 correct : 27747/29223 -> 94.9492%\n",
            "test dataset : 29/2000 epochs spend time : 0.4773 sec  / total_loss : 2.3013 correct : 4987/9197 -> 54.2242%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 30/2000 epochs spend time : 1.7861 sec / total_loss : 0.1941 correct : 28147/29223 -> 96.3180%\n",
            "test dataset : 30/2000 epochs spend time : 0.4781 sec  / total_loss : 2.5659 correct : 5013/9197 -> 54.5069%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 31/2000 epochs spend time : 1.7925 sec / total_loss : 0.3645 correct : 26337/29223 -> 90.1242%\n",
            "test dataset : 31/2000 epochs spend time : 0.4713 sec  / total_loss : 1.9751 correct : 5007/9197 -> 54.4417%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 32/2000 epochs spend time : 1.7911 sec / total_loss : 0.1962 correct : 28222/29223 -> 96.5746%\n",
            "test dataset : 32/2000 epochs spend time : 0.5015 sec  / total_loss : 2.2581 correct : 4973/9197 -> 54.0720%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 33/2000 epochs spend time : 1.8293 sec / total_loss : 0.1426 correct : 28742/29223 -> 98.3540%\n",
            "test dataset : 33/2000 epochs spend time : 0.4836 sec  / total_loss : 2.7186 correct : 4962/9197 -> 53.9524%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 34/2000 epochs spend time : 1.8127 sec / total_loss : 0.1229 correct : 28904/29223 -> 98.9084%\n",
            "test dataset : 34/2000 epochs spend time : 0.4801 sec  / total_loss : 2.8393 correct : 4947/9197 -> 53.7893%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 35/2000 epochs spend time : 1.7920 sec / total_loss : 0.1120 correct : 28960/29223 -> 99.1000%\n",
            "test dataset : 35/2000 epochs spend time : 0.4737 sec  / total_loss : 3.0085 correct : 4921/9197 -> 53.5066%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 36/2000 epochs spend time : 1.7891 sec / total_loss : 0.0989 correct : 29086/29223 -> 99.5312%\n",
            "test dataset : 36/2000 epochs spend time : 0.4846 sec  / total_loss : 3.2119 correct : 4904/9197 -> 53.3217%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 37/2000 epochs spend time : 1.7992 sec / total_loss : 0.0905 correct : 29139/29223 -> 99.7126%\n",
            "test dataset : 37/2000 epochs spend time : 0.5174 sec  / total_loss : 3.2860 correct : 4928/9197 -> 53.5827%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 38/2000 epochs spend time : 1.8319 sec / total_loss : 0.0841 correct : 29198/29223 -> 99.9145%\n",
            "test dataset : 38/2000 epochs spend time : 0.4680 sec  / total_loss : 3.4091 correct : 4924/9197 -> 53.5392%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 39/2000 epochs spend time : 1.7686 sec / total_loss : 0.0817 correct : 29199/29223 -> 99.9179%\n",
            "test dataset : 39/2000 epochs spend time : 0.4902 sec  / total_loss : 3.4684 correct : 4942/9197 -> 53.7349%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 40/2000 epochs spend time : 1.8017 sec / total_loss : 0.0802 correct : 29208/29223 -> 99.9487%\n",
            "test dataset : 40/2000 epochs spend time : 0.4861 sec  / total_loss : 3.5410 correct : 4941/9197 -> 53.7240%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 41/2000 epochs spend time : 1.7701 sec / total_loss : 0.0793 correct : 29213/29223 -> 99.9658%\n",
            "test dataset : 41/2000 epochs spend time : 0.4889 sec  / total_loss : 3.6459 correct : 4937/9197 -> 53.6805%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 42/2000 epochs spend time : 1.8480 sec / total_loss : 0.0785 correct : 29216/29223 -> 99.9760%\n",
            "test dataset : 42/2000 epochs spend time : 0.4729 sec  / total_loss : 3.7016 correct : 4927/9197 -> 53.5718%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 43/2000 epochs spend time : 1.7813 sec / total_loss : 0.0779 correct : 29218/29223 -> 99.9829%\n",
            "test dataset : 43/2000 epochs spend time : 0.4906 sec  / total_loss : 3.7596 correct : 4936/9197 -> 53.6697%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 44/2000 epochs spend time : 1.8369 sec / total_loss : 0.0774 correct : 29220/29223 -> 99.9897%\n",
            "test dataset : 44/2000 epochs spend time : 0.4948 sec  / total_loss : 3.8333 correct : 4917/9197 -> 53.4631%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 45/2000 epochs spend time : 1.8580 sec / total_loss : 0.0770 correct : 29222/29223 -> 99.9966%\n",
            "test dataset : 45/2000 epochs spend time : 0.4997 sec  / total_loss : 3.8772 correct : 4927/9197 -> 53.5718%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 46/2000 epochs spend time : 1.7981 sec / total_loss : 0.0767 correct : 29223/29223 -> 100.0000%\n",
            "test dataset : 46/2000 epochs spend time : 0.4881 sec  / total_loss : 3.9326 correct : 4925/9197 -> 53.5501%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 47/2000 epochs spend time : 1.8203 sec / total_loss : 0.0764 correct : 29223/29223 -> 100.0000%\n",
            "test dataset : 47/2000 epochs spend time : 0.4796 sec  / total_loss : 3.9765 correct : 4932/9197 -> 53.6262%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 48/2000 epochs spend time : 1.7873 sec / total_loss : 0.0761 correct : 29223/29223 -> 100.0000%\n",
            "test dataset : 48/2000 epochs spend time : 0.4930 sec  / total_loss : 4.0090 correct : 4915/9197 -> 53.4413%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 49/2000 epochs spend time : 1.8022 sec / total_loss : 0.0760 correct : 29223/29223 -> 100.0000%\n",
            "test dataset : 49/2000 epochs spend time : 0.4781 sec  / total_loss : 4.0281 correct : 4919/9197 -> 53.4848%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 50/2000 epochs spend time : 1.7813 sec / total_loss : 0.0759 correct : 29223/29223 -> 100.0000%\n",
            "test dataset : 50/2000 epochs spend time : 0.4808 sec  / total_loss : 4.0567 correct : 4917/9197 -> 53.4631%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 51/2000 epochs spend time : 1.8617 sec / total_loss : 0.0758 correct : 29223/29223 -> 100.0000%\n",
            "test dataset : 51/2000 epochs spend time : 0.4805 sec  / total_loss : 4.0751 correct : 4915/9197 -> 53.4413%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 52/2000 epochs spend time : 1.7972 sec / total_loss : 0.0757 correct : 29223/29223 -> 100.0000%\n",
            "test dataset : 52/2000 epochs spend time : 0.4802 sec  / total_loss : 4.0977 correct : 4919/9197 -> 53.4848%\n",
            "best epoch : 22/2000 / val accuracy : 56.344460%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 53/2000 epochs spend time : 1.8140 sec / total_loss : 0.0756 correct : 29223/29223 -> 100.0000%\n",
            "test dataset : 53/2000 epochs spend time : 0.4855 sec  / total_loss : 4.1157 correct : 4915/9197 -> 53.4413%\n",
            "Early Stopping\n",
            "best epoch : 22/2000 / accuracy : 56.344460%\n",
            "==============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCE73qoRBUC2"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}