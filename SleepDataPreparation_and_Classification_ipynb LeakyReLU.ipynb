{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SleepDataPreparation_and_Classification.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/SleepCapstone/blob/master/SleepDataPreparation_and_Classification_ipynb%20LeakyReLU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAGJRSb3xOT1"
      },
      "source": [
        "한림대학교 소프트웨어 융합대학\n",
        "빅데이터 캡스톤 프로젝트\n",
        "\n",
        "딥러닝에 기반한 수면 분류 모델 개발"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SaQVdPHgfmx",
        "outputId": "e22d6f55-04d4-4618-81b7-ab46e82a1747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohoEZomKw0cz",
        "outputId": "b937c6d1-cb1a-45be-c3ae-440bb1c08a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "cd drive/My\\ Drive"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/My Drive'\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvna-5vRzmTQ",
        "outputId": "87273eec-2db6-4e66-c64e-527d713134cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "# GPU 가용성 체크\n",
        "!nvidia-smi"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Oct 25 07:17:44 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0    32W /  70W |   1017MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF2HFogi4Nzq",
        "outputId": "9abb3eb3-2819-40cf-d4f4-638a78ae89bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip install pyedflib"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyedflib in /usr/local/lib/python3.6/dist-packages (0.1.19)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from pyedflib) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1zNHBNc4beq"
      },
      "source": [
        "import numpy as np\n",
        "from pyedflib import highlevel\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n9vmRl3x-1e"
      },
      "source": [
        "def search_signals_npy(dirname):\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for file in filenames if file.endswith(\".npy\")]\n",
        "    return filenames\n",
        "\n",
        "def search_correct_signals_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vSGY6sO9vgC",
        "outputId": "0daf6b02-e93e-435f-eb64-8a64c49bfa44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "\n",
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  '/content/drive/My Drive/data_2013_with_npy/origin_npy/annotations/remove_wake/'\n",
        "signals_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "total_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list:\n",
        "    label = np.load(path + filename)\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    \n",
        "    signals = np.load(signals_path+signals_filename)\n",
        "    \n",
        "    \n",
        "    #print('remove start index : %d / remove end index : %d'%(remove_start_index,remove_end_index))\n",
        "    #print(np.bincount(label,minlength=6))\n",
        "    if len(label) !=len(signals[0])//30//fs:\n",
        "        print('file is fault!!!')\n",
        "    for i in range(6):\n",
        "        total_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "print(total_label)\n",
        "\n",
        "x = np.arange(len(total_label))\n",
        "\n",
        "plt.bar(x,total_label,width=0.7)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4258  2762 17340  5575  7522    59]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 6 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVVZuGLx-ADW",
        "outputId": "32444376-72c5-4830-c0cc-623a96df177c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  '/content/drive/My Drive/data_2013_with_npy/origin_npy/annotations/remove_wake/'\n",
        "signals_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "random.shuffle(annotations_npy_list)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "trainDataset_count = 30\n",
        "testDataset_count = len(annotations_npy_list)-trainDataset_count\n",
        "\n",
        "print(trainDataset_count)\n",
        "print(testDataset_count)\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4001EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4191EP-Hypnogram.npy']\n",
            "['SC4192EV-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy']\n",
            "30\n",
            "8\n",
            "[12.24051302  7.0606902  45.41848473 14.54779849 20.54740182  0.18511173]\n",
            "[7.64041850e+00 8.61784141e+00 4.95594714e+01 1.61618943e+01\n",
            " 1.79790749e+01 4.12995595e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAKH9Mb0_NF3"
      },
      "source": [
        "signals_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "save_train_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/train/'\n",
        "save_test_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/test/'\n",
        "\n",
        "os.makedirs(save_train_path,exist_ok=True)\n",
        "os.makedirs(save_test_path,exist_ok=True)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_train_path+filename)\n",
        "    \n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_test_path+filename)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4EYtlUH-e9f",
        "outputId": "7de930ed-82a7-4b54-e058-6963882c8a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "def search_correct_annotations_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename\n",
        "\n",
        "train_path =  '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/train/'\n",
        "test_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/annotations/remove_wake/'\n",
        "\n",
        "train_list = search_signals_npy(train_path)\n",
        "test_list = search_signals_npy(test_path)\n",
        "\n",
        "print(train_list)\n",
        "print(test_list)\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in train_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in test_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4122EV-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy']\n",
            "['SC4151EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy']\n",
            "[11.50688668  7.36669849 45.97027138 15.04977499 19.94545207  0.16091641]\n",
            "[13.44931717  7.98215171 45.13453824 13.92707441 19.38522558  0.12169288]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBwqVERu_aw-"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pylab as plt\n",
        "from  torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSQaEXQr_h8m"
      },
      "source": [
        "def data_preprocessing_torch(signals): # 하나의 데이터셋에 대한 data_preprocessing (using torch)\n",
        "    signals = (signals - signals.mean(dim=1).unsqueeze(1))/signals.std(dim=1).unsqueeze(1)\n",
        "\n",
        "    return signals\n",
        "\n",
        "def data_preprocessing_oneToOne_torch(signals,min,max,max_value):\n",
        "    signals_std = (signals + max_value) / (2*max_value)\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def data_preprocessing_minmax_torch(signals,min,max):\n",
        "    signals_std = (signals - signals.min(dim=1).unsqueeze(1)) / (\n",
        "            signals.max(dim=1).unsqueeze(1) - signals.min(dim=1).unsqueeze(1))\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def get_dataset_one_channel_norm_withoutCut(dirname,annotations_dir,data_path,use_noise=True,epsilon=0.5,noise_scale=2e-6,preprocessing=True,norm_methods='Standard'):\n",
        "    # npy read!\n",
        "    path = dirname + data_path\n",
        "    signals = np.load(path)\n",
        "\n",
        "    signals = torch.from_numpy(signals).float().to(device)\n",
        "\n",
        "    if use_noise:\n",
        "        if np.random.rand() < epsilon:\n",
        "            # noise = np.random.normal(loc=0,scale=noise_scale,size=signals.shape)\n",
        "            # signals = signals + noise\n",
        "            noise = torch.normal(mean=0., std=noise_scale, size=signals.shape).to(device)\n",
        "            signals = signals + noise\n",
        "\n",
        "    if preprocessing:\n",
        "        if norm_methods == 'Standard':\n",
        "            signals = data_preprocessing_torch(signals)\n",
        "        elif norm_methods == 'minmax':\n",
        "            signals = data_preprocessing_minmax_torch(signals,0,1)\n",
        "        elif norm_methods == 'oneToOne':\n",
        "            signals = data_preprocessing_oneToOne_torch(signals,-1,1,1e-4)\n",
        "    label = get_annotations(annotations_dir, data_path)\n",
        "    return signals, label\n",
        "\n",
        "# model conv layer weight init function\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:         # Conv weight init\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "        \n",
        "def suffle_dataset_list(dataset_list): # 데이터 셔플\n",
        "    random.shuffle(dataset_list)\n",
        "    return dataset_list\n",
        "\n",
        "\n",
        "# npy파일을 통해 label을 가져오는 함수\n",
        "def get_annotations(label_dir,file_name):\n",
        "    label_path = label_dir + file_name\n",
        "    label = np.load(label_path)\n",
        "    return label\n",
        "\n",
        "def signals_expand_torch_one_channel(signals): # 2차원 데이터를 3차원으로 변환 (8,N) -> (batch,8,6000) 형태로\n",
        "    signals = signals.expand(1,1,-1)\n",
        "    #print(signals.shape)\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    signals = signals.reshape(-1, 3000, 1) # 형태 변환\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    return signals"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx4M7Rfq_lwo"
      },
      "source": [
        "class DeepSleepNet_Classification(nn.Module):  # input channel = 8channel / output = 5\n",
        "    def __init__(self,in_channel=1,out_channel=6,layer=[64,128,128,128],sample_rate = 100):\n",
        "        super(DeepSleepNet_Classification, self).__init__()\n",
        "\n",
        "        \n",
        "        self.fc1 = nn.Linear(3000,1024)\n",
        "        self.fc2 = nn.Linear(1024,1024)\n",
        "        self.fc3 = nn.Linear(1024,512)\n",
        "        self.fc4 = nn.Linear(512, 256)\n",
        "        self.fc5 = nn.Linear(256, out_channel)\n",
        "\n",
        "        self.LeakyReLU = nn.LeakyReLU()\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "        # print(\"feature_extract_2d.shape : \", feature_extract_2d.shape)\n",
        "        # 여기서 문제 발생 weight의 경우에는 [64 , 32 , 100] 이지만 input 이 2차원 [32, 750]이라 문제 발생!\n",
        "        out = torch.flatten(input, 1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.LeakyReLU(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.LeakyReLU(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.LeakyReLU(out)\n",
        "        out = self.fc4(out)\n",
        "        out = self.LeakyReLU(out)\n",
        "        out = self.fc5(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYZ6YSZHArdJ",
        "outputId": "af194f0b-a12e-4dd0-a55d-00c7cef92d63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "model = DeepSleepNet_Classification(in_channel=1,out_channel=6)\n",
        "summary(model.cuda(),(1,3000))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 1024]       3,073,024\n",
            "         LeakyReLU-2                 [-1, 1024]               0\n",
            "            Linear-3                 [-1, 1024]       1,049,600\n",
            "         LeakyReLU-4                 [-1, 1024]               0\n",
            "            Linear-5                  [-1, 512]         524,800\n",
            "         LeakyReLU-6                  [-1, 512]               0\n",
            "            Linear-7                  [-1, 256]         131,328\n",
            "         LeakyReLU-8                  [-1, 256]               0\n",
            "            Linear-9                    [-1, 6]           1,542\n",
            "================================================================\n",
            "Total params: 4,780,294\n",
            "Trainable params: 4,780,294\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.04\n",
            "Params size (MB): 18.24\n",
            "Estimated Total Size (MB): 18.29\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li3ftAC1Az0Q"
      },
      "source": [
        "def search_npy_list(dirname):  # 매개변수 dir에서 모든 npy파일을 찾고 fold에 따른 dataset 나누기\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for _, file in enumerate(filenames) if file.endswith(\".npy\")]\n",
        "    return filenames\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQH_VRRFBGF1"
      },
      "source": [
        "def train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, test_signal_dir,annotations_dir\n",
        "                               ,epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                               layer_filters=[64,128,256,512],first_conv=[200,40,100],optim='Adam',lf='CE',\n",
        "                               epsilon=0.7,noise_scale=2e-6,min_value=-1e-4,max_value=1e-4,preprocessing=True,\n",
        "                               norm_methods='Standard',use_noise=True,loss_type='softmax'):\n",
        "    # Adam optimizer param\n",
        "    b1 = 0.5\n",
        "    b2 = 0.999\n",
        "\n",
        "    beta = 0.001\n",
        "\n",
        "    check_file = open(logging_filename, 'w')  # logging file\n",
        "\n",
        "    print('Preproceesing  : ',preprocessing)\n",
        "    print('min/max value : %f/%f'%(min_value,max_value))\n",
        "    print('noise scale : ',noise_scale)\n",
        "    print('loss function : ',lf)\n",
        "    print('epsilon : ',epsilon)\n",
        "    print('norm methods : ',norm_methods)\n",
        "\n",
        "    print('logging file name : ', logging_filename)\n",
        "    print('save file name : ', save_filename)\n",
        "    print('layer filters : ',layer_filters)\n",
        "    print('fisrt_conv info : ',first_conv)\n",
        "    print('loss type : ',loss_type)\n",
        "    print('training data oversampling noise : ',use_noise)\n",
        "    best_accuracy = 0.\n",
        "    best_epoch = 0\n",
        "\n",
        "    train_dataset_list = search_npy_list(train_signal_dir)\n",
        "    test_dataset_list = search_npy_list(test_signal_dir)\n",
        "\n",
        "    train_dataset_len = len(train_dataset_list)\n",
        "    test_dataset_len = len(test_dataset_list)\n",
        "\n",
        "    print('train_dataset length : ', len(train_dataset_list))\n",
        "    print(train_dataset_list)\n",
        "\n",
        "    print('test_dataset length : ',test_dataset_len)\n",
        "    print(test_dataset_list)\n",
        "\n",
        "\n",
        "    model = DeepSleepNet_Classification()\n",
        "\n",
        "    #model = resnet18_200hz(in_channel=1,layer_filters=layer_filters,first_conv=first_conv,use_batchnorm=True,num_classes=5)\n",
        "\n",
        "    model.apply(weights_init) # weight init\n",
        "\n",
        "    cuda = torch.cuda.is_available()\n",
        "\n",
        "    if cuda:\n",
        "        print('can use CUDA!!!')\n",
        "        model = model.cuda()\n",
        "    #summary(model,[1,6000])\n",
        "    print('torch.cuda.device_count() : ', torch.cuda.device_count())\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print('Multi GPU Activation !!!')\n",
        "        #model = nn.DataParallel(model)\n",
        "\n",
        "    # loss funcition\n",
        "    if lf == 'CE':\n",
        "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "    elif lf == 'CEW':\n",
        "        samples_per_cls = [27,15,41,5,11]\n",
        "        no_of_classes = 5\n",
        "        effective_num = 1.0 - np.power(beta,samples_per_cls)\n",
        "        #print(effective_num)\n",
        "        weights = (1.0 - beta) / np.array(effective_num)\n",
        "        #print(weights)\n",
        "        weights = weights / np.sum(weights) * no_of_classes\n",
        "        weights = torch.tensor(weights).float()\n",
        "        weights = weights.to(device)\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=weights).to(device)\n",
        "    elif lf == 'FL':\n",
        "        loss_fn = FocalLoss(gamma=2).to(device)\n",
        "    elif lf == 'CBL':\n",
        "        loss_fn = CB_loss(samples_per_cls=[27,15,41,5,11],no_of_classes=5,loss_type=loss_type,beta=0.9999,gamma=2.0)\n",
        "    #loss_fn = FocalLoss(gamma=2).to(device)\n",
        "\n",
        "    # optimizer ADAM (SGD의 경우에는 정상적으로 학습이 진행되지 않았음)\n",
        "    if optim == 'Adam':\n",
        "        print('Optimizer : Adam')\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "    elif optim == 'RMS':\n",
        "        print('Optimizer : RMSprop')\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "    elif optim == 'SGD':\n",
        "        print('Optimizer : SGD')\n",
        "        optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=gamma, patience=10,\n",
        "                                                           min_lr=1e-6)\n",
        "    #stride = 40 일 때, batch_size = 20이면 16GB정도의 메모리 사용\n",
        "    batch_size = 5\n",
        "    norm_square = 2\n",
        "\n",
        "    train_batch_size = math.ceil(train_dataset_len / batch_size)\n",
        "    print('train_batch_size : ',train_batch_size)\n",
        "\n",
        "    test_batch_size = test_dataset_len\n",
        "\n",
        "    best_accuracy = 0.\n",
        "    stop_count = 0\n",
        "    for epoch in range(epochs):\n",
        "        train_dataset = suffle_dataset_list(train_dataset_list) # 매 epoch마다 train_dataset shuffle !\n",
        "        count = 0  # check batch\n",
        "        train_total_loss = 0.0\n",
        "        train_total_count = 0\n",
        "        train_total_data = 0\n",
        "\n",
        "        val_total_loss = 0.0\n",
        "        val_total_count = 0\n",
        "        val_total_data = 0\n",
        "\n",
        "        test_total_loss = 0.0\n",
        "        test_total_count = 0\n",
        "        test_total_data = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "\n",
        "        output_str = 'current_lr : %f\\n'%(optimizer.state_dict()['param_groups'][0]['lr'])\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "        for index, file_name in enumerate(train_dataset):\n",
        "            #print('index : ',index)\n",
        "            if index % batch_size == 0:\n",
        "                batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "            else:\n",
        "                new_signal, new_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "                batch_signal = torch.cat((batch_signal, new_signal),dim=1)\n",
        "                batch_label = np.concatenate((batch_label, new_label))\n",
        "            count += 1\n",
        "            if count == batch_size or index == len(train_dataset) - 1:  # batch 학습 시작!\n",
        "                batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                #batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                # batch_signal = torch.from_numpy(batch_signal).float().to(device)\n",
        "                batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "                optimizer.zero_grad()\n",
        "                # print(batch_signal.shape)\n",
        "                # print(batch_signal)\n",
        "                pred = model(batch_signal)\n",
        "                norm = 0\n",
        "\n",
        "                for parameter in model.parameters():\n",
        "                    norm += torch.norm(parameter, p=norm_square)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label) + beta * norm\n",
        "                #print('loss : ',loss.item())\n",
        "                # loss = loss_fn(pred, batch_label)\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                train_total_loss += loss.item()\n",
        "\n",
        "                train_total_count += check_count\n",
        "                train_total_data += len(batch_signal)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "                count = 0\n",
        "\n",
        "        train_total_loss /= train_batch_size\n",
        "        train_accuracy = train_total_count / train_total_data * 100\n",
        "\n",
        "        output_str = 'train dataset : %d/%d epochs spend time : %.4f sec / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, train_total_loss,\n",
        "                        train_total_count, train_total_data, train_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "    \n",
        "\n",
        "        #check test dataset\n",
        "        start_time = time.time()\n",
        "        for file_name in test_dataset_list:\n",
        "            batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(test_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=False,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "            batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "\n",
        "            batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred = model(batch_signal)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label)\n",
        "\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                test_total_loss += loss.item()\n",
        "                test_total_count += check_count\n",
        "                test_total_data += len(batch_signal)\n",
        "\n",
        "                # 사용하지 않는 변수 제거\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "        test_total_loss /= test_batch_size\n",
        "        test_accuracy = test_total_count / test_total_data * 100\n",
        "\n",
        "\n",
        "\n",
        "        output_str = 'test dataset : %d/%d epochs spend time : %.4f sec  / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, test_total_loss,\n",
        "                        test_total_count, test_total_data, test_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "        scheduler.step(float(test_total_loss))\n",
        "        #scheduler.step()\n",
        "\n",
        "        if epoch == 0:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_epoch = epoch\n",
        "            save_file = save_filename\n",
        "            #save_file = save_path + 'best_SleepEEGNet_CNN_channel%d.pth'%channel\n",
        "            torch.save(model.state_dict(),save_file)\n",
        "            stop_count = 0\n",
        "        else:\n",
        "            if best_accuracy < test_accuracy:\n",
        "                best_accuracy = test_accuracy\n",
        "                best_epoch = epoch\n",
        "                save_file = save_filename\n",
        "                torch.save(model.state_dict(), save_file)\n",
        "                stop_count = 0\n",
        "            else:\n",
        "                stop_count += 1\n",
        "        if stop_count > 30:\n",
        "            print('Early Stopping')\n",
        "            break\n",
        "\n",
        "        output_str = 'best epoch : %d/%d / val accuracy : %f%%\\n' \\\n",
        "                     % (best_epoch+1, epochs, best_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        print('=' * 30)\n",
        "\n",
        "\n",
        "    output_str = 'best epoch : %d/%d / accuracy : %f%%\\n' \\\n",
        "                 % (best_epoch+1, epochs, best_accuracy)\n",
        "    sys.stdout.write(output_str)\n",
        "    check_file.write(output_str)\n",
        "    print('=' * 30)\n",
        "\n",
        "    check_file.close()"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-qN1b_NBMC1",
        "outputId": "1f9556b6-bb05-4491-ad98-85df395e67ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "save_filename = './train.pth'\n",
        "logging_filename = './logging.txt'\n",
        "train_signal_dir = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/train/'\n",
        "test_signal_dir = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_dir = '/content/drive/My Drive/data_2013_with_npy/origin_npy/annotations/remove_wake/'\n",
        "train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, \n",
        "                           test_signal_dir,annotations_dir,\n",
        "                           epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                           layer_filters=[64,128,256,512],first_conv=[200,40,100],\n",
        "                           optim='Adam',lf='CE',epsilon=0,noise_scale=0,\n",
        "                           min_value=-0,max_value=0,preprocessing=True,\n",
        "                           norm_methods='Standard',use_noise=False,loss_type='softmax')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preproceesing  :  True\n",
            "min/max value : 0.000000/0.000000\n",
            "noise scale :  0\n",
            "loss function :  CE\n",
            "epsilon :  0\n",
            "norm methods :  Standard\n",
            "logging file name :  ./logging.txt\n",
            "save file name :  ./train.pth\n",
            "layer filters :  [64, 128, 256, 512]\n",
            "fisrt_conv info :  [200, 40, 100]\n",
            "loss type :  softmax\n",
            "training data oversampling noise :  False\n",
            "train_dataset length :  37\n",
            "['SC4122EV-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy']\n",
            "test_dataset length :  22\n",
            "['SC4151EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy']\n",
            "can use CUDA!!!\n",
            "torch.cuda.device_count() :  1\n",
            "Optimizer : Adam\n",
            "train_batch_size :  8\n",
            "current_lr : 0.001000\n",
            "train dataset : 1/2000 epochs spend time : 1.6743 sec / total_loss : 1.6116 correct : 17007/36665 -> 46.3848%\n",
            "test dataset : 1/2000 epochs spend time : 0.9139 sec  / total_loss : 1.4920 correct : 10014/22187 -> 45.1345%\n",
            "best epoch : 1/2000 / val accuracy : 45.134538%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 2/2000 epochs spend time : 1.7621 sec / total_loss : 1.4640 correct : 16627/36665 -> 45.3484%\n",
            "test dataset : 2/2000 epochs spend time : 0.9094 sec  / total_loss : 1.5196 correct : 4603/22187 -> 20.7464%\n",
            "best epoch : 1/2000 / val accuracy : 45.134538%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 3/2000 epochs spend time : 1.6683 sec / total_loss : 1.4081 correct : 16010/36665 -> 43.6656%\n",
            "test dataset : 3/2000 epochs spend time : 0.8946 sec  / total_loss : 1.3968 correct : 10014/22187 -> 45.1345%\n",
            "best epoch : 1/2000 / val accuracy : 45.134538%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 4/2000 epochs spend time : 1.6351 sec / total_loss : 1.3865 correct : 16325/36665 -> 44.5248%\n",
            "test dataset : 4/2000 epochs spend time : 0.9185 sec  / total_loss : 1.2185 correct : 10110/22187 -> 45.5672%\n",
            "best epoch : 4/2000 / val accuracy : 45.567224%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 5/2000 epochs spend time : 1.6860 sec / total_loss : 1.4819 correct : 16425/36665 -> 44.7975%\n",
            "test dataset : 5/2000 epochs spend time : 0.8915 sec  / total_loss : 1.2508 correct : 10244/22187 -> 46.1712%\n",
            "best epoch : 5/2000 / val accuracy : 46.171181%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 6/2000 epochs spend time : 1.7099 sec / total_loss : 1.2528 correct : 18845/36665 -> 51.3978%\n",
            "test dataset : 6/2000 epochs spend time : 0.8955 sec  / total_loss : 1.1473 correct : 11452/22187 -> 51.6158%\n",
            "best epoch : 6/2000 / val accuracy : 51.615811%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 7/2000 epochs spend time : 1.6500 sec / total_loss : 1.2318 correct : 18743/36665 -> 51.1196%\n",
            "test dataset : 7/2000 epochs spend time : 0.8907 sec  / total_loss : 1.2411 correct : 10653/22187 -> 48.0146%\n",
            "best epoch : 6/2000 / val accuracy : 51.615811%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 8/2000 epochs spend time : 1.6575 sec / total_loss : 1.1960 correct : 19614/36665 -> 53.4952%\n",
            "test dataset : 8/2000 epochs spend time : 0.9011 sec  / total_loss : 1.0884 correct : 12263/22187 -> 55.2711%\n",
            "best epoch : 8/2000 / val accuracy : 55.271105%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 9/2000 epochs spend time : 1.6967 sec / total_loss : 1.0739 correct : 21634/36665 -> 59.0045%\n",
            "test dataset : 9/2000 epochs spend time : 0.8776 sec  / total_loss : 1.0426 correct : 12979/22187 -> 58.4982%\n",
            "best epoch : 9/2000 / val accuracy : 58.498220%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 10/2000 epochs spend time : 1.7273 sec / total_loss : 1.0113 correct : 22509/36665 -> 61.3910%\n",
            "test dataset : 10/2000 epochs spend time : 0.9681 sec  / total_loss : 0.9083 correct : 13625/22187 -> 61.4098%\n",
            "best epoch : 10/2000 / val accuracy : 61.409835%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 11/2000 epochs spend time : 1.7007 sec / total_loss : 0.9782 correct : 23201/36665 -> 63.2783%\n",
            "test dataset : 11/2000 epochs spend time : 0.8852 sec  / total_loss : 0.8866 correct : 13406/22187 -> 60.4228%\n",
            "best epoch : 10/2000 / val accuracy : 61.409835%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 12/2000 epochs spend time : 1.6859 sec / total_loss : 0.8936 correct : 24450/36665 -> 66.6848%\n",
            "test dataset : 12/2000 epochs spend time : 0.8832 sec  / total_loss : 1.0845 correct : 11542/22187 -> 52.0215%\n",
            "best epoch : 10/2000 / val accuracy : 61.409835%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 13/2000 epochs spend time : 1.6665 sec / total_loss : 0.9392 correct : 22850/36665 -> 62.3210%\n",
            "test dataset : 13/2000 epochs spend time : 0.9015 sec  / total_loss : 0.7292 correct : 14885/22187 -> 67.0888%\n",
            "best epoch : 13/2000 / val accuracy : 67.088836%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 14/2000 epochs spend time : 1.7065 sec / total_loss : 0.7494 correct : 26652/36665 -> 72.6906%\n",
            "test dataset : 14/2000 epochs spend time : 0.8927 sec  / total_loss : 1.2868 correct : 12005/22187 -> 54.1083%\n",
            "best epoch : 13/2000 / val accuracy : 67.088836%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 15/2000 epochs spend time : 1.6566 sec / total_loss : 0.9612 correct : 24030/36665 -> 65.5393%\n",
            "test dataset : 15/2000 epochs spend time : 0.8803 sec  / total_loss : 0.6009 correct : 16365/22187 -> 73.7594%\n",
            "best epoch : 15/2000 / val accuracy : 73.759409%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 16/2000 epochs spend time : 1.6975 sec / total_loss : 0.8304 correct : 25645/36665 -> 69.9441%\n",
            "test dataset : 16/2000 epochs spend time : 0.8816 sec  / total_loss : 0.5625 correct : 17248/22187 -> 77.7392%\n",
            "best epoch : 16/2000 / val accuracy : 77.739217%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 17/2000 epochs spend time : 1.6857 sec / total_loss : 0.6799 correct : 28023/36665 -> 76.4298%\n",
            "test dataset : 17/2000 epochs spend time : 0.8995 sec  / total_loss : 0.6023 correct : 16944/22187 -> 76.3690%\n",
            "best epoch : 16/2000 / val accuracy : 77.739217%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 18/2000 epochs spend time : 1.7043 sec / total_loss : 0.6907 correct : 28487/36665 -> 77.6953%\n",
            "test dataset : 18/2000 epochs spend time : 0.8820 sec  / total_loss : 0.5579 correct : 16781/22187 -> 75.6344%\n",
            "best epoch : 16/2000 / val accuracy : 77.739217%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 19/2000 epochs spend time : 1.6641 sec / total_loss : 0.6524 correct : 28338/36665 -> 77.2890%\n",
            "test dataset : 19/2000 epochs spend time : 0.8982 sec  / total_loss : 0.5690 correct : 17450/22187 -> 78.6497%\n",
            "best epoch : 19/2000 / val accuracy : 78.649660%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 20/2000 epochs spend time : 1.7006 sec / total_loss : 0.5125 correct : 30543/36665 -> 83.3029%\n",
            "test dataset : 20/2000 epochs spend time : 0.8748 sec  / total_loss : 0.3750 correct : 19093/22187 -> 86.0549%\n",
            "best epoch : 20/2000 / val accuracy : 86.054897%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 21/2000 epochs spend time : 1.6687 sec / total_loss : 0.4135 correct : 31856/36665 -> 86.8839%\n",
            "test dataset : 21/2000 epochs spend time : 0.8987 sec  / total_loss : 0.3354 correct : 19778/22187 -> 89.1423%\n",
            "best epoch : 21/2000 / val accuracy : 89.142291%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 22/2000 epochs spend time : 1.7164 sec / total_loss : 0.4433 correct : 31642/36665 -> 86.3003%\n",
            "test dataset : 22/2000 epochs spend time : 0.8817 sec  / total_loss : 0.3904 correct : 19036/22187 -> 85.7980%\n",
            "best epoch : 21/2000 / val accuracy : 89.142291%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 23/2000 epochs spend time : 1.6612 sec / total_loss : 0.6289 correct : 30321/36665 -> 82.6974%\n",
            "test dataset : 23/2000 epochs spend time : 0.9136 sec  / total_loss : 1.8992 correct : 8994/22187 -> 40.5373%\n",
            "best epoch : 21/2000 / val accuracy : 89.142291%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 24/2000 epochs spend time : 1.7101 sec / total_loss : 1.2412 correct : 21881/36665 -> 59.6782%\n",
            "test dataset : 24/2000 epochs spend time : 0.9022 sec  / total_loss : 0.6043 correct : 16709/22187 -> 75.3099%\n",
            "best epoch : 21/2000 / val accuracy : 89.142291%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 25/2000 epochs spend time : 1.6816 sec / total_loss : 1.6116 correct : 20900/36665 -> 57.0026%\n",
            "test dataset : 25/2000 epochs spend time : 0.9119 sec  / total_loss : 0.8467 correct : 15155/22187 -> 68.3058%\n",
            "best epoch : 21/2000 / val accuracy : 89.142291%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 26/2000 epochs spend time : 1.6525 sec / total_loss : 1.0812 correct : 26280/36665 -> 71.6760%\n",
            "test dataset : 26/2000 epochs spend time : 0.8991 sec  / total_loss : 1.0545 correct : 15420/22187 -> 69.5002%\n",
            "best epoch : 21/2000 / val accuracy : 89.142291%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 27/2000 epochs spend time : 1.6507 sec / total_loss : 0.7413 correct : 27483/36665 -> 74.9570%\n",
            "test dataset : 27/2000 epochs spend time : 0.9032 sec  / total_loss : 0.5433 correct : 17808/22187 -> 80.2632%\n",
            "best epoch : 21/2000 / val accuracy : 89.142291%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 28/2000 epochs spend time : 1.6643 sec / total_loss : 0.4795 correct : 31244/36665 -> 85.2148%\n",
            "test dataset : 28/2000 epochs spend time : 0.8847 sec  / total_loss : 0.5325 correct : 18447/22187 -> 83.1433%\n",
            "best epoch : 21/2000 / val accuracy : 89.142291%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 29/2000 epochs spend time : 1.6803 sec / total_loss : 0.3982 correct : 32361/36665 -> 88.2613%\n",
            "test dataset : 29/2000 epochs spend time : 0.9184 sec  / total_loss : 0.3634 correct : 19748/22187 -> 89.0071%\n",
            "best epoch : 21/2000 / val accuracy : 89.142291%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 30/2000 epochs spend time : 1.6688 sec / total_loss : 0.3532 correct : 33321/36665 -> 90.8796%\n",
            "test dataset : 30/2000 epochs spend time : 0.9008 sec  / total_loss : 0.3889 correct : 19009/22187 -> 85.6763%\n",
            "best epoch : 21/2000 / val accuracy : 89.142291%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 31/2000 epochs spend time : 1.6697 sec / total_loss : 0.3051 correct : 34054/36665 -> 92.8788%\n",
            "test dataset : 31/2000 epochs spend time : 0.9164 sec  / total_loss : 0.8801 correct : 16955/22187 -> 76.4186%\n",
            "best epoch : 21/2000 / val accuracy : 89.142291%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 32/2000 epochs spend time : 1.6611 sec / total_loss : 0.6690 correct : 29237/36665 -> 79.7409%\n",
            "test dataset : 32/2000 epochs spend time : 0.8852 sec  / total_loss : 0.3344 correct : 19981/22187 -> 90.0572%\n",
            "best epoch : 32/2000 / val accuracy : 90.057241%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 33/2000 epochs spend time : 1.7327 sec / total_loss : 0.2922 correct : 34026/36665 -> 92.8024%\n",
            "test dataset : 33/2000 epochs spend time : 0.9890 sec  / total_loss : 0.3395 correct : 19711/22187 -> 88.8403%\n",
            "best epoch : 32/2000 / val accuracy : 90.057241%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 34/2000 epochs spend time : 1.7047 sec / total_loss : 0.2863 correct : 33853/36665 -> 92.3306%\n",
            "test dataset : 34/2000 epochs spend time : 0.9275 sec  / total_loss : 0.2762 correct : 20370/22187 -> 91.8105%\n",
            "best epoch : 34/2000 / val accuracy : 91.810520%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 35/2000 epochs spend time : 1.6717 sec / total_loss : 0.2850 correct : 33912/36665 -> 92.4915%\n",
            "test dataset : 35/2000 epochs spend time : 0.9037 sec  / total_loss : 0.2367 correct : 20847/22187 -> 93.9604%\n",
            "best epoch : 35/2000 / val accuracy : 93.960427%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 36/2000 epochs spend time : 1.6831 sec / total_loss : 0.2219 correct : 35137/36665 -> 95.8325%\n",
            "test dataset : 36/2000 epochs spend time : 0.8918 sec  / total_loss : 0.1894 correct : 21129/22187 -> 95.2314%\n",
            "best epoch : 36/2000 / val accuracy : 95.231442%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 37/2000 epochs spend time : 1.7579 sec / total_loss : 0.2478 correct : 35273/36665 -> 96.2035%\n",
            "test dataset : 37/2000 epochs spend time : 0.9119 sec  / total_loss : 0.1831 correct : 21194/22187 -> 95.5244%\n",
            "best epoch : 37/2000 / val accuracy : 95.524406%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 38/2000 epochs spend time : 1.6766 sec / total_loss : 0.2234 correct : 35414/36665 -> 96.5880%\n",
            "test dataset : 38/2000 epochs spend time : 0.9036 sec  / total_loss : 0.2036 correct : 21002/22187 -> 94.6590%\n",
            "best epoch : 37/2000 / val accuracy : 95.524406%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 39/2000 epochs spend time : 1.6687 sec / total_loss : 0.1649 correct : 35823/36665 -> 97.7035%\n",
            "test dataset : 39/2000 epochs spend time : 0.8614 sec  / total_loss : 0.1704 correct : 21404/22187 -> 96.4709%\n",
            "best epoch : 39/2000 / val accuracy : 96.470906%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 40/2000 epochs spend time : 1.6739 sec / total_loss : 0.1468 correct : 35923/36665 -> 97.9763%\n",
            "test dataset : 40/2000 epochs spend time : 0.9133 sec  / total_loss : 0.1950 correct : 21267/22187 -> 95.8534%\n",
            "best epoch : 39/2000 / val accuracy : 96.470906%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 41/2000 epochs spend time : 1.6839 sec / total_loss : 0.2705 correct : 34541/36665 -> 94.2070%\n",
            "test dataset : 41/2000 epochs spend time : 0.8746 sec  / total_loss : 0.4612 correct : 18903/22187 -> 85.1985%\n",
            "best epoch : 39/2000 / val accuracy : 96.470906%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 42/2000 epochs spend time : 1.6779 sec / total_loss : 0.2452 correct : 34626/36665 -> 94.4388%\n",
            "test dataset : 42/2000 epochs spend time : 0.8958 sec  / total_loss : 0.1731 correct : 21192/22187 -> 95.5154%\n",
            "best epoch : 39/2000 / val accuracy : 96.470906%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 43/2000 epochs spend time : 1.6749 sec / total_loss : 0.1450 correct : 35978/36665 -> 98.1263%\n",
            "test dataset : 43/2000 epochs spend time : 0.8685 sec  / total_loss : 0.1558 correct : 21518/22187 -> 96.9847%\n",
            "best epoch : 43/2000 / val accuracy : 96.984721%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 44/2000 epochs spend time : 1.6731 sec / total_loss : 0.1294 correct : 36166/36665 -> 98.6390%\n",
            "test dataset : 44/2000 epochs spend time : 0.8946 sec  / total_loss : 0.1488 correct : 21606/22187 -> 97.3813%\n",
            "best epoch : 44/2000 / val accuracy : 97.381349%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 45/2000 epochs spend time : 1.7277 sec / total_loss : 0.1110 correct : 36409/36665 -> 99.3018%\n",
            "test dataset : 45/2000 epochs spend time : 0.9080 sec  / total_loss : 0.1551 correct : 21625/22187 -> 97.4670%\n",
            "best epoch : 45/2000 / val accuracy : 97.466985%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 46/2000 epochs spend time : 1.6737 sec / total_loss : 0.1056 correct : 36487/36665 -> 99.5145%\n",
            "test dataset : 46/2000 epochs spend time : 0.8822 sec  / total_loss : 0.1553 correct : 21676/22187 -> 97.6968%\n",
            "best epoch : 46/2000 / val accuracy : 97.696850%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 47/2000 epochs spend time : 1.6801 sec / total_loss : 0.1328 correct : 36089/36665 -> 98.4290%\n",
            "test dataset : 47/2000 epochs spend time : 0.8928 sec  / total_loss : 0.1760 correct : 21449/22187 -> 96.6737%\n",
            "best epoch : 46/2000 / val accuracy : 97.696850%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 48/2000 epochs spend time : 1.6837 sec / total_loss : 0.1180 correct : 36325/36665 -> 99.0727%\n",
            "test dataset : 48/2000 epochs spend time : 0.9026 sec  / total_loss : 0.1643 correct : 21591/22187 -> 97.3137%\n",
            "best epoch : 46/2000 / val accuracy : 97.696850%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 49/2000 epochs spend time : 1.7016 sec / total_loss : 0.9759 correct : 33130/36665 -> 90.3587%\n",
            "test dataset : 49/2000 epochs spend time : 0.9364 sec  / total_loss : 1.2406 correct : 12696/22187 -> 57.2227%\n",
            "best epoch : 46/2000 / val accuracy : 97.696850%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 50/2000 epochs spend time : 1.6464 sec / total_loss : 1.3435 correct : 18815/36665 -> 51.3160%\n",
            "test dataset : 50/2000 epochs spend time : 0.8885 sec  / total_loss : 1.1647 correct : 10555/22187 -> 47.5729%\n",
            "best epoch : 46/2000 / val accuracy : 97.696850%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 51/2000 epochs spend time : 1.6939 sec / total_loss : 1.0806 correct : 21821/36665 -> 59.5145%\n",
            "test dataset : 51/2000 epochs spend time : 0.8915 sec  / total_loss : 0.9518 correct : 12949/22187 -> 58.3630%\n",
            "best epoch : 46/2000 / val accuracy : 97.696850%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 52/2000 epochs spend time : 1.7297 sec / total_loss : 0.7564 correct : 26665/36665 -> 72.7260%\n",
            "test dataset : 52/2000 epochs spend time : 0.9239 sec  / total_loss : 0.7840 correct : 15490/22187 -> 69.8157%\n",
            "best epoch : 46/2000 / val accuracy : 97.696850%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 53/2000 epochs spend time : 1.7138 sec / total_loss : 0.7703 correct : 27602/36665 -> 75.2816%\n",
            "test dataset : 53/2000 epochs spend time : 0.9619 sec  / total_loss : 0.5718 correct : 17490/22187 -> 78.8299%\n",
            "best epoch : 46/2000 / val accuracy : 97.696850%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 54/2000 epochs spend time : 1.7283 sec / total_loss : 0.5674 correct : 31005/36665 -> 84.5629%\n",
            "test dataset : 54/2000 epochs spend time : 0.8995 sec  / total_loss : 0.3408 correct : 20171/22187 -> 90.9136%\n",
            "best epoch : 46/2000 / val accuracy : 97.696850%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 55/2000 epochs spend time : 1.7027 sec / total_loss : 0.3349 correct : 33821/36665 -> 92.2433%\n",
            "test dataset : 55/2000 epochs spend time : 0.9267 sec  / total_loss : 0.3170 correct : 20269/22187 -> 91.3553%\n",
            "best epoch : 46/2000 / val accuracy : 97.696850%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 56/2000 epochs spend time : 1.7559 sec / total_loss : 0.2143 correct : 35410/36665 -> 96.5771%\n",
            "test dataset : 56/2000 epochs spend time : 0.9848 sec  / total_loss : 0.1446 correct : 21543/22187 -> 97.0974%\n",
            "best epoch : 46/2000 / val accuracy : 97.696850%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 57/2000 epochs spend time : 1.6894 sec / total_loss : 0.1375 correct : 36306/36665 -> 99.0209%\n",
            "test dataset : 57/2000 epochs spend time : 0.8905 sec  / total_loss : 0.1304 correct : 21602/22187 -> 97.3633%\n",
            "best epoch : 46/2000 / val accuracy : 97.696850%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 58/2000 epochs spend time : 1.6708 sec / total_loss : 0.1220 correct : 36451/36665 -> 99.4163%\n",
            "test dataset : 58/2000 epochs spend time : 0.8842 sec  / total_loss : 0.1324 correct : 21634/22187 -> 97.5075%\n",
            "best epoch : 46/2000 / val accuracy : 97.696850%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 59/2000 epochs spend time : 1.6428 sec / total_loss : 0.1125 correct : 36532/36665 -> 99.6373%\n",
            "test dataset : 59/2000 epochs spend time : 0.8843 sec  / total_loss : 0.1314 correct : 21698/22187 -> 97.7960%\n",
            "best epoch : 59/2000 / val accuracy : 97.796007%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 60/2000 epochs spend time : 1.6781 sec / total_loss : 0.1039 correct : 36612/36665 -> 99.8554%\n",
            "test dataset : 60/2000 epochs spend time : 0.9210 sec  / total_loss : 0.1330 correct : 21718/22187 -> 97.8861%\n",
            "best epoch : 60/2000 / val accuracy : 97.886150%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 61/2000 epochs spend time : 1.6939 sec / total_loss : 0.1009 correct : 36635/36665 -> 99.9182%\n",
            "test dataset : 61/2000 epochs spend time : 0.9090 sec  / total_loss : 0.1361 correct : 21727/22187 -> 97.9267%\n",
            "best epoch : 61/2000 / val accuracy : 97.926714%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 62/2000 epochs spend time : 1.6706 sec / total_loss : 0.0985 correct : 36646/36665 -> 99.9482%\n",
            "test dataset : 62/2000 epochs spend time : 0.9177 sec  / total_loss : 0.1394 correct : 21727/22187 -> 97.9267%\n",
            "best epoch : 61/2000 / val accuracy : 97.926714%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 63/2000 epochs spend time : 1.6599 sec / total_loss : 0.0974 correct : 36661/36665 -> 99.9891%\n",
            "test dataset : 63/2000 epochs spend time : 0.8980 sec  / total_loss : 0.1422 correct : 21731/22187 -> 97.9447%\n",
            "best epoch : 63/2000 / val accuracy : 97.944742%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 64/2000 epochs spend time : 1.7249 sec / total_loss : 0.0962 correct : 36662/36665 -> 99.9918%\n",
            "test dataset : 64/2000 epochs spend time : 0.9326 sec  / total_loss : 0.1460 correct : 21732/22187 -> 97.9492%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 65/2000 epochs spend time : 1.7303 sec / total_loss : 0.0956 correct : 36663/36665 -> 99.9945%\n",
            "test dataset : 65/2000 epochs spend time : 0.8812 sec  / total_loss : 0.1484 correct : 21729/22187 -> 97.9357%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 66/2000 epochs spend time : 1.6844 sec / total_loss : 0.0951 correct : 36664/36665 -> 99.9973%\n",
            "test dataset : 66/2000 epochs spend time : 0.8966 sec  / total_loss : 0.1506 correct : 21728/22187 -> 97.9312%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 67/2000 epochs spend time : 1.7265 sec / total_loss : 0.0948 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 67/2000 epochs spend time : 0.9093 sec  / total_loss : 0.1531 correct : 21724/22187 -> 97.9132%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 68/2000 epochs spend time : 1.6586 sec / total_loss : 0.0945 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 68/2000 epochs spend time : 0.8926 sec  / total_loss : 0.1552 correct : 21726/22187 -> 97.9222%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 69/2000 epochs spend time : 1.6661 sec / total_loss : 0.0944 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 69/2000 epochs spend time : 0.8926 sec  / total_loss : 0.1567 correct : 21727/22187 -> 97.9267%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 70/2000 epochs spend time : 1.6794 sec / total_loss : 0.0942 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 70/2000 epochs spend time : 0.8577 sec  / total_loss : 0.1577 correct : 21726/22187 -> 97.9222%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 71/2000 epochs spend time : 1.6625 sec / total_loss : 0.0942 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 71/2000 epochs spend time : 0.8841 sec  / total_loss : 0.1585 correct : 21726/22187 -> 97.9222%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 72/2000 epochs spend time : 1.7129 sec / total_loss : 0.0940 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 72/2000 epochs spend time : 0.8936 sec  / total_loss : 0.1597 correct : 21723/22187 -> 97.9087%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 73/2000 epochs spend time : 1.6838 sec / total_loss : 0.0939 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 73/2000 epochs spend time : 0.9015 sec  / total_loss : 0.1608 correct : 21721/22187 -> 97.8997%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 74/2000 epochs spend time : 1.6956 sec / total_loss : 0.0939 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 74/2000 epochs spend time : 0.8860 sec  / total_loss : 0.1618 correct : 21721/22187 -> 97.8997%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 75/2000 epochs spend time : 1.6552 sec / total_loss : 0.0938 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 75/2000 epochs spend time : 0.8919 sec  / total_loss : 0.1630 correct : 21720/22187 -> 97.8952%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 76/2000 epochs spend time : 1.6754 sec / total_loss : 0.0937 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 76/2000 epochs spend time : 0.8951 sec  / total_loss : 0.1641 correct : 21718/22187 -> 97.8861%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 77/2000 epochs spend time : 1.6737 sec / total_loss : 0.0936 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 77/2000 epochs spend time : 0.8897 sec  / total_loss : 0.1648 correct : 21718/22187 -> 97.8861%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 78/2000 epochs spend time : 1.6945 sec / total_loss : 0.0935 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 78/2000 epochs spend time : 0.8791 sec  / total_loss : 0.1656 correct : 21718/22187 -> 97.8861%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 79/2000 epochs spend time : 1.6668 sec / total_loss : 0.0934 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 79/2000 epochs spend time : 0.9705 sec  / total_loss : 0.1662 correct : 21719/22187 -> 97.8907%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 80/2000 epochs spend time : 1.7109 sec / total_loss : 0.0934 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 80/2000 epochs spend time : 0.8890 sec  / total_loss : 0.1665 correct : 21719/22187 -> 97.8907%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 81/2000 epochs spend time : 1.6826 sec / total_loss : 0.0933 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 81/2000 epochs spend time : 0.8899 sec  / total_loss : 0.1666 correct : 21721/22187 -> 97.8997%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 82/2000 epochs spend time : 1.7045 sec / total_loss : 0.0933 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 82/2000 epochs spend time : 0.8887 sec  / total_loss : 0.1669 correct : 21720/22187 -> 97.8952%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 83/2000 epochs spend time : 1.6771 sec / total_loss : 0.0933 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 83/2000 epochs spend time : 0.8948 sec  / total_loss : 0.1673 correct : 21719/22187 -> 97.8907%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 84/2000 epochs spend time : 1.7073 sec / total_loss : 0.0932 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 84/2000 epochs spend time : 0.8937 sec  / total_loss : 0.1678 correct : 21719/22187 -> 97.8907%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 85/2000 epochs spend time : 1.6862 sec / total_loss : 0.0933 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 85/2000 epochs spend time : 0.9166 sec  / total_loss : 0.1683 correct : 21719/22187 -> 97.8907%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 86/2000 epochs spend time : 1.7004 sec / total_loss : 0.0932 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 86/2000 epochs spend time : 0.8938 sec  / total_loss : 0.1685 correct : 21719/22187 -> 97.8907%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 87/2000 epochs spend time : 1.6797 sec / total_loss : 0.0932 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 87/2000 epochs spend time : 0.8937 sec  / total_loss : 0.1689 correct : 21721/22187 -> 97.8997%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 88/2000 epochs spend time : 1.6923 sec / total_loss : 0.0931 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 88/2000 epochs spend time : 0.8930 sec  / total_loss : 0.1691 correct : 21720/22187 -> 97.8952%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 89/2000 epochs spend time : 1.6722 sec / total_loss : 0.0931 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 89/2000 epochs spend time : 0.8958 sec  / total_loss : 0.1696 correct : 21720/22187 -> 97.8952%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 90/2000 epochs spend time : 1.6797 sec / total_loss : 0.0930 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 90/2000 epochs spend time : 0.8735 sec  / total_loss : 0.1700 correct : 21720/22187 -> 97.8952%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 91/2000 epochs spend time : 1.6846 sec / total_loss : 0.0930 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 91/2000 epochs spend time : 0.8727 sec  / total_loss : 0.1702 correct : 21720/22187 -> 97.8952%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 92/2000 epochs spend time : 1.6708 sec / total_loss : 0.0930 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 92/2000 epochs spend time : 0.8824 sec  / total_loss : 0.1704 correct : 21720/22187 -> 97.8952%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 93/2000 epochs spend time : 1.6711 sec / total_loss : 0.0930 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 93/2000 epochs spend time : 0.8886 sec  / total_loss : 0.1706 correct : 21720/22187 -> 97.8952%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 94/2000 epochs spend time : 1.6888 sec / total_loss : 0.0930 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 94/2000 epochs spend time : 0.8828 sec  / total_loss : 0.1710 correct : 21720/22187 -> 97.8952%\n",
            "best epoch : 64/2000 / val accuracy : 97.949250%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 95/2000 epochs spend time : 1.6500 sec / total_loss : 0.0929 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 95/2000 epochs spend time : 0.9188 sec  / total_loss : 0.1711 correct : 21719/22187 -> 97.8907%\n",
            "Early Stopping\n",
            "best epoch : 64/2000 / accuracy : 97.949250%\n",
            "==============================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}