{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SleepDataPreparation_and_Classification.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/SleepCapstone/blob/master/SleepDataPreparation_and_Classification_ipynb%20ELU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAGJRSb3xOT1"
      },
      "source": [
        "한림대학교 소프트웨어 융합대학\n",
        "빅데이터 캡스톤 프로젝트\n",
        "\n",
        "딥러닝에 기반한 수면 분류 모델 개발"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SaQVdPHgfmx",
        "outputId": "e22d6f55-04d4-4618-81b7-ab46e82a1747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohoEZomKw0cz",
        "outputId": "b937c6d1-cb1a-45be-c3ae-440bb1c08a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "cd drive/My\\ Drive"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/My Drive'\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvna-5vRzmTQ",
        "outputId": "87273eec-2db6-4e66-c64e-527d713134cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "# GPU 가용성 체크\n",
        "!nvidia-smi"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Oct 25 07:17:44 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0    32W /  70W |   1017MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF2HFogi4Nzq",
        "outputId": "9abb3eb3-2819-40cf-d4f4-638a78ae89bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip install pyedflib"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyedflib in /usr/local/lib/python3.6/dist-packages (0.1.19)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from pyedflib) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1zNHBNc4beq"
      },
      "source": [
        "import numpy as np\n",
        "from pyedflib import highlevel\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n9vmRl3x-1e"
      },
      "source": [
        "def search_signals_npy(dirname):\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for file in filenames if file.endswith(\".npy\")]\n",
        "    return filenames\n",
        "\n",
        "def search_correct_signals_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vSGY6sO9vgC",
        "outputId": "0daf6b02-e93e-435f-eb64-8a64c49bfa44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "\n",
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  '/content/drive/My Drive/data_2013_with_npy/origin_npy/annotations/remove_wake/'\n",
        "signals_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "total_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list:\n",
        "    label = np.load(path + filename)\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    \n",
        "    signals = np.load(signals_path+signals_filename)\n",
        "    \n",
        "    \n",
        "    #print('remove start index : %d / remove end index : %d'%(remove_start_index,remove_end_index))\n",
        "    #print(np.bincount(label,minlength=6))\n",
        "    if len(label) !=len(signals[0])//30//fs:\n",
        "        print('file is fault!!!')\n",
        "    for i in range(6):\n",
        "        total_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "print(total_label)\n",
        "\n",
        "x = np.arange(len(total_label))\n",
        "\n",
        "plt.bar(x,total_label,width=0.7)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4258  2762 17340  5575  7522    59]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 6 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVVZuGLx-ADW",
        "outputId": "32444376-72c5-4830-c0cc-623a96df177c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  '/content/drive/My Drive/data_2013_with_npy/origin_npy/annotations/remove_wake/'\n",
        "signals_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "random.shuffle(annotations_npy_list)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "trainDataset_count = 30\n",
        "testDataset_count = len(annotations_npy_list)-trainDataset_count\n",
        "\n",
        "print(trainDataset_count)\n",
        "print(testDataset_count)\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4001EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4191EP-Hypnogram.npy']\n",
            "['SC4192EV-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy']\n",
            "30\n",
            "8\n",
            "[12.24051302  7.0606902  45.41848473 14.54779849 20.54740182  0.18511173]\n",
            "[7.64041850e+00 8.61784141e+00 4.95594714e+01 1.61618943e+01\n",
            " 1.79790749e+01 4.12995595e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAKH9Mb0_NF3"
      },
      "source": [
        "signals_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "save_train_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/train/'\n",
        "save_test_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/test/'\n",
        "\n",
        "os.makedirs(save_train_path,exist_ok=True)\n",
        "os.makedirs(save_test_path,exist_ok=True)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_train_path+filename)\n",
        "    \n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_test_path+filename)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4EYtlUH-e9f",
        "outputId": "7de930ed-82a7-4b54-e058-6963882c8a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "def search_correct_annotations_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename\n",
        "\n",
        "train_path =  '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/train/'\n",
        "test_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/annotations/remove_wake/'\n",
        "\n",
        "train_list = search_signals_npy(train_path)\n",
        "test_list = search_signals_npy(test_path)\n",
        "\n",
        "print(train_list)\n",
        "print(test_list)\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in train_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in test_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4122EV-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy']\n",
            "['SC4151EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy']\n",
            "[11.50688668  7.36669849 45.97027138 15.04977499 19.94545207  0.16091641]\n",
            "[13.44931717  7.98215171 45.13453824 13.92707441 19.38522558  0.12169288]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBwqVERu_aw-"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pylab as plt\n",
        "from  torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSQaEXQr_h8m"
      },
      "source": [
        "def data_preprocessing_torch(signals): # 하나의 데이터셋에 대한 data_preprocessing (using torch)\n",
        "    signals = (signals - signals.mean(dim=1).unsqueeze(1))/signals.std(dim=1).unsqueeze(1)\n",
        "\n",
        "    return signals\n",
        "\n",
        "def data_preprocessing_oneToOne_torch(signals,min,max,max_value):\n",
        "    signals_std = (signals + max_value) / (2*max_value)\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def data_preprocessing_minmax_torch(signals,min,max):\n",
        "    signals_std = (signals - signals.min(dim=1).unsqueeze(1)) / (\n",
        "            signals.max(dim=1).unsqueeze(1) - signals.min(dim=1).unsqueeze(1))\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def get_dataset_one_channel_norm_withoutCut(dirname,annotations_dir,data_path,use_noise=True,epsilon=0.5,noise_scale=2e-6,preprocessing=True,norm_methods='Standard'):\n",
        "    # npy read!\n",
        "    path = dirname + data_path\n",
        "    signals = np.load(path)\n",
        "\n",
        "    signals = torch.from_numpy(signals).float().to(device)\n",
        "\n",
        "    if use_noise:\n",
        "        if np.random.rand() < epsilon:\n",
        "            # noise = np.random.normal(loc=0,scale=noise_scale,size=signals.shape)\n",
        "            # signals = signals + noise\n",
        "            noise = torch.normal(mean=0., std=noise_scale, size=signals.shape).to(device)\n",
        "            signals = signals + noise\n",
        "\n",
        "    if preprocessing:\n",
        "        if norm_methods == 'Standard':\n",
        "            signals = data_preprocessing_torch(signals)\n",
        "        elif norm_methods == 'minmax':\n",
        "            signals = data_preprocessing_minmax_torch(signals,0,1)\n",
        "        elif norm_methods == 'oneToOne':\n",
        "            signals = data_preprocessing_oneToOne_torch(signals,-1,1,1e-4)\n",
        "    label = get_annotations(annotations_dir, data_path)\n",
        "    return signals, label\n",
        "\n",
        "# model conv layer weight init function\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:         # Conv weight init\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "        \n",
        "def suffle_dataset_list(dataset_list): # 데이터 셔플\n",
        "    random.shuffle(dataset_list)\n",
        "    return dataset_list\n",
        "\n",
        "\n",
        "# npy파일을 통해 label을 가져오는 함수\n",
        "def get_annotations(label_dir,file_name):\n",
        "    label_path = label_dir + file_name\n",
        "    label = np.load(label_path)\n",
        "    return label\n",
        "\n",
        "def signals_expand_torch_one_channel(signals): # 2차원 데이터를 3차원으로 변환 (8,N) -> (batch,8,6000) 형태로\n",
        "    signals = signals.expand(1,1,-1)\n",
        "    #print(signals.shape)\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    signals = signals.reshape(-1, 3000, 1) # 형태 변환\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    return signals"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx4M7Rfq_lwo"
      },
      "source": [
        "class DeepSleepNet_Classification(nn.Module):  # input channel = 8channel / output = 5\n",
        "    def __init__(self,in_channel=1,out_channel=6,layer=[64,128,128,128],sample_rate = 100):\n",
        "        super(DeepSleepNet_Classification, self).__init__()\n",
        "\n",
        "        \n",
        "        self.fc1 = nn.Linear(3000,1024)\n",
        "        self.fc2 = nn.Linear(1024,1024)\n",
        "        self.fc3 = nn.Linear(1024,512)\n",
        "        self.fc4 = nn.Linear(512, 256)\n",
        "        self.fc5 = nn.Linear(256, out_channel)\n",
        "\n",
        "        self.ELU = nn.ELU()\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "        # print(\"feature_extract_2d.shape : \", feature_extract_2d.shape)\n",
        "        # 여기서 문제 발생 weight의 경우에는 [64 , 32 , 100] 이지만 input 이 2차원 [32, 750]이라 문제 발생!\n",
        "        out = torch.flatten(input, 1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.ELU(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.ELU(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.ELU(out)\n",
        "        out = self.fc4(out)\n",
        "        out = self.ELU(out)\n",
        "        out = self.fc5(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYZ6YSZHArdJ",
        "outputId": "005badea-2d6f-427c-e0f1-66d1748e7f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "model = DeepSleepNet_Classification(in_channel=1,out_channel=6)\n",
        "summary(model.cuda(),(1,3000))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 1024]       3,073,024\n",
            "               ELU-2                 [-1, 1024]               0\n",
            "            Linear-3                 [-1, 1024]       1,049,600\n",
            "               ELU-4                 [-1, 1024]               0\n",
            "            Linear-5                  [-1, 512]         524,800\n",
            "               ELU-6                  [-1, 512]               0\n",
            "            Linear-7                  [-1, 256]         131,328\n",
            "               ELU-8                  [-1, 256]               0\n",
            "            Linear-9                    [-1, 6]           1,542\n",
            "================================================================\n",
            "Total params: 4,780,294\n",
            "Trainable params: 4,780,294\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.04\n",
            "Params size (MB): 18.24\n",
            "Estimated Total Size (MB): 18.29\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li3ftAC1Az0Q"
      },
      "source": [
        "def search_npy_list(dirname):  # 매개변수 dir에서 모든 npy파일을 찾고 fold에 따른 dataset 나누기\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for _, file in enumerate(filenames) if file.endswith(\".npy\")]\n",
        "    return filenames\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQH_VRRFBGF1"
      },
      "source": [
        "def train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, test_signal_dir,annotations_dir\n",
        "                               ,epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                               layer_filters=[64,128,256,512],first_conv=[200,40,100],optim='Adam',lf='CE',\n",
        "                               epsilon=0.7,noise_scale=2e-6,min_value=-1e-4,max_value=1e-4,preprocessing=True,\n",
        "                               norm_methods='Standard',use_noise=True,loss_type='softmax'):\n",
        "    # Adam optimizer param\n",
        "    b1 = 0.5\n",
        "    b2 = 0.999\n",
        "\n",
        "    beta = 0.001\n",
        "\n",
        "    check_file = open(logging_filename, 'w')  # logging file\n",
        "\n",
        "    print('Preproceesing  : ',preprocessing)\n",
        "    print('min/max value : %f/%f'%(min_value,max_value))\n",
        "    print('noise scale : ',noise_scale)\n",
        "    print('loss function : ',lf)\n",
        "    print('epsilon : ',epsilon)\n",
        "    print('norm methods : ',norm_methods)\n",
        "\n",
        "    print('logging file name : ', logging_filename)\n",
        "    print('save file name : ', save_filename)\n",
        "    print('layer filters : ',layer_filters)\n",
        "    print('fisrt_conv info : ',first_conv)\n",
        "    print('loss type : ',loss_type)\n",
        "    print('training data oversampling noise : ',use_noise)\n",
        "    best_accuracy = 0.\n",
        "    best_epoch = 0\n",
        "\n",
        "    train_dataset_list = search_npy_list(train_signal_dir)\n",
        "    test_dataset_list = search_npy_list(test_signal_dir)\n",
        "\n",
        "    train_dataset_len = len(train_dataset_list)\n",
        "    test_dataset_len = len(test_dataset_list)\n",
        "\n",
        "    print('train_dataset length : ', len(train_dataset_list))\n",
        "    print(train_dataset_list)\n",
        "\n",
        "    print('test_dataset length : ',test_dataset_len)\n",
        "    print(test_dataset_list)\n",
        "\n",
        "\n",
        "    model = DeepSleepNet_Classification()\n",
        "\n",
        "    #model = resnet18_200hz(in_channel=1,layer_filters=layer_filters,first_conv=first_conv,use_batchnorm=True,num_classes=5)\n",
        "\n",
        "    model.apply(weights_init) # weight init\n",
        "\n",
        "    cuda = torch.cuda.is_available()\n",
        "\n",
        "    if cuda:\n",
        "        print('can use CUDA!!!')\n",
        "        model = model.cuda()\n",
        "    #summary(model,[1,6000])\n",
        "    print('torch.cuda.device_count() : ', torch.cuda.device_count())\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print('Multi GPU Activation !!!')\n",
        "        #model = nn.DataParallel(model)\n",
        "\n",
        "    # loss funcition\n",
        "    if lf == 'CE':\n",
        "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "    elif lf == 'CEW':\n",
        "        samples_per_cls = [27,15,41,5,11]\n",
        "        no_of_classes = 5\n",
        "        effective_num = 1.0 - np.power(beta,samples_per_cls)\n",
        "        #print(effective_num)\n",
        "        weights = (1.0 - beta) / np.array(effective_num)\n",
        "        #print(weights)\n",
        "        weights = weights / np.sum(weights) * no_of_classes\n",
        "        weights = torch.tensor(weights).float()\n",
        "        weights = weights.to(device)\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=weights).to(device)\n",
        "    elif lf == 'FL':\n",
        "        loss_fn = FocalLoss(gamma=2).to(device)\n",
        "    elif lf == 'CBL':\n",
        "        loss_fn = CB_loss(samples_per_cls=[27,15,41,5,11],no_of_classes=5,loss_type=loss_type,beta=0.9999,gamma=2.0)\n",
        "    #loss_fn = FocalLoss(gamma=2).to(device)\n",
        "\n",
        "    # optimizer ADAM (SGD의 경우에는 정상적으로 학습이 진행되지 않았음)\n",
        "    if optim == 'Adam':\n",
        "        print('Optimizer : Adam')\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "    elif optim == 'RMS':\n",
        "        print('Optimizer : RMSprop')\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "    elif optim == 'SGD':\n",
        "        print('Optimizer : SGD')\n",
        "        optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=gamma, patience=10,\n",
        "                                                           min_lr=1e-6)\n",
        "    #stride = 40 일 때, batch_size = 20이면 16GB정도의 메모리 사용\n",
        "    batch_size = 5\n",
        "    norm_square = 2\n",
        "\n",
        "    train_batch_size = math.ceil(train_dataset_len / batch_size)\n",
        "    print('train_batch_size : ',train_batch_size)\n",
        "\n",
        "    test_batch_size = test_dataset_len\n",
        "\n",
        "    best_accuracy = 0.\n",
        "    stop_count = 0\n",
        "    for epoch in range(epochs):\n",
        "        train_dataset = suffle_dataset_list(train_dataset_list) # 매 epoch마다 train_dataset shuffle !\n",
        "        count = 0  # check batch\n",
        "        train_total_loss = 0.0\n",
        "        train_total_count = 0\n",
        "        train_total_data = 0\n",
        "\n",
        "        val_total_loss = 0.0\n",
        "        val_total_count = 0\n",
        "        val_total_data = 0\n",
        "\n",
        "        test_total_loss = 0.0\n",
        "        test_total_count = 0\n",
        "        test_total_data = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "\n",
        "        output_str = 'current_lr : %f\\n'%(optimizer.state_dict()['param_groups'][0]['lr'])\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "        for index, file_name in enumerate(train_dataset):\n",
        "            #print('index : ',index)\n",
        "            if index % batch_size == 0:\n",
        "                batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "            else:\n",
        "                new_signal, new_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "                batch_signal = torch.cat((batch_signal, new_signal),dim=1)\n",
        "                batch_label = np.concatenate((batch_label, new_label))\n",
        "            count += 1\n",
        "            if count == batch_size or index == len(train_dataset) - 1:  # batch 학습 시작!\n",
        "                batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                #batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                # batch_signal = torch.from_numpy(batch_signal).float().to(device)\n",
        "                batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "                optimizer.zero_grad()\n",
        "                # print(batch_signal.shape)\n",
        "                # print(batch_signal)\n",
        "                pred = model(batch_signal)\n",
        "                norm = 0\n",
        "\n",
        "                for parameter in model.parameters():\n",
        "                    norm += torch.norm(parameter, p=norm_square)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label) + beta * norm\n",
        "                #print('loss : ',loss.item())\n",
        "                # loss = loss_fn(pred, batch_label)\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                train_total_loss += loss.item()\n",
        "\n",
        "                train_total_count += check_count\n",
        "                train_total_data += len(batch_signal)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "                count = 0\n",
        "\n",
        "        train_total_loss /= train_batch_size\n",
        "        train_accuracy = train_total_count / train_total_data * 100\n",
        "\n",
        "        output_str = 'train dataset : %d/%d epochs spend time : %.4f sec / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, train_total_loss,\n",
        "                        train_total_count, train_total_data, train_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "    \n",
        "\n",
        "        #check test dataset\n",
        "        start_time = time.time()\n",
        "        for file_name in test_dataset_list:\n",
        "            batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(test_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=False,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "            batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "\n",
        "            batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred = model(batch_signal)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label)\n",
        "\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                test_total_loss += loss.item()\n",
        "                test_total_count += check_count\n",
        "                test_total_data += len(batch_signal)\n",
        "\n",
        "                # 사용하지 않는 변수 제거\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "        test_total_loss /= test_batch_size\n",
        "        test_accuracy = test_total_count / test_total_data * 100\n",
        "\n",
        "\n",
        "\n",
        "        output_str = 'test dataset : %d/%d epochs spend time : %.4f sec  / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, test_total_loss,\n",
        "                        test_total_count, test_total_data, test_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "        scheduler.step(float(test_total_loss))\n",
        "        #scheduler.step()\n",
        "\n",
        "        if epoch == 0:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_epoch = epoch\n",
        "            save_file = save_filename\n",
        "            #save_file = save_path + 'best_SleepEEGNet_CNN_channel%d.pth'%channel\n",
        "            torch.save(model.state_dict(),save_file)\n",
        "            stop_count = 0\n",
        "        else:\n",
        "            if best_accuracy < test_accuracy:\n",
        "                best_accuracy = test_accuracy\n",
        "                best_epoch = epoch\n",
        "                save_file = save_filename\n",
        "                torch.save(model.state_dict(), save_file)\n",
        "                stop_count = 0\n",
        "            else:\n",
        "                stop_count += 1\n",
        "        if stop_count > 30:\n",
        "            print('Early Stopping')\n",
        "            break\n",
        "\n",
        "        output_str = 'best epoch : %d/%d / val accuracy : %f%%\\n' \\\n",
        "                     % (best_epoch+1, epochs, best_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        print('=' * 30)\n",
        "\n",
        "\n",
        "    output_str = 'best epoch : %d/%d / accuracy : %f%%\\n' \\\n",
        "                 % (best_epoch+1, epochs, best_accuracy)\n",
        "    sys.stdout.write(output_str)\n",
        "    check_file.write(output_str)\n",
        "    print('=' * 30)\n",
        "\n",
        "    check_file.close()"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-qN1b_NBMC1",
        "outputId": "1f15d4b6-09ed-4a82-c109-95af1e192ae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "save_filename = './train.pth'\n",
        "logging_filename = './logging.txt'\n",
        "train_signal_dir = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/train/'\n",
        "test_signal_dir = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_dir = '/content/drive/My Drive/data_2013_with_npy/origin_npy/annotations/remove_wake/'\n",
        "train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, \n",
        "                           test_signal_dir,annotations_dir,\n",
        "                           epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                           layer_filters=[64,128,256,512],first_conv=[200,40,100],\n",
        "                           optim='Adam',lf='CE',epsilon=0,noise_scale=0,\n",
        "                           min_value=-0,max_value=0,preprocessing=True,\n",
        "                           norm_methods='Standard',use_noise=False,loss_type='softmax')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preproceesing  :  True\n",
            "min/max value : 0.000000/0.000000\n",
            "noise scale :  0\n",
            "loss function :  CE\n",
            "epsilon :  0\n",
            "norm methods :  Standard\n",
            "logging file name :  ./logging.txt\n",
            "save file name :  ./train.pth\n",
            "layer filters :  [64, 128, 256, 512]\n",
            "fisrt_conv info :  [200, 40, 100]\n",
            "loss type :  softmax\n",
            "training data oversampling noise :  False\n",
            "train_dataset length :  37\n",
            "['SC4122EV-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy']\n",
            "test_dataset length :  22\n",
            "['SC4151EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy']\n",
            "can use CUDA!!!\n",
            "torch.cuda.device_count() :  1\n",
            "Optimizer : Adam\n",
            "train_batch_size :  8\n",
            "current_lr : 0.001000\n",
            "train dataset : 1/2000 epochs spend time : 1.6724 sec / total_loss : 1.6574 correct : 13211/36665 -> 36.0316%\n",
            "test dataset : 1/2000 epochs spend time : 0.8800 sec  / total_loss : 1.3454 correct : 10199/22187 -> 45.9684%\n",
            "best epoch : 1/2000 / val accuracy : 45.968360%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 2/2000 epochs spend time : 1.6870 sec / total_loss : 1.5718 correct : 15479/36665 -> 42.2174%\n",
            "test dataset : 2/2000 epochs spend time : 0.9112 sec  / total_loss : 1.3641 correct : 10259/22187 -> 46.2388%\n",
            "best epoch : 2/2000 / val accuracy : 46.238788%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 3/2000 epochs spend time : 1.6828 sec / total_loss : 1.3194 correct : 18550/36665 -> 50.5932%\n",
            "test dataset : 3/2000 epochs spend time : 0.8797 sec  / total_loss : 1.1772 correct : 11464/22187 -> 51.6699%\n",
            "best epoch : 3/2000 / val accuracy : 51.669897%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 4/2000 epochs spend time : 1.6769 sec / total_loss : 1.2221 correct : 20457/36665 -> 55.7944%\n",
            "test dataset : 4/2000 epochs spend time : 0.9234 sec  / total_loss : 1.1312 correct : 11618/22187 -> 52.3640%\n",
            "best epoch : 4/2000 / val accuracy : 52.363997%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 5/2000 epochs spend time : 1.6617 sec / total_loss : 1.2174 correct : 19926/36665 -> 54.3461%\n",
            "test dataset : 5/2000 epochs spend time : 0.8814 sec  / total_loss : 1.0924 correct : 12022/22187 -> 54.1849%\n",
            "best epoch : 5/2000 / val accuracy : 54.184883%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 6/2000 epochs spend time : 1.6927 sec / total_loss : 1.1070 correct : 21600/36665 -> 58.9118%\n",
            "test dataset : 6/2000 epochs spend time : 0.9058 sec  / total_loss : 1.0072 correct : 12866/22187 -> 57.9889%\n",
            "best epoch : 6/2000 / val accuracy : 57.988912%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 7/2000 epochs spend time : 1.6444 sec / total_loss : 1.1133 correct : 21003/36665 -> 57.2835%\n",
            "test dataset : 7/2000 epochs spend time : 0.8919 sec  / total_loss : 1.1318 correct : 12101/22187 -> 54.5409%\n",
            "best epoch : 6/2000 / val accuracy : 57.988912%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 8/2000 epochs spend time : 1.6243 sec / total_loss : 1.0544 correct : 22671/36665 -> 61.8328%\n",
            "test dataset : 8/2000 epochs spend time : 0.9063 sec  / total_loss : 1.0715 correct : 11987/22187 -> 54.0271%\n",
            "best epoch : 6/2000 / val accuracy : 57.988912%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 9/2000 epochs spend time : 1.6505 sec / total_loss : 1.0937 correct : 21490/36665 -> 58.6118%\n",
            "test dataset : 9/2000 epochs spend time : 0.8801 sec  / total_loss : 1.1426 correct : 11621/22187 -> 52.3775%\n",
            "best epoch : 6/2000 / val accuracy : 57.988912%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 10/2000 epochs spend time : 1.6375 sec / total_loss : 1.0797 correct : 21729/36665 -> 59.2636%\n",
            "test dataset : 10/2000 epochs spend time : 0.9211 sec  / total_loss : 0.8700 correct : 14182/22187 -> 63.9203%\n",
            "best epoch : 10/2000 / val accuracy : 63.920314%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 11/2000 epochs spend time : 1.6511 sec / total_loss : 0.9228 correct : 24036/36665 -> 65.5557%\n",
            "test dataset : 11/2000 epochs spend time : 0.8654 sec  / total_loss : 0.9085 correct : 13849/22187 -> 62.4194%\n",
            "best epoch : 10/2000 / val accuracy : 63.920314%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 12/2000 epochs spend time : 1.6577 sec / total_loss : 0.9231 correct : 24155/36665 -> 65.8803%\n",
            "test dataset : 12/2000 epochs spend time : 0.9521 sec  / total_loss : 0.8007 correct : 14976/22187 -> 67.4990%\n",
            "best epoch : 12/2000 / val accuracy : 67.498986%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 13/2000 epochs spend time : 1.6697 sec / total_loss : 0.9009 correct : 24537/36665 -> 66.9221%\n",
            "test dataset : 13/2000 epochs spend time : 0.8972 sec  / total_loss : 0.8299 correct : 14858/22187 -> 66.9671%\n",
            "best epoch : 12/2000 / val accuracy : 67.498986%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 14/2000 epochs spend time : 1.6683 sec / total_loss : 0.9142 correct : 24738/36665 -> 67.4703%\n",
            "test dataset : 14/2000 epochs spend time : 0.9320 sec  / total_loss : 0.8535 correct : 14693/22187 -> 66.2235%\n",
            "best epoch : 12/2000 / val accuracy : 67.498986%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 15/2000 epochs spend time : 1.6529 sec / total_loss : 0.8947 correct : 24877/36665 -> 67.8494%\n",
            "test dataset : 15/2000 epochs spend time : 0.8921 sec  / total_loss : 0.8402 correct : 14965/22187 -> 67.4494%\n",
            "best epoch : 12/2000 / val accuracy : 67.498986%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 16/2000 epochs spend time : 1.6661 sec / total_loss : 0.8445 correct : 25584/36665 -> 69.7777%\n",
            "test dataset : 16/2000 epochs spend time : 0.9111 sec  / total_loss : 0.6713 correct : 16216/22187 -> 73.0878%\n",
            "best epoch : 16/2000 / val accuracy : 73.087844%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 17/2000 epochs spend time : 1.7861 sec / total_loss : 0.7755 correct : 26919/36665 -> 73.4188%\n",
            "test dataset : 17/2000 epochs spend time : 0.9320 sec  / total_loss : 0.9332 correct : 13697/22187 -> 61.7343%\n",
            "best epoch : 16/2000 / val accuracy : 73.087844%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 18/2000 epochs spend time : 1.6912 sec / total_loss : 0.8160 correct : 25964/36665 -> 70.8141%\n",
            "test dataset : 18/2000 epochs spend time : 0.8957 sec  / total_loss : 0.8102 correct : 14855/22187 -> 66.9536%\n",
            "best epoch : 16/2000 / val accuracy : 73.087844%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 19/2000 epochs spend time : 1.6815 sec / total_loss : 0.7584 correct : 26984/36665 -> 73.5961%\n",
            "test dataset : 19/2000 epochs spend time : 0.9312 sec  / total_loss : 0.9905 correct : 13068/22187 -> 58.8994%\n",
            "best epoch : 16/2000 / val accuracy : 73.087844%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 20/2000 epochs spend time : 1.6762 sec / total_loss : 0.7587 correct : 26682/36665 -> 72.7724%\n",
            "test dataset : 20/2000 epochs spend time : 0.8704 sec  / total_loss : 0.7291 correct : 15619/22187 -> 70.3971%\n",
            "best epoch : 16/2000 / val accuracy : 73.087844%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 21/2000 epochs spend time : 1.6511 sec / total_loss : 0.7249 correct : 27410/36665 -> 74.7579%\n",
            "test dataset : 21/2000 epochs spend time : 0.8851 sec  / total_loss : 0.9510 correct : 13700/22187 -> 61.7479%\n",
            "best epoch : 16/2000 / val accuracy : 73.087844%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 22/2000 epochs spend time : 1.6547 sec / total_loss : 0.8033 correct : 26635/36665 -> 72.6442%\n",
            "test dataset : 22/2000 epochs spend time : 0.8700 sec  / total_loss : 0.7365 correct : 15207/22187 -> 68.5401%\n",
            "best epoch : 16/2000 / val accuracy : 73.087844%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 23/2000 epochs spend time : 1.6426 sec / total_loss : 0.7312 correct : 27173/36665 -> 74.1116%\n",
            "test dataset : 23/2000 epochs spend time : 0.8683 sec  / total_loss : 0.4718 correct : 18276/22187 -> 82.3726%\n",
            "best epoch : 23/2000 / val accuracy : 82.372561%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 24/2000 epochs spend time : 1.6501 sec / total_loss : 0.6875 correct : 28406/36665 -> 77.4744%\n",
            "test dataset : 24/2000 epochs spend time : 0.8683 sec  / total_loss : 0.8033 correct : 15166/22187 -> 68.3553%\n",
            "best epoch : 23/2000 / val accuracy : 82.372561%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 25/2000 epochs spend time : 1.6270 sec / total_loss : 0.6181 correct : 28827/36665 -> 78.6227%\n",
            "test dataset : 25/2000 epochs spend time : 0.9084 sec  / total_loss : 0.5287 correct : 17347/22187 -> 78.1854%\n",
            "best epoch : 23/2000 / val accuracy : 82.372561%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 26/2000 epochs spend time : 1.6835 sec / total_loss : 0.5913 correct : 29357/36665 -> 80.0682%\n",
            "test dataset : 26/2000 epochs spend time : 0.8730 sec  / total_loss : 0.5195 correct : 17745/22187 -> 79.9793%\n",
            "best epoch : 23/2000 / val accuracy : 82.372561%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 27/2000 epochs spend time : 1.6306 sec / total_loss : 0.4867 correct : 30920/36665 -> 84.3311%\n",
            "test dataset : 27/2000 epochs spend time : 0.8840 sec  / total_loss : 0.3677 correct : 18957/22187 -> 85.4419%\n",
            "best epoch : 27/2000 / val accuracy : 85.441925%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 28/2000 epochs spend time : 1.6912 sec / total_loss : 0.4737 correct : 31167/36665 -> 85.0048%\n",
            "test dataset : 28/2000 epochs spend time : 0.8833 sec  / total_loss : 0.3646 correct : 19280/22187 -> 86.8977%\n",
            "best epoch : 28/2000 / val accuracy : 86.897733%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 29/2000 epochs spend time : 1.6745 sec / total_loss : 0.5434 correct : 30624/36665 -> 83.5238%\n",
            "test dataset : 29/2000 epochs spend time : 0.8832 sec  / total_loss : 0.4899 correct : 18113/22187 -> 81.6379%\n",
            "best epoch : 28/2000 / val accuracy : 86.897733%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 30/2000 epochs spend time : 1.6595 sec / total_loss : 0.4916 correct : 31054/36665 -> 84.6966%\n",
            "test dataset : 30/2000 epochs spend time : 0.8848 sec  / total_loss : 0.3445 correct : 19268/22187 -> 86.8436%\n",
            "best epoch : 28/2000 / val accuracy : 86.897733%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 31/2000 epochs spend time : 1.6540 sec / total_loss : 0.3984 correct : 32534/36665 -> 88.7331%\n",
            "test dataset : 31/2000 epochs spend time : 0.8736 sec  / total_loss : 0.5158 correct : 18175/22187 -> 81.9173%\n",
            "best epoch : 28/2000 / val accuracy : 86.897733%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 32/2000 epochs spend time : 1.6650 sec / total_loss : 0.4866 correct : 32035/36665 -> 87.3722%\n",
            "test dataset : 32/2000 epochs spend time : 0.8856 sec  / total_loss : 0.4088 correct : 18798/22187 -> 84.7253%\n",
            "best epoch : 28/2000 / val accuracy : 86.897733%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 33/2000 epochs spend time : 1.6439 sec / total_loss : 0.4246 correct : 32741/36665 -> 89.2977%\n",
            "test dataset : 33/2000 epochs spend time : 0.8710 sec  / total_loss : 0.2190 correct : 20867/22187 -> 94.0506%\n",
            "best epoch : 33/2000 / val accuracy : 94.050570%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 34/2000 epochs spend time : 1.6788 sec / total_loss : 0.2370 correct : 35048/36665 -> 95.5898%\n",
            "test dataset : 34/2000 epochs spend time : 0.8810 sec  / total_loss : 0.3317 correct : 19782/22187 -> 89.1603%\n",
            "best epoch : 33/2000 / val accuracy : 94.050570%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 35/2000 epochs spend time : 1.6433 sec / total_loss : 0.3071 correct : 33942/36665 -> 92.5733%\n",
            "test dataset : 35/2000 epochs spend time : 0.9247 sec  / total_loss : 0.2840 correct : 20117/22187 -> 90.6702%\n",
            "best epoch : 33/2000 / val accuracy : 94.050570%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 36/2000 epochs spend time : 1.6445 sec / total_loss : 0.2673 correct : 34233/36665 -> 93.3670%\n",
            "test dataset : 36/2000 epochs spend time : 0.8875 sec  / total_loss : 0.2505 correct : 20403/22187 -> 91.9593%\n",
            "best epoch : 33/2000 / val accuracy : 94.050570%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 37/2000 epochs spend time : 1.6583 sec / total_loss : 0.1901 correct : 35507/36665 -> 96.8417%\n",
            "test dataset : 37/2000 epochs spend time : 0.8902 sec  / total_loss : 0.2220 correct : 20742/22187 -> 93.4872%\n",
            "best epoch : 33/2000 / val accuracy : 94.050570%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 38/2000 epochs spend time : 1.6689 sec / total_loss : 0.2064 correct : 35070/36665 -> 95.6498%\n",
            "test dataset : 38/2000 epochs spend time : 0.8797 sec  / total_loss : 0.1695 correct : 21304/22187 -> 96.0202%\n",
            "best epoch : 38/2000 / val accuracy : 96.020192%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 39/2000 epochs spend time : 1.6568 sec / total_loss : 0.1338 correct : 36251/36665 -> 98.8709%\n",
            "test dataset : 39/2000 epochs spend time : 0.8823 sec  / total_loss : 0.1605 correct : 21349/22187 -> 96.2230%\n",
            "best epoch : 39/2000 / val accuracy : 96.223013%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 40/2000 epochs spend time : 1.7129 sec / total_loss : 0.1176 correct : 36403/36665 -> 99.2854%\n",
            "test dataset : 40/2000 epochs spend time : 0.9544 sec  / total_loss : 0.1017 correct : 21802/22187 -> 98.2647%\n",
            "best epoch : 40/2000 / val accuracy : 98.264750%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 41/2000 epochs spend time : 1.6912 sec / total_loss : 0.0986 correct : 36615/36665 -> 99.8636%\n",
            "test dataset : 41/2000 epochs spend time : 0.8850 sec  / total_loss : 0.1022 correct : 21829/22187 -> 98.3864%\n",
            "best epoch : 41/2000 / val accuracy : 98.386443%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 42/2000 epochs spend time : 1.7111 sec / total_loss : 0.0931 correct : 36658/36665 -> 99.9809%\n",
            "test dataset : 42/2000 epochs spend time : 0.8863 sec  / total_loss : 0.1017 correct : 21835/22187 -> 98.4135%\n",
            "best epoch : 42/2000 / val accuracy : 98.413485%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 43/2000 epochs spend time : 1.6482 sec / total_loss : 0.0910 correct : 36658/36665 -> 99.9809%\n",
            "test dataset : 43/2000 epochs spend time : 0.8938 sec  / total_loss : 0.1014 correct : 21844/22187 -> 98.4540%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 44/2000 epochs spend time : 1.6778 sec / total_loss : 0.0907 correct : 36646/36665 -> 99.9482%\n",
            "test dataset : 44/2000 epochs spend time : 0.8865 sec  / total_loss : 0.1058 correct : 21833/22187 -> 98.4045%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 45/2000 epochs spend time : 1.6325 sec / total_loss : 0.0972 correct : 36599/36665 -> 99.8200%\n",
            "test dataset : 45/2000 epochs spend time : 0.8805 sec  / total_loss : 0.1133 correct : 21797/22187 -> 98.2422%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 46/2000 epochs spend time : 1.6942 sec / total_loss : 0.1584 correct : 36239/36665 -> 98.8381%\n",
            "test dataset : 46/2000 epochs spend time : 0.8887 sec  / total_loss : 0.1213 correct : 21760/22187 -> 98.0754%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 47/2000 epochs spend time : 1.6648 sec / total_loss : 0.1287 correct : 36331/36665 -> 99.0890%\n",
            "test dataset : 47/2000 epochs spend time : 0.8687 sec  / total_loss : 0.1247 correct : 21765/22187 -> 98.0980%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 48/2000 epochs spend time : 1.6555 sec / total_loss : 0.1125 correct : 36556/36665 -> 99.7027%\n",
            "test dataset : 48/2000 epochs spend time : 0.8861 sec  / total_loss : 0.1773 correct : 21356/22187 -> 96.2546%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 49/2000 epochs spend time : 1.6371 sec / total_loss : 5.4001 correct : 17708/36665 -> 48.2967%\n",
            "test dataset : 49/2000 epochs spend time : 0.8525 sec  / total_loss : 2.2443 correct : 6864/22187 -> 30.9370%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 50/2000 epochs spend time : 1.6087 sec / total_loss : 1.5965 correct : 16874/36665 -> 46.0221%\n",
            "test dataset : 50/2000 epochs spend time : 0.8796 sec  / total_loss : 1.1287 correct : 11967/22187 -> 53.9370%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 51/2000 epochs spend time : 1.6387 sec / total_loss : 1.2605 correct : 20572/36665 -> 56.1080%\n",
            "test dataset : 51/2000 epochs spend time : 0.8744 sec  / total_loss : 1.2928 correct : 11230/22187 -> 50.6152%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 52/2000 epochs spend time : 1.6545 sec / total_loss : 1.0649 correct : 23296/36665 -> 63.5374%\n",
            "test dataset : 52/2000 epochs spend time : 0.8745 sec  / total_loss : 0.8541 correct : 14465/22187 -> 65.1958%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 53/2000 epochs spend time : 1.6328 sec / total_loss : 0.9610 correct : 24109/36665 -> 65.7548%\n",
            "test dataset : 53/2000 epochs spend time : 0.8745 sec  / total_loss : 0.7603 correct : 15566/22187 -> 70.1582%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 54/2000 epochs spend time : 1.6710 sec / total_loss : 0.7380 correct : 27820/36665 -> 75.8762%\n",
            "test dataset : 54/2000 epochs spend time : 0.8745 sec  / total_loss : 0.7668 correct : 15785/22187 -> 71.1453%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 55/2000 epochs spend time : 1.6375 sec / total_loss : 0.6566 correct : 29118/36665 -> 79.4163%\n",
            "test dataset : 55/2000 epochs spend time : 0.8945 sec  / total_loss : 0.4264 correct : 18979/22187 -> 85.5411%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 56/2000 epochs spend time : 1.6601 sec / total_loss : 0.4798 correct : 31471/36665 -> 85.8339%\n",
            "test dataset : 56/2000 epochs spend time : 0.8938 sec  / total_loss : 0.3968 correct : 19002/22187 -> 85.6447%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 57/2000 epochs spend time : 1.6406 sec / total_loss : 0.4312 correct : 32168/36665 -> 87.7349%\n",
            "test dataset : 57/2000 epochs spend time : 0.8868 sec  / total_loss : 0.3888 correct : 18808/22187 -> 84.7704%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 58/2000 epochs spend time : 1.6624 sec / total_loss : 0.4402 correct : 31950/36665 -> 87.1403%\n",
            "test dataset : 58/2000 epochs spend time : 0.8722 sec  / total_loss : 0.3659 correct : 19212/22187 -> 86.5912%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 59/2000 epochs spend time : 1.6274 sec / total_loss : 0.3409 correct : 33410/36665 -> 91.1223%\n",
            "test dataset : 59/2000 epochs spend time : 0.8897 sec  / total_loss : 0.2685 correct : 20066/22187 -> 90.4403%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 60/2000 epochs spend time : 1.6694 sec / total_loss : 0.2732 correct : 34366/36665 -> 93.7297%\n",
            "test dataset : 60/2000 epochs spend time : 0.8848 sec  / total_loss : 0.2986 correct : 19990/22187 -> 90.0978%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 61/2000 epochs spend time : 1.6228 sec / total_loss : 0.2313 correct : 34987/36665 -> 95.4234%\n",
            "test dataset : 61/2000 epochs spend time : 0.8759 sec  / total_loss : 0.1566 correct : 21328/22187 -> 96.1284%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 62/2000 epochs spend time : 1.6458 sec / total_loss : 0.1754 correct : 35822/36665 -> 97.7008%\n",
            "test dataset : 62/2000 epochs spend time : 0.8580 sec  / total_loss : 0.1366 correct : 21597/22187 -> 97.3408%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 63/2000 epochs spend time : 1.5999 sec / total_loss : 0.1510 correct : 36230/36665 -> 98.8136%\n",
            "test dataset : 63/2000 epochs spend time : 0.8712 sec  / total_loss : 0.2747 correct : 20499/22187 -> 92.3919%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 64/2000 epochs spend time : 1.6938 sec / total_loss : 0.2429 correct : 34954/36665 -> 95.3334%\n",
            "test dataset : 64/2000 epochs spend time : 0.9363 sec  / total_loss : 0.1442 correct : 21407/22187 -> 96.4844%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 65/2000 epochs spend time : 1.6337 sec / total_loss : 0.1492 correct : 36213/36665 -> 98.7672%\n",
            "test dataset : 65/2000 epochs spend time : 0.8803 sec  / total_loss : 0.1454 correct : 21485/22187 -> 96.8360%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 66/2000 epochs spend time : 1.6693 sec / total_loss : 0.1261 correct : 36451/36665 -> 99.4163%\n",
            "test dataset : 66/2000 epochs spend time : 0.8751 sec  / total_loss : 0.1068 correct : 21809/22187 -> 98.2963%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 67/2000 epochs spend time : 1.6287 sec / total_loss : 0.1116 correct : 36607/36665 -> 99.8418%\n",
            "test dataset : 67/2000 epochs spend time : 0.8882 sec  / total_loss : 0.1080 correct : 21824/22187 -> 98.3639%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 68/2000 epochs spend time : 1.6393 sec / total_loss : 0.1087 correct : 36619/36665 -> 99.8745%\n",
            "test dataset : 68/2000 epochs spend time : 0.8635 sec  / total_loss : 0.1126 correct : 21824/22187 -> 98.3639%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 69/2000 epochs spend time : 1.6222 sec / total_loss : 0.1074 correct : 36636/36665 -> 99.9209%\n",
            "test dataset : 69/2000 epochs spend time : 0.8760 sec  / total_loss : 0.1115 correct : 21829/22187 -> 98.3864%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 70/2000 epochs spend time : 1.6729 sec / total_loss : 0.1056 correct : 36640/36665 -> 99.9318%\n",
            "test dataset : 70/2000 epochs spend time : 0.8753 sec  / total_loss : 0.1087 correct : 21842/22187 -> 98.4450%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 71/2000 epochs spend time : 1.6134 sec / total_loss : 0.1041 correct : 36649/36665 -> 99.9564%\n",
            "test dataset : 71/2000 epochs spend time : 0.9009 sec  / total_loss : 0.1120 correct : 21838/22187 -> 98.4270%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 72/2000 epochs spend time : 1.6544 sec / total_loss : 0.1038 correct : 36652/36665 -> 99.9645%\n",
            "test dataset : 72/2000 epochs spend time : 0.8847 sec  / total_loss : 0.1147 correct : 21835/22187 -> 98.4135%\n",
            "best epoch : 43/2000 / val accuracy : 98.454050%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 73/2000 epochs spend time : 1.6190 sec / total_loss : 0.1026 correct : 36658/36665 -> 99.9809%\n",
            "test dataset : 73/2000 epochs spend time : 0.8830 sec  / total_loss : 0.1142 correct : 21846/22187 -> 98.4631%\n",
            "best epoch : 73/2000 / val accuracy : 98.463064%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 74/2000 epochs spend time : 1.7305 sec / total_loss : 0.1018 correct : 36661/36665 -> 99.9891%\n",
            "test dataset : 74/2000 epochs spend time : 0.8867 sec  / total_loss : 0.1132 correct : 21848/22187 -> 98.4721%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 75/2000 epochs spend time : 1.6463 sec / total_loss : 0.1013 correct : 36662/36665 -> 99.9918%\n",
            "test dataset : 75/2000 epochs spend time : 0.8826 sec  / total_loss : 0.1190 correct : 21837/22187 -> 98.4225%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 76/2000 epochs spend time : 1.6186 sec / total_loss : 0.1008 correct : 36662/36665 -> 99.9918%\n",
            "test dataset : 76/2000 epochs spend time : 0.8918 sec  / total_loss : 0.1169 correct : 21845/22187 -> 98.4586%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 77/2000 epochs spend time : 1.6497 sec / total_loss : 0.1005 correct : 36663/36665 -> 99.9945%\n",
            "test dataset : 77/2000 epochs spend time : 0.9231 sec  / total_loss : 0.1177 correct : 21845/22187 -> 98.4586%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 78/2000 epochs spend time : 1.6654 sec / total_loss : 0.1001 correct : 36663/36665 -> 99.9945%\n",
            "test dataset : 78/2000 epochs spend time : 0.9316 sec  / total_loss : 0.1184 correct : 21841/22187 -> 98.4405%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 79/2000 epochs spend time : 1.6340 sec / total_loss : 0.1000 correct : 36663/36665 -> 99.9945%\n",
            "test dataset : 79/2000 epochs spend time : 0.8891 sec  / total_loss : 0.1184 correct : 21843/22187 -> 98.4495%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 80/2000 epochs spend time : 1.6681 sec / total_loss : 0.0999 correct : 36663/36665 -> 99.9945%\n",
            "test dataset : 80/2000 epochs spend time : 0.8720 sec  / total_loss : 0.1191 correct : 21842/22187 -> 98.4450%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 81/2000 epochs spend time : 1.6343 sec / total_loss : 0.0998 correct : 36664/36665 -> 99.9973%\n",
            "test dataset : 81/2000 epochs spend time : 0.8870 sec  / total_loss : 0.1197 correct : 21841/22187 -> 98.4405%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 82/2000 epochs spend time : 1.6682 sec / total_loss : 0.0996 correct : 36664/36665 -> 99.9973%\n",
            "test dataset : 82/2000 epochs spend time : 0.9119 sec  / total_loss : 0.1204 correct : 21842/22187 -> 98.4450%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 83/2000 epochs spend time : 1.6564 sec / total_loss : 0.0995 correct : 36664/36665 -> 99.9973%\n",
            "test dataset : 83/2000 epochs spend time : 0.8860 sec  / total_loss : 0.1207 correct : 21842/22187 -> 98.4450%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 84/2000 epochs spend time : 1.6570 sec / total_loss : 0.0994 correct : 36664/36665 -> 99.9973%\n",
            "test dataset : 84/2000 epochs spend time : 0.8733 sec  / total_loss : 0.1206 correct : 21843/22187 -> 98.4495%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 85/2000 epochs spend time : 1.6309 sec / total_loss : 0.0992 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 85/2000 epochs spend time : 0.8836 sec  / total_loss : 0.1211 correct : 21843/22187 -> 98.4495%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 86/2000 epochs spend time : 1.6542 sec / total_loss : 0.0990 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 86/2000 epochs spend time : 0.8892 sec  / total_loss : 0.1225 correct : 21841/22187 -> 98.4405%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000125\n",
            "train dataset : 87/2000 epochs spend time : 1.6216 sec / total_loss : 0.0990 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 87/2000 epochs spend time : 0.9166 sec  / total_loss : 0.1217 correct : 21845/22187 -> 98.4586%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 88/2000 epochs spend time : 1.7722 sec / total_loss : 0.0988 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 88/2000 epochs spend time : 0.8707 sec  / total_loss : 0.1220 correct : 21844/22187 -> 98.4540%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 89/2000 epochs spend time : 1.6328 sec / total_loss : 0.0988 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 89/2000 epochs spend time : 0.8615 sec  / total_loss : 0.1225 correct : 21844/22187 -> 98.4540%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 90/2000 epochs spend time : 1.6487 sec / total_loss : 0.0988 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 90/2000 epochs spend time : 0.8936 sec  / total_loss : 0.1229 correct : 21843/22187 -> 98.4495%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 91/2000 epochs spend time : 1.6355 sec / total_loss : 0.0986 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 91/2000 epochs spend time : 0.8570 sec  / total_loss : 0.1232 correct : 21843/22187 -> 98.4495%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 92/2000 epochs spend time : 1.6194 sec / total_loss : 0.0987 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 92/2000 epochs spend time : 0.8752 sec  / total_loss : 0.1232 correct : 21843/22187 -> 98.4495%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 93/2000 epochs spend time : 1.6265 sec / total_loss : 0.0986 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 93/2000 epochs spend time : 0.8711 sec  / total_loss : 0.1234 correct : 21844/22187 -> 98.4540%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 94/2000 epochs spend time : 1.6487 sec / total_loss : 0.0985 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 94/2000 epochs spend time : 0.8758 sec  / total_loss : 0.1238 correct : 21844/22187 -> 98.4540%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 95/2000 epochs spend time : 1.6255 sec / total_loss : 0.0985 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 95/2000 epochs spend time : 0.8771 sec  / total_loss : 0.1240 correct : 21844/22187 -> 98.4540%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 96/2000 epochs spend time : 1.6406 sec / total_loss : 0.0985 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 96/2000 epochs spend time : 0.8668 sec  / total_loss : 0.1242 correct : 21843/22187 -> 98.4495%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 97/2000 epochs spend time : 1.6342 sec / total_loss : 0.0985 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 97/2000 epochs spend time : 0.8778 sec  / total_loss : 0.1245 correct : 21842/22187 -> 98.4450%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000063\n",
            "train dataset : 98/2000 epochs spend time : 1.6382 sec / total_loss : 0.0983 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 98/2000 epochs spend time : 0.8724 sec  / total_loss : 0.1249 correct : 21843/22187 -> 98.4495%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 99/2000 epochs spend time : 1.6247 sec / total_loss : 0.0984 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 99/2000 epochs spend time : 0.8776 sec  / total_loss : 0.1247 correct : 21843/22187 -> 98.4495%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 100/2000 epochs spend time : 1.6481 sec / total_loss : 0.0983 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 100/2000 epochs spend time : 0.8586 sec  / total_loss : 0.1246 correct : 21843/22187 -> 98.4495%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 101/2000 epochs spend time : 1.5922 sec / total_loss : 0.0982 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 101/2000 epochs spend time : 0.8770 sec  / total_loss : 0.1248 correct : 21843/22187 -> 98.4495%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 102/2000 epochs spend time : 1.6680 sec / total_loss : 0.0982 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 102/2000 epochs spend time : 0.8688 sec  / total_loss : 0.1248 correct : 21843/22187 -> 98.4495%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 103/2000 epochs spend time : 1.6029 sec / total_loss : 0.0983 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 103/2000 epochs spend time : 0.8740 sec  / total_loss : 0.1249 correct : 21843/22187 -> 98.4495%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 104/2000 epochs spend time : 1.6235 sec / total_loss : 0.0982 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 104/2000 epochs spend time : 0.8668 sec  / total_loss : 0.1250 correct : 21843/22187 -> 98.4495%\n",
            "best epoch : 74/2000 / val accuracy : 98.472078%\n",
            "==============================\n",
            "current_lr : 0.000031\n",
            "train dataset : 105/2000 epochs spend time : 1.6540 sec / total_loss : 0.0982 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 105/2000 epochs spend time : 0.8741 sec  / total_loss : 0.1252 correct : 21843/22187 -> 98.4495%\n",
            "Early Stopping\n",
            "best epoch : 74/2000 / accuracy : 98.472078%\n",
            "==============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCE73qoRBUC2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}