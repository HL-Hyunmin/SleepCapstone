{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SleepDataPreparation_and_Classification.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/SleepCapstone/blob/master/SleepDataPreparation_and_Classification_ipynb%20CELU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAGJRSb3xOT1"
      },
      "source": [
        "한림대학교 소프트웨어 융합대학\n",
        "빅데이터 캡스톤 프로젝트\n",
        "\n",
        "딥러닝에 기반한 수면 분류 모델 개발"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SaQVdPHgfmx",
        "outputId": "e22d6f55-04d4-4618-81b7-ab46e82a1747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohoEZomKw0cz",
        "outputId": "b937c6d1-cb1a-45be-c3ae-440bb1c08a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "cd drive/My\\ Drive"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/My Drive'\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvna-5vRzmTQ",
        "outputId": "87273eec-2db6-4e66-c64e-527d713134cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "# GPU 가용성 체크\n",
        "!nvidia-smi"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Oct 25 07:17:44 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0    32W /  70W |   1017MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF2HFogi4Nzq",
        "outputId": "9abb3eb3-2819-40cf-d4f4-638a78ae89bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip install pyedflib"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyedflib in /usr/local/lib/python3.6/dist-packages (0.1.19)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from pyedflib) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1zNHBNc4beq"
      },
      "source": [
        "import numpy as np\n",
        "from pyedflib import highlevel\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n9vmRl3x-1e"
      },
      "source": [
        "def search_signals_npy(dirname):\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for file in filenames if file.endswith(\".npy\")]\n",
        "    return filenames\n",
        "\n",
        "def search_correct_signals_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vSGY6sO9vgC",
        "outputId": "0daf6b02-e93e-435f-eb64-8a64c49bfa44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "\n",
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  '/content/drive/My Drive/data_2013_with_npy/origin_npy/annotations/remove_wake/'\n",
        "signals_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "total_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list:\n",
        "    label = np.load(path + filename)\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    \n",
        "    signals = np.load(signals_path+signals_filename)\n",
        "    \n",
        "    \n",
        "    #print('remove start index : %d / remove end index : %d'%(remove_start_index,remove_end_index))\n",
        "    #print(np.bincount(label,minlength=6))\n",
        "    if len(label) !=len(signals[0])//30//fs:\n",
        "        print('file is fault!!!')\n",
        "    for i in range(6):\n",
        "        total_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "print(total_label)\n",
        "\n",
        "x = np.arange(len(total_label))\n",
        "\n",
        "plt.bar(x,total_label,width=0.7)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4258  2762 17340  5575  7522    59]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 6 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVVZuGLx-ADW",
        "outputId": "32444376-72c5-4830-c0cc-623a96df177c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "fs = 100                                # Sampling rate (512 Hz)\n",
        "epoch_size = 30\n",
        "#data = np.random.uniform(0, 100, 1024)  # 2 sec of data b/w 0.0-100.0\n",
        "\n",
        "path =  '/content/drive/My Drive/data_2013_with_npy/origin_npy/annotations/remove_wake/'\n",
        "signals_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "annotations_npy_list = search_signals_npy(path)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "random.shuffle(annotations_npy_list)\n",
        "\n",
        "print(annotations_npy_list)\n",
        "\n",
        "trainDataset_count = 30\n",
        "testDataset_count = len(annotations_npy_list)-trainDataset_count\n",
        "\n",
        "print(trainDataset_count)\n",
        "print(testDataset_count)\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    label = np.load(path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4001EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4191EP-Hypnogram.npy']\n",
            "['SC4192EV-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4122EV-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy']\n",
            "30\n",
            "8\n",
            "[12.24051302  7.0606902  45.41848473 14.54779849 20.54740182  0.18511173]\n",
            "[7.64041850e+00 8.61784141e+00 4.95594714e+01 1.61618943e+01\n",
            " 1.79790749e+01 4.12995595e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAKH9Mb0_NF3"
      },
      "source": [
        "signals_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/'\n",
        "\n",
        "save_train_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/train/'\n",
        "save_test_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/test/'\n",
        "\n",
        "os.makedirs(save_train_path,exist_ok=True)\n",
        "os.makedirs(save_test_path,exist_ok=True)\n",
        "\n",
        "for filename in annotations_npy_list[:trainDataset_count]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_train_path+filename)\n",
        "    \n",
        "\n",
        "        \n",
        "for filename in annotations_npy_list[trainDataset_count:]:\n",
        "    signals_filename = search_correct_signals_npy(signals_path,filename)[0]\n",
        "    shutil.copy(signals_path+signals_filename,save_test_path+filename)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4EYtlUH-e9f",
        "outputId": "7de930ed-82a7-4b54-e058-6963882c8a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "def search_correct_annotations_npy(dirname,filename):\n",
        "    search_filename = filename.split('-')[0][:-2]\n",
        "    file_list = os.listdir(dirname)\n",
        "    filename = [file for file in file_list if search_filename in file if file.endswith(\"npy\")]\n",
        "    \n",
        "    return filename\n",
        "\n",
        "train_path =  '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/train/'\n",
        "test_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_path = '/content/drive/My Drive/data_2013_with_npy/origin_npy/annotations/remove_wake/'\n",
        "\n",
        "train_list = search_signals_npy(train_path)\n",
        "test_list = search_signals_npy(test_path)\n",
        "\n",
        "print(train_list)\n",
        "print(test_list)\n",
        "\n",
        "train_label = np.zeros([6],dtype=int)\n",
        "test_label = np.zeros([6],dtype=int)\n",
        "\n",
        "for filename in train_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        train_label[i] += np.bincount(label,minlength=6)[i]\n",
        "\n",
        "        \n",
        "for filename in test_list:\n",
        "    filename = search_correct_annotations_npy(annotations_path,filename)[0]\n",
        "    label = np.load(annotations_path + filename)\n",
        "    \n",
        "    for i in range(6):\n",
        "        test_label[i] += np.bincount(label,minlength=6)[i]\n",
        "        \n",
        "train_label = train_label / np.sum(train_label) * 100\n",
        "test_label = test_label / np.sum(test_label) * 100\n",
        "print(train_label)\n",
        "print(test_label)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SC4122EV-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy']\n",
            "['SC4151EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy']\n",
            "[11.50688668  7.36669849 45.97027138 15.04977499 19.94545207  0.16091641]\n",
            "[13.44931717  7.98215171 45.13453824 13.92707441 19.38522558  0.12169288]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBwqVERu_aw-"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pylab as plt\n",
        "from  torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSQaEXQr_h8m"
      },
      "source": [
        "def data_preprocessing_torch(signals): # 하나의 데이터셋에 대한 data_preprocessing (using torch)\n",
        "    signals = (signals - signals.mean(dim=1).unsqueeze(1))/signals.std(dim=1).unsqueeze(1)\n",
        "\n",
        "    return signals\n",
        "\n",
        "def data_preprocessing_oneToOne_torch(signals,min,max,max_value):\n",
        "    signals_std = (signals + max_value) / (2*max_value)\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def data_preprocessing_minmax_torch(signals,min,max):\n",
        "    signals_std = (signals - signals.min(dim=1).unsqueeze(1)) / (\n",
        "            signals.max(dim=1).unsqueeze(1) - signals.min(dim=1).unsqueeze(1))\n",
        "    signals_scaled = signals_std * (max - min) + min\n",
        "    return signals_scaled\n",
        "\n",
        "def get_dataset_one_channel_norm_withoutCut(dirname,annotations_dir,data_path,use_noise=True,epsilon=0.5,noise_scale=2e-6,preprocessing=True,norm_methods='Standard'):\n",
        "    # npy read!\n",
        "    path = dirname + data_path\n",
        "    signals = np.load(path)\n",
        "\n",
        "    signals = torch.from_numpy(signals).float().to(device)\n",
        "\n",
        "    if use_noise:\n",
        "        if np.random.rand() < epsilon:\n",
        "            # noise = np.random.normal(loc=0,scale=noise_scale,size=signals.shape)\n",
        "            # signals = signals + noise\n",
        "            noise = torch.normal(mean=0., std=noise_scale, size=signals.shape).to(device)\n",
        "            signals = signals + noise\n",
        "\n",
        "    if preprocessing:\n",
        "        if norm_methods == 'Standard':\n",
        "            signals = data_preprocessing_torch(signals)\n",
        "        elif norm_methods == 'minmax':\n",
        "            signals = data_preprocessing_minmax_torch(signals,0,1)\n",
        "        elif norm_methods == 'oneToOne':\n",
        "            signals = data_preprocessing_oneToOne_torch(signals,-1,1,1e-4)\n",
        "    label = get_annotations(annotations_dir, data_path)\n",
        "    return signals, label\n",
        "\n",
        "# model conv layer weight init function\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:         # Conv weight init\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "        \n",
        "def suffle_dataset_list(dataset_list): # 데이터 셔플\n",
        "    random.shuffle(dataset_list)\n",
        "    return dataset_list\n",
        "\n",
        "\n",
        "# npy파일을 통해 label을 가져오는 함수\n",
        "def get_annotations(label_dir,file_name):\n",
        "    label_path = label_dir + file_name\n",
        "    label = np.load(label_path)\n",
        "    return label\n",
        "\n",
        "def signals_expand_torch_one_channel(signals): # 2차원 데이터를 3차원으로 변환 (8,N) -> (batch,8,6000) 형태로\n",
        "    signals = signals.expand(1,1,-1)\n",
        "    #print(signals.shape)\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    signals = signals.reshape(-1, 3000, 1) # 형태 변환\n",
        "    signals = signals.transpose(2, 1) # 차원 변경\n",
        "    return signals"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx4M7Rfq_lwo"
      },
      "source": [
        "class DeepSleepNet_Classification(nn.Module):  # input channel = 8channel / output = 5\n",
        "    def __init__(self,in_channel=1,out_channel=6,layer=[64,128,128,128],sample_rate = 100):\n",
        "        super(DeepSleepNet_Classification, self).__init__()\n",
        "\n",
        "        \n",
        "        self.fc1 = nn.Linear(3000,1024)\n",
        "        self.fc2 = nn.Linear(1024,1024)\n",
        "        self.fc3 = nn.Linear(1024,512)\n",
        "        self.fc4 = nn.Linear(512, 256)\n",
        "        self.fc5 = nn.Linear(256, out_channel)\n",
        "\n",
        "        self.CELU = nn.CELU()\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "        # print(\"feature_extract_2d.shape : \", feature_extract_2d.shape)\n",
        "        # 여기서 문제 발생 weight의 경우에는 [64 , 32 , 100] 이지만 input 이 2차원 [32, 750]이라 문제 발생!\n",
        "        out = torch.flatten(input, 1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.CELU(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.CELU(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.CELU(out)\n",
        "        out = self.fc4(out)\n",
        "        out = self.CELU(out)\n",
        "        out = self.fc5(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYZ6YSZHArdJ",
        "outputId": "6d7a054e-2686-49a5-b4bb-25dde496e4de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "model = DeepSleepNet_Classification(in_channel=1,out_channel=6)\n",
        "summary(model.cuda(),(1,3000))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 1024]       3,073,024\n",
            "              CELU-2                 [-1, 1024]               0\n",
            "            Linear-3                 [-1, 1024]       1,049,600\n",
            "              CELU-4                 [-1, 1024]               0\n",
            "            Linear-5                  [-1, 512]         524,800\n",
            "              CELU-6                  [-1, 512]               0\n",
            "            Linear-7                  [-1, 256]         131,328\n",
            "              CELU-8                  [-1, 256]               0\n",
            "            Linear-9                    [-1, 6]           1,542\n",
            "================================================================\n",
            "Total params: 4,780,294\n",
            "Trainable params: 4,780,294\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.04\n",
            "Params size (MB): 18.24\n",
            "Estimated Total Size (MB): 18.29\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li3ftAC1Az0Q"
      },
      "source": [
        "def search_npy_list(dirname):  # 매개변수 dir에서 모든 npy파일을 찾고 fold에 따른 dataset 나누기\n",
        "    filenames = os.listdir(dirname)\n",
        "    filenames = [file for _, file in enumerate(filenames) if file.endswith(\".npy\")]\n",
        "    return filenames\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQH_VRRFBGF1"
      },
      "source": [
        "def train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, test_signal_dir,annotations_dir\n",
        "                               ,epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                               layer_filters=[64,128,256,512],first_conv=[200,40,100],optim='Adam',lf='CE',\n",
        "                               epsilon=0.7,noise_scale=2e-6,min_value=-1e-4,max_value=1e-4,preprocessing=True,\n",
        "                               norm_methods='Standard',use_noise=True,loss_type='softmax'):\n",
        "    # Adam optimizer param\n",
        "    b1 = 0.5\n",
        "    b2 = 0.999\n",
        "\n",
        "    beta = 0.001\n",
        "\n",
        "    check_file = open(logging_filename, 'w')  # logging file\n",
        "\n",
        "    print('Preproceesing  : ',preprocessing)\n",
        "    print('min/max value : %f/%f'%(min_value,max_value))\n",
        "    print('noise scale : ',noise_scale)\n",
        "    print('loss function : ',lf)\n",
        "    print('epsilon : ',epsilon)\n",
        "    print('norm methods : ',norm_methods)\n",
        "\n",
        "    print('logging file name : ', logging_filename)\n",
        "    print('save file name : ', save_filename)\n",
        "    print('layer filters : ',layer_filters)\n",
        "    print('fisrt_conv info : ',first_conv)\n",
        "    print('loss type : ',loss_type)\n",
        "    print('training data oversampling noise : ',use_noise)\n",
        "    best_accuracy = 0.\n",
        "    best_epoch = 0\n",
        "\n",
        "    train_dataset_list = search_npy_list(train_signal_dir)\n",
        "    test_dataset_list = search_npy_list(test_signal_dir)\n",
        "\n",
        "    train_dataset_len = len(train_dataset_list)\n",
        "    test_dataset_len = len(test_dataset_list)\n",
        "\n",
        "    print('train_dataset length : ', len(train_dataset_list))\n",
        "    print(train_dataset_list)\n",
        "\n",
        "    print('test_dataset length : ',test_dataset_len)\n",
        "    print(test_dataset_list)\n",
        "\n",
        "\n",
        "    model = DeepSleepNet_Classification()\n",
        "\n",
        "    #model = resnet18_200hz(in_channel=1,layer_filters=layer_filters,first_conv=first_conv,use_batchnorm=True,num_classes=5)\n",
        "\n",
        "    model.apply(weights_init) # weight init\n",
        "\n",
        "    cuda = torch.cuda.is_available()\n",
        "\n",
        "    if cuda:\n",
        "        print('can use CUDA!!!')\n",
        "        model = model.cuda()\n",
        "    #summary(model,[1,6000])\n",
        "    print('torch.cuda.device_count() : ', torch.cuda.device_count())\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print('Multi GPU Activation !!!')\n",
        "        #model = nn.DataParallel(model)\n",
        "\n",
        "    # loss funcition\n",
        "    if lf == 'CE':\n",
        "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "    elif lf == 'CEW':\n",
        "        samples_per_cls = [27,15,41,5,11]\n",
        "        no_of_classes = 5\n",
        "        effective_num = 1.0 - np.power(beta,samples_per_cls)\n",
        "        #print(effective_num)\n",
        "        weights = (1.0 - beta) / np.array(effective_num)\n",
        "        #print(weights)\n",
        "        weights = weights / np.sum(weights) * no_of_classes\n",
        "        weights = torch.tensor(weights).float()\n",
        "        weights = weights.to(device)\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=weights).to(device)\n",
        "    elif lf == 'FL':\n",
        "        loss_fn = FocalLoss(gamma=2).to(device)\n",
        "    elif lf == 'CBL':\n",
        "        loss_fn = CB_loss(samples_per_cls=[27,15,41,5,11],no_of_classes=5,loss_type=loss_type,beta=0.9999,gamma=2.0)\n",
        "    #loss_fn = FocalLoss(gamma=2).to(device)\n",
        "\n",
        "    # optimizer ADAM (SGD의 경우에는 정상적으로 학습이 진행되지 않았음)\n",
        "    if optim == 'Adam':\n",
        "        print('Optimizer : Adam')\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "    elif optim == 'RMS':\n",
        "        print('Optimizer : RMSprop')\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "    elif optim == 'SGD':\n",
        "        print('Optimizer : SGD')\n",
        "        optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=gamma, patience=10,\n",
        "                                                           min_lr=1e-6)\n",
        "    #stride = 40 일 때, batch_size = 20이면 16GB정도의 메모리 사용\n",
        "    batch_size = 5\n",
        "    norm_square = 2\n",
        "\n",
        "    train_batch_size = math.ceil(train_dataset_len / batch_size)\n",
        "    print('train_batch_size : ',train_batch_size)\n",
        "\n",
        "    test_batch_size = test_dataset_len\n",
        "\n",
        "    best_accuracy = 0.\n",
        "    stop_count = 0\n",
        "    for epoch in range(epochs):\n",
        "        train_dataset = suffle_dataset_list(train_dataset_list) # 매 epoch마다 train_dataset shuffle !\n",
        "        count = 0  # check batch\n",
        "        train_total_loss = 0.0\n",
        "        train_total_count = 0\n",
        "        train_total_data = 0\n",
        "\n",
        "        val_total_loss = 0.0\n",
        "        val_total_count = 0\n",
        "        val_total_data = 0\n",
        "\n",
        "        test_total_loss = 0.0\n",
        "        test_total_count = 0\n",
        "        test_total_data = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "\n",
        "        output_str = 'current_lr : %f\\n'%(optimizer.state_dict()['param_groups'][0]['lr'])\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "        for index, file_name in enumerate(train_dataset):\n",
        "            #print('index : ',index)\n",
        "            if index % batch_size == 0:\n",
        "                batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "            else:\n",
        "                new_signal, new_label = get_dataset_one_channel_norm_withoutCut(train_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=use_noise,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "                batch_signal = torch.cat((batch_signal, new_signal),dim=1)\n",
        "                batch_label = np.concatenate((batch_label, new_label))\n",
        "            count += 1\n",
        "            if count == batch_size or index == len(train_dataset) - 1:  # batch 학습 시작!\n",
        "                batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                #batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "                # batch_signal = torch.from_numpy(batch_signal).float().to(device)\n",
        "                batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "                optimizer.zero_grad()\n",
        "                # print(batch_signal.shape)\n",
        "                # print(batch_signal)\n",
        "                pred = model(batch_signal)\n",
        "                norm = 0\n",
        "\n",
        "                for parameter in model.parameters():\n",
        "                    norm += torch.norm(parameter, p=norm_square)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label) + beta * norm\n",
        "                #print('loss : ',loss.item())\n",
        "                # loss = loss_fn(pred, batch_label)\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                train_total_loss += loss.item()\n",
        "\n",
        "                train_total_count += check_count\n",
        "                train_total_data += len(batch_signal)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "                count = 0\n",
        "\n",
        "        train_total_loss /= train_batch_size\n",
        "        train_accuracy = train_total_count / train_total_data * 100\n",
        "\n",
        "        output_str = 'train dataset : %d/%d epochs spend time : %.4f sec / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, train_total_loss,\n",
        "                        train_total_count, train_total_data, train_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "    \n",
        "\n",
        "        #check test dataset\n",
        "        start_time = time.time()\n",
        "        for file_name in test_dataset_list:\n",
        "            batch_signal, batch_label = get_dataset_one_channel_norm_withoutCut(test_signal_dir,annotations_dir,file_name,\n",
        "                                                                         use_noise=False,epsilon=epsilon,noise_scale=noise_scale,\n",
        "                                                                         preprocessing=preprocessing,norm_methods=norm_methods)\n",
        "\n",
        "            batch_signal = signals_expand_torch_one_channel(batch_signal)\n",
        "\n",
        "            batch_label = torch.from_numpy(batch_label).long().to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred = model(batch_signal)\n",
        "\n",
        "                loss = loss_fn(pred, batch_label)\n",
        "\n",
        "                # acc\n",
        "                _, predict = torch.max(pred, 1)\n",
        "                check_count = (predict == batch_label).sum().item()\n",
        "\n",
        "                test_total_loss += loss.item()\n",
        "                test_total_count += check_count\n",
        "                test_total_data += len(batch_signal)\n",
        "\n",
        "                # 사용하지 않는 변수 제거\n",
        "                del (batch_signal)\n",
        "                del (batch_label)\n",
        "                del (loss)\n",
        "                del (pred)\n",
        "                torch.cuda.empty_cache()\n",
        "        test_total_loss /= test_batch_size\n",
        "        test_accuracy = test_total_count / test_total_data * 100\n",
        "\n",
        "\n",
        "\n",
        "        output_str = 'test dataset : %d/%d epochs spend time : %.4f sec  / total_loss : %.4f correct : %d/%d -> %.4f%%\\n' \\\n",
        "                     % (epoch + 1, epochs, time.time() - start_time, test_total_loss,\n",
        "                        test_total_count, test_total_data, test_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        check_file.write(output_str)\n",
        "\n",
        "        scheduler.step(float(test_total_loss))\n",
        "        #scheduler.step()\n",
        "\n",
        "        if epoch == 0:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_epoch = epoch\n",
        "            save_file = save_filename\n",
        "            #save_file = save_path + 'best_SleepEEGNet_CNN_channel%d.pth'%channel\n",
        "            torch.save(model.state_dict(),save_file)\n",
        "            stop_count = 0\n",
        "        else:\n",
        "            if best_accuracy < test_accuracy:\n",
        "                best_accuracy = test_accuracy\n",
        "                best_epoch = epoch\n",
        "                save_file = save_filename\n",
        "                torch.save(model.state_dict(), save_file)\n",
        "                stop_count = 0\n",
        "            else:\n",
        "                stop_count += 1\n",
        "        if stop_count > 30:\n",
        "            print('Early Stopping')\n",
        "            break\n",
        "\n",
        "        output_str = 'best epoch : %d/%d / val accuracy : %f%%\\n' \\\n",
        "                     % (best_epoch+1, epochs, best_accuracy)\n",
        "        sys.stdout.write(output_str)\n",
        "        print('=' * 30)\n",
        "\n",
        "\n",
        "    output_str = 'best epoch : %d/%d / accuracy : %f%%\\n' \\\n",
        "                 % (best_epoch+1, epochs, best_accuracy)\n",
        "    sys.stdout.write(output_str)\n",
        "    check_file.write(output_str)\n",
        "    print('=' * 30)\n",
        "\n",
        "    check_file.close()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-qN1b_NBMC1",
        "outputId": "704811ec-29b9-4fa4-d373-6d826ead018d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "save_filename = './train.pth'\n",
        "logging_filename = './logging.txt'\n",
        "train_signal_dir = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/train/'\n",
        "test_signal_dir = '/content/drive/My Drive/data_2013_with_npy/origin_npy/Fpz-Cz/remove_wake/test/'\n",
        "annotations_dir = '/content/drive/My Drive/data_2013_with_npy/origin_npy/annotations/remove_wake/'\n",
        "train_model_withNoise_norm(save_filename,logging_filename,train_signal_dir, \n",
        "                           test_signal_dir,annotations_dir,\n",
        "                           epochs=2000,learning_rate=0.001,step_size=100,gamma=0.5,channel=0,\n",
        "                           layer_filters=[64,128,256,512],first_conv=[200,40,100],\n",
        "                           optim='Adam',lf='CE',epsilon=0,noise_scale=0,\n",
        "                           min_value=-0,max_value=0,preprocessing=True,\n",
        "                           norm_methods='Standard',use_noise=False,loss_type='softmax')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preproceesing  :  True\n",
            "min/max value : 0.000000/0.000000\n",
            "noise scale :  0\n",
            "loss function :  CE\n",
            "epsilon :  0\n",
            "norm methods :  Standard\n",
            "logging file name :  ./logging.txt\n",
            "save file name :  ./train.pth\n",
            "layer filters :  [64, 128, 256, 512]\n",
            "fisrt_conv info :  [200, 40, 100]\n",
            "loss type :  softmax\n",
            "training data oversampling noise :  False\n",
            "train_dataset length :  37\n",
            "['SC4122EV-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4091EC-Hypnogram.npy', 'SC4131EC-Hypnogram.npy', 'SC4142EU-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4061EC-Hypnogram.npy', 'SC4092EC-Hypnogram.npy', 'SC4112EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4111EC-Hypnogram.npy', 'SC4171EU-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4191EP-Hypnogram.npy', 'SC4081EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4021EH-Hypnogram.npy', 'SC4181EC-Hypnogram.npy', 'SC4161EC-Hypnogram.npy', 'SC4052EC-Hypnogram.npy', 'SC4082EP-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4062EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4151EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy']\n",
            "test_dataset length :  22\n",
            "['SC4151EC-Hypnogram.npy', 'SC4141EU-Hypnogram.npy', 'SC4031EC-Hypnogram.npy', 'SC4072EH-Hypnogram.npy', 'SC4101EC-Hypnogram.npy', 'SC4042EC-Hypnogram.npy', 'SC4182EC-Hypnogram.npy', 'SC4002EC-Hypnogram.npy', 'SC4022EJ-Hypnogram.npy', 'SC4192EV-Hypnogram.npy', 'SC4071EC-Hypnogram.npy', 'SC4011EH-Hypnogram.npy', 'SC4172EC-Hypnogram.npy', 'SC4121EC-Hypnogram.npy', 'SC4152EC-Hypnogram.npy', 'SC4032EP-Hypnogram.npy', 'SC4102EC-Hypnogram.npy', 'SC4012EC-Hypnogram.npy', 'SC4041EC-Hypnogram.npy', 'SC4001EC-Hypnogram.npy', 'SC4051EC-Hypnogram.npy', 'SC4181EC-Hypnogram.npy']\n",
            "can use CUDA!!!\n",
            "torch.cuda.device_count() :  1\n",
            "Optimizer : Adam\n",
            "train_batch_size :  8\n",
            "current_lr : 0.001000\n",
            "train dataset : 1/2000 epochs spend time : 1.7473 sec / total_loss : 1.6719 correct : 12421/36665 -> 33.8770%\n",
            "test dataset : 1/2000 epochs spend time : 0.8948 sec  / total_loss : 1.3639 correct : 9920/22187 -> 44.7109%\n",
            "best epoch : 1/2000 / val accuracy : 44.710867%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 2/2000 epochs spend time : 1.6872 sec / total_loss : 1.3529 correct : 17754/36665 -> 48.4222%\n",
            "test dataset : 2/2000 epochs spend time : 0.8858 sec  / total_loss : 2.2897 correct : 5223/22187 -> 23.5408%\n",
            "best epoch : 1/2000 / val accuracy : 44.710867%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 3/2000 epochs spend time : 1.6818 sec / total_loss : 1.6052 correct : 15111/36665 -> 41.2137%\n",
            "test dataset : 3/2000 epochs spend time : 0.8752 sec  / total_loss : 1.2032 correct : 11510/22187 -> 51.8772%\n",
            "best epoch : 3/2000 / val accuracy : 51.877225%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 4/2000 epochs spend time : 1.6763 sec / total_loss : 1.2461 correct : 19601/36665 -> 53.4597%\n",
            "test dataset : 4/2000 epochs spend time : 0.8862 sec  / total_loss : 1.1267 correct : 11991/22187 -> 54.0452%\n",
            "best epoch : 4/2000 / val accuracy : 54.045162%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 5/2000 epochs spend time : 1.6671 sec / total_loss : 1.2372 correct : 20146/36665 -> 54.9461%\n",
            "test dataset : 5/2000 epochs spend time : 0.9010 sec  / total_loss : 1.2279 correct : 11233/22187 -> 50.6287%\n",
            "best epoch : 4/2000 / val accuracy : 54.045162%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 6/2000 epochs spend time : 1.6831 sec / total_loss : 1.1572 correct : 21039/36665 -> 57.3817%\n",
            "test dataset : 6/2000 epochs spend time : 0.9088 sec  / total_loss : 1.1726 correct : 11170/22187 -> 50.3448%\n",
            "best epoch : 4/2000 / val accuracy : 54.045162%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 7/2000 epochs spend time : 1.6988 sec / total_loss : 1.1384 correct : 21146/36665 -> 57.6735%\n",
            "test dataset : 7/2000 epochs spend time : 0.8862 sec  / total_loss : 1.0900 correct : 12268/22187 -> 55.2936%\n",
            "best epoch : 7/2000 / val accuracy : 55.293640%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 8/2000 epochs spend time : 1.7088 sec / total_loss : 1.0721 correct : 21912/36665 -> 59.7627%\n",
            "test dataset : 8/2000 epochs spend time : 0.8712 sec  / total_loss : 0.9147 correct : 14122/22187 -> 63.6499%\n",
            "best epoch : 8/2000 / val accuracy : 63.649885%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 9/2000 epochs spend time : 1.6767 sec / total_loss : 0.9981 correct : 23081/36665 -> 62.9510%\n",
            "test dataset : 9/2000 epochs spend time : 0.8792 sec  / total_loss : 0.8799 correct : 14160/22187 -> 63.8212%\n",
            "best epoch : 9/2000 / val accuracy : 63.821157%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 10/2000 epochs spend time : 1.6810 sec / total_loss : 0.9543 correct : 23862/36665 -> 65.0811%\n",
            "test dataset : 10/2000 epochs spend time : 0.8755 sec  / total_loss : 0.8418 correct : 14664/22187 -> 66.0928%\n",
            "best epoch : 10/2000 / val accuracy : 66.092757%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 11/2000 epochs spend time : 1.6794 sec / total_loss : 0.9849 correct : 23657/36665 -> 64.5220%\n",
            "test dataset : 11/2000 epochs spend time : 0.8642 sec  / total_loss : 0.9588 correct : 13278/22187 -> 59.8459%\n",
            "best epoch : 10/2000 / val accuracy : 66.092757%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 12/2000 epochs spend time : 1.6408 sec / total_loss : 0.9816 correct : 23918/36665 -> 65.2339%\n",
            "test dataset : 12/2000 epochs spend time : 0.8609 sec  / total_loss : 1.3592 correct : 9684/22187 -> 43.6472%\n",
            "best epoch : 10/2000 / val accuracy : 66.092757%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 13/2000 epochs spend time : 1.6125 sec / total_loss : 1.0268 correct : 22645/36665 -> 61.7619%\n",
            "test dataset : 13/2000 epochs spend time : 0.8774 sec  / total_loss : 0.8432 correct : 14722/22187 -> 66.3542%\n",
            "best epoch : 13/2000 / val accuracy : 66.354171%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 14/2000 epochs spend time : 1.6502 sec / total_loss : 0.9476 correct : 24106/36665 -> 65.7466%\n",
            "test dataset : 14/2000 epochs spend time : 0.8737 sec  / total_loss : 0.7796 correct : 15371/22187 -> 69.2793%\n",
            "best epoch : 14/2000 / val accuracy : 69.279308%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 15/2000 epochs spend time : 1.6815 sec / total_loss : 0.8780 correct : 25420/36665 -> 69.3304%\n",
            "test dataset : 15/2000 epochs spend time : 0.8720 sec  / total_loss : 0.8445 correct : 14503/22187 -> 65.3671%\n",
            "best epoch : 14/2000 / val accuracy : 69.279308%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 16/2000 epochs spend time : 1.6535 sec / total_loss : 0.8576 correct : 25662/36665 -> 69.9905%\n",
            "test dataset : 16/2000 epochs spend time : 0.8777 sec  / total_loss : 0.7525 correct : 15596/22187 -> 70.2934%\n",
            "best epoch : 16/2000 / val accuracy : 70.293415%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 17/2000 epochs spend time : 1.6928 sec / total_loss : 0.8466 correct : 25598/36665 -> 69.8159%\n",
            "test dataset : 17/2000 epochs spend time : 0.8794 sec  / total_loss : 0.7585 correct : 14988/22187 -> 67.5531%\n",
            "best epoch : 16/2000 / val accuracy : 70.293415%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 18/2000 epochs spend time : 1.6613 sec / total_loss : 0.8057 correct : 25967/36665 -> 70.8223%\n",
            "test dataset : 18/2000 epochs spend time : 0.8804 sec  / total_loss : 0.7683 correct : 15401/22187 -> 69.4145%\n",
            "best epoch : 16/2000 / val accuracy : 70.293415%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 19/2000 epochs spend time : 1.6669 sec / total_loss : 0.7760 correct : 26877/36665 -> 73.3042%\n",
            "test dataset : 19/2000 epochs spend time : 0.8902 sec  / total_loss : 0.8647 correct : 14896/22187 -> 67.1384%\n",
            "best epoch : 16/2000 / val accuracy : 70.293415%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 20/2000 epochs spend time : 1.7385 sec / total_loss : 0.8585 correct : 25836/36665 -> 70.4650%\n",
            "test dataset : 20/2000 epochs spend time : 0.8790 sec  / total_loss : 0.7871 correct : 15212/22187 -> 68.5627%\n",
            "best epoch : 16/2000 / val accuracy : 70.293415%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 21/2000 epochs spend time : 1.6646 sec / total_loss : 0.7447 correct : 26937/36665 -> 73.4679%\n",
            "test dataset : 21/2000 epochs spend time : 0.8808 sec  / total_loss : 0.6978 correct : 16167/22187 -> 72.8670%\n",
            "best epoch : 21/2000 / val accuracy : 72.866994%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 22/2000 epochs spend time : 1.6755 sec / total_loss : 0.6807 correct : 27974/36665 -> 76.2962%\n",
            "test dataset : 22/2000 epochs spend time : 0.8851 sec  / total_loss : 0.5545 correct : 17636/22187 -> 79.4880%\n",
            "best epoch : 22/2000 / val accuracy : 79.487988%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 23/2000 epochs spend time : 1.6851 sec / total_loss : 0.6918 correct : 28018/36665 -> 76.4162%\n",
            "test dataset : 23/2000 epochs spend time : 0.8804 sec  / total_loss : 0.6784 correct : 16245/22187 -> 73.2186%\n",
            "best epoch : 22/2000 / val accuracy : 79.487988%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 24/2000 epochs spend time : 1.7376 sec / total_loss : 0.6668 correct : 28447/36665 -> 77.5863%\n",
            "test dataset : 24/2000 epochs spend time : 0.9665 sec  / total_loss : 0.6665 correct : 16687/22187 -> 75.2107%\n",
            "best epoch : 22/2000 / val accuracy : 79.487988%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 25/2000 epochs spend time : 1.6658 sec / total_loss : 0.6114 correct : 29398/36665 -> 80.1800%\n",
            "test dataset : 25/2000 epochs spend time : 0.8703 sec  / total_loss : 0.5938 correct : 17053/22187 -> 76.8603%\n",
            "best epoch : 22/2000 / val accuracy : 79.487988%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 26/2000 epochs spend time : 1.6550 sec / total_loss : 0.5816 correct : 29644/36665 -> 80.8509%\n",
            "test dataset : 26/2000 epochs spend time : 0.8633 sec  / total_loss : 1.0450 correct : 14791/22187 -> 66.6652%\n",
            "best epoch : 22/2000 / val accuracy : 79.487988%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 27/2000 epochs spend time : 1.6225 sec / total_loss : 0.7131 correct : 27744/36665 -> 75.6689%\n",
            "test dataset : 27/2000 epochs spend time : 0.8762 sec  / total_loss : 0.4429 correct : 18283/22187 -> 82.4041%\n",
            "best epoch : 27/2000 / val accuracy : 82.404111%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 28/2000 epochs spend time : 1.6915 sec / total_loss : 0.4968 correct : 30865/36665 -> 84.1811%\n",
            "test dataset : 28/2000 epochs spend time : 0.8607 sec  / total_loss : 0.4714 correct : 18202/22187 -> 82.0390%\n",
            "best epoch : 27/2000 / val accuracy : 82.404111%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 29/2000 epochs spend time : 1.6321 sec / total_loss : 0.4816 correct : 31177/36665 -> 85.0320%\n",
            "test dataset : 29/2000 epochs spend time : 0.8987 sec  / total_loss : 0.4148 correct : 18902/22187 -> 85.1940%\n",
            "best epoch : 29/2000 / val accuracy : 85.194033%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 30/2000 epochs spend time : 1.6845 sec / total_loss : 0.4297 correct : 31713/36665 -> 86.4939%\n",
            "test dataset : 30/2000 epochs spend time : 0.8694 sec  / total_loss : 0.3617 correct : 19293/22187 -> 86.9563%\n",
            "best epoch : 30/2000 / val accuracy : 86.956326%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 31/2000 epochs spend time : 1.6721 sec / total_loss : 0.4003 correct : 32481/36665 -> 88.5886%\n",
            "test dataset : 31/2000 epochs spend time : 0.8730 sec  / total_loss : 0.3711 correct : 19173/22187 -> 86.4155%\n",
            "best epoch : 30/2000 / val accuracy : 86.956326%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 32/2000 epochs spend time : 1.6521 sec / total_loss : 0.3978 correct : 32379/36665 -> 88.3104%\n",
            "test dataset : 32/2000 epochs spend time : 0.9262 sec  / total_loss : 0.2709 correct : 20236/22187 -> 91.2066%\n",
            "best epoch : 32/2000 / val accuracy : 91.206562%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 33/2000 epochs spend time : 1.6707 sec / total_loss : 0.3458 correct : 33423/36665 -> 91.1578%\n",
            "test dataset : 33/2000 epochs spend time : 0.8846 sec  / total_loss : 0.8634 correct : 16231/22187 -> 73.1555%\n",
            "best epoch : 32/2000 / val accuracy : 91.206562%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 34/2000 epochs spend time : 1.6373 sec / total_loss : 1.0430 correct : 25541/36665 -> 69.6604%\n",
            "test dataset : 34/2000 epochs spend time : 0.9003 sec  / total_loss : 2.3036 correct : 7171/22187 -> 32.3207%\n",
            "best epoch : 32/2000 / val accuracy : 91.206562%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 35/2000 epochs spend time : 1.6336 sec / total_loss : 1.0435 correct : 23695/36665 -> 64.6257%\n",
            "test dataset : 35/2000 epochs spend time : 0.9230 sec  / total_loss : 0.5209 correct : 17904/22187 -> 80.6959%\n",
            "best epoch : 32/2000 / val accuracy : 91.206562%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 36/2000 epochs spend time : 1.6569 sec / total_loss : 0.5109 correct : 31087/36665 -> 84.7866%\n",
            "test dataset : 36/2000 epochs spend time : 0.9024 sec  / total_loss : 0.5249 correct : 17618/22187 -> 79.4069%\n",
            "best epoch : 32/2000 / val accuracy : 91.206562%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 37/2000 epochs spend time : 1.6419 sec / total_loss : 0.4090 correct : 32235/36665 -> 87.9176%\n",
            "test dataset : 37/2000 epochs spend time : 0.8838 sec  / total_loss : 0.3792 correct : 19372/22187 -> 87.3124%\n",
            "best epoch : 32/2000 / val accuracy : 91.206562%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 38/2000 epochs spend time : 1.6279 sec / total_loss : 0.3293 correct : 33332/36665 -> 90.9096%\n",
            "test dataset : 38/2000 epochs spend time : 0.8851 sec  / total_loss : 0.2040 correct : 20825/22187 -> 93.8613%\n",
            "best epoch : 38/2000 / val accuracy : 93.861270%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 39/2000 epochs spend time : 1.6555 sec / total_loss : 0.2099 correct : 35399/36665 -> 96.5471%\n",
            "test dataset : 39/2000 epochs spend time : 0.8776 sec  / total_loss : 0.2423 correct : 20445/22187 -> 92.1486%\n",
            "best epoch : 38/2000 / val accuracy : 93.861270%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 40/2000 epochs spend time : 1.6714 sec / total_loss : 0.2498 correct : 34770/36665 -> 94.8316%\n",
            "test dataset : 40/2000 epochs spend time : 0.9380 sec  / total_loss : 0.1491 correct : 21327/22187 -> 96.1239%\n",
            "best epoch : 40/2000 / val accuracy : 96.123856%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 41/2000 epochs spend time : 1.6730 sec / total_loss : 0.1477 correct : 36135/36665 -> 98.5545%\n",
            "test dataset : 41/2000 epochs spend time : 0.8857 sec  / total_loss : 0.1076 correct : 21711/22187 -> 97.8546%\n",
            "best epoch : 41/2000 / val accuracy : 97.854600%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 42/2000 epochs spend time : 1.6544 sec / total_loss : 0.1381 correct : 36141/36665 -> 98.5708%\n",
            "test dataset : 42/2000 epochs spend time : 0.9314 sec  / total_loss : 0.1098 correct : 21682/22187 -> 97.7239%\n",
            "best epoch : 41/2000 / val accuracy : 97.854600%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 43/2000 epochs spend time : 1.6306 sec / total_loss : 0.1264 correct : 36272/36665 -> 98.9281%\n",
            "test dataset : 43/2000 epochs spend time : 0.8730 sec  / total_loss : 0.1006 correct : 21787/22187 -> 98.1971%\n",
            "best epoch : 43/2000 / val accuracy : 98.197142%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 44/2000 epochs spend time : 1.6882 sec / total_loss : 0.1136 correct : 36480/36665 -> 99.4954%\n",
            "test dataset : 44/2000 epochs spend time : 0.9296 sec  / total_loss : 0.1024 correct : 21808/22187 -> 98.2918%\n",
            "best epoch : 44/2000 / val accuracy : 98.291792%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 45/2000 epochs spend time : 1.6698 sec / total_loss : 0.1293 correct : 36434/36665 -> 99.3700%\n",
            "test dataset : 45/2000 epochs spend time : 0.8674 sec  / total_loss : 0.0934 correct : 21851/22187 -> 98.4856%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 46/2000 epochs spend time : 1.6439 sec / total_loss : 0.1183 correct : 36472/36665 -> 99.4736%\n",
            "test dataset : 46/2000 epochs spend time : 0.9313 sec  / total_loss : 0.1083 correct : 21810/22187 -> 98.3008%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 47/2000 epochs spend time : 1.7029 sec / total_loss : 0.1098 correct : 36513/36665 -> 99.5854%\n",
            "test dataset : 47/2000 epochs spend time : 0.9048 sec  / total_loss : 0.1024 correct : 21835/22187 -> 98.4135%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 48/2000 epochs spend time : 1.7514 sec / total_loss : 0.0972 correct : 36635/36665 -> 99.9182%\n",
            "test dataset : 48/2000 epochs spend time : 0.9079 sec  / total_loss : 0.1021 correct : 21842/22187 -> 98.4450%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 49/2000 epochs spend time : 1.6515 sec / total_loss : 0.0955 correct : 36641/36665 -> 99.9345%\n",
            "test dataset : 49/2000 epochs spend time : 0.8849 sec  / total_loss : 0.1034 correct : 21841/22187 -> 98.4405%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 50/2000 epochs spend time : 1.6580 sec / total_loss : 0.0953 correct : 36646/36665 -> 99.9482%\n",
            "test dataset : 50/2000 epochs spend time : 0.8814 sec  / total_loss : 0.1116 correct : 21820/22187 -> 98.3459%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 51/2000 epochs spend time : 1.6295 sec / total_loss : 0.1217 correct : 36448/36665 -> 99.4082%\n",
            "test dataset : 51/2000 epochs spend time : 0.8823 sec  / total_loss : 0.1079 correct : 21832/22187 -> 98.4000%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 52/2000 epochs spend time : 1.6715 sec / total_loss : 0.1152 correct : 36511/36665 -> 99.5800%\n",
            "test dataset : 52/2000 epochs spend time : 0.9172 sec  / total_loss : 0.1030 correct : 21840/22187 -> 98.4360%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 53/2000 epochs spend time : 1.6432 sec / total_loss : 0.1068 correct : 36595/36665 -> 99.8091%\n",
            "test dataset : 53/2000 epochs spend time : 0.8785 sec  / total_loss : 0.1549 correct : 21474/22187 -> 96.7864%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 54/2000 epochs spend time : 1.6300 sec / total_loss : 1.1568 correct : 29444/36665 -> 80.3055%\n",
            "test dataset : 54/2000 epochs spend time : 0.9155 sec  / total_loss : 1.5854 correct : 7360/22187 -> 33.1726%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 55/2000 epochs spend time : 1.6477 sec / total_loss : 1.2951 correct : 20204/36665 -> 55.1043%\n",
            "test dataset : 55/2000 epochs spend time : 0.8993 sec  / total_loss : 0.7278 correct : 15718/22187 -> 70.8433%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.001000\n",
            "train dataset : 56/2000 epochs spend time : 1.6375 sec / total_loss : 0.6546 correct : 29014/36665 -> 79.1327%\n",
            "test dataset : 56/2000 epochs spend time : 0.8952 sec  / total_loss : 0.4908 correct : 18141/22187 -> 81.7641%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 57/2000 epochs spend time : 1.6564 sec / total_loss : 0.4128 correct : 32197/36665 -> 87.8140%\n",
            "test dataset : 57/2000 epochs spend time : 0.8723 sec  / total_loss : 0.2494 correct : 20578/22187 -> 92.7480%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 58/2000 epochs spend time : 1.6455 sec / total_loss : 0.2720 correct : 34596/36665 -> 94.3570%\n",
            "test dataset : 58/2000 epochs spend time : 0.8995 sec  / total_loss : 0.2077 correct : 20928/22187 -> 94.3255%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 59/2000 epochs spend time : 1.6382 sec / total_loss : 0.2153 correct : 35330/36665 -> 96.3589%\n",
            "test dataset : 59/2000 epochs spend time : 0.8778 sec  / total_loss : 0.2086 correct : 20977/22187 -> 94.5464%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 60/2000 epochs spend time : 1.6677 sec / total_loss : 0.2204 correct : 35129/36665 -> 95.8107%\n",
            "test dataset : 60/2000 epochs spend time : 0.8938 sec  / total_loss : 0.1737 correct : 21165/22187 -> 95.3937%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 61/2000 epochs spend time : 1.6381 sec / total_loss : 0.1602 correct : 36094/36665 -> 98.4427%\n",
            "test dataset : 61/2000 epochs spend time : 0.8783 sec  / total_loss : 0.1208 correct : 21699/22187 -> 97.8005%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 62/2000 epochs spend time : 1.6335 sec / total_loss : 0.1243 correct : 36456/36665 -> 99.4300%\n",
            "test dataset : 62/2000 epochs spend time : 0.8753 sec  / total_loss : 0.1036 correct : 21807/22187 -> 98.2873%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 63/2000 epochs spend time : 1.6081 sec / total_loss : 0.1089 correct : 36618/36665 -> 99.8718%\n",
            "test dataset : 63/2000 epochs spend time : 0.8809 sec  / total_loss : 0.1077 correct : 21814/22187 -> 98.3188%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 64/2000 epochs spend time : 1.6902 sec / total_loss : 0.1042 correct : 36652/36665 -> 99.9645%\n",
            "test dataset : 64/2000 epochs spend time : 0.8763 sec  / total_loss : 0.1017 correct : 21836/22187 -> 98.4180%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 65/2000 epochs spend time : 1.6462 sec / total_loss : 0.1017 correct : 36650/36665 -> 99.9591%\n",
            "test dataset : 65/2000 epochs spend time : 0.8666 sec  / total_loss : 0.1068 correct : 21832/22187 -> 98.4000%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 66/2000 epochs spend time : 1.6353 sec / total_loss : 0.0987 correct : 36662/36665 -> 99.9918%\n",
            "test dataset : 66/2000 epochs spend time : 0.8799 sec  / total_loss : 0.1118 correct : 21825/22187 -> 98.3684%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000500\n",
            "train dataset : 67/2000 epochs spend time : 1.6285 sec / total_loss : 0.0973 correct : 36664/36665 -> 99.9973%\n",
            "test dataset : 67/2000 epochs spend time : 0.8726 sec  / total_loss : 0.1146 correct : 21827/22187 -> 98.3774%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 68/2000 epochs spend time : 1.6558 sec / total_loss : 0.0964 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 68/2000 epochs spend time : 0.9080 sec  / total_loss : 0.1150 correct : 21827/22187 -> 98.3774%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 69/2000 epochs spend time : 1.6434 sec / total_loss : 0.0963 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 69/2000 epochs spend time : 0.8812 sec  / total_loss : 0.1154 correct : 21830/22187 -> 98.3909%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 70/2000 epochs spend time : 1.6262 sec / total_loss : 0.0960 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 70/2000 epochs spend time : 0.8981 sec  / total_loss : 0.1169 correct : 21827/22187 -> 98.3774%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 71/2000 epochs spend time : 1.6770 sec / total_loss : 0.0957 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 71/2000 epochs spend time : 0.9512 sec  / total_loss : 0.1199 correct : 21825/22187 -> 98.3684%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 72/2000 epochs spend time : 1.6755 sec / total_loss : 0.0954 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 72/2000 epochs spend time : 0.8778 sec  / total_loss : 0.1176 correct : 21831/22187 -> 98.3955%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 73/2000 epochs spend time : 1.6191 sec / total_loss : 0.0952 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 73/2000 epochs spend time : 0.8704 sec  / total_loss : 0.1210 correct : 21824/22187 -> 98.3639%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 74/2000 epochs spend time : 1.6237 sec / total_loss : 0.0950 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 74/2000 epochs spend time : 0.8795 sec  / total_loss : 0.1214 correct : 21825/22187 -> 98.3684%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 75/2000 epochs spend time : 1.5904 sec / total_loss : 0.0948 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 75/2000 epochs spend time : 0.8549 sec  / total_loss : 0.1223 correct : 21825/22187 -> 98.3684%\n",
            "best epoch : 45/2000 / val accuracy : 98.485600%\n",
            "==============================\n",
            "current_lr : 0.000250\n",
            "train dataset : 76/2000 epochs spend time : 1.6263 sec / total_loss : 0.0948 correct : 36665/36665 -> 100.0000%\n",
            "test dataset : 76/2000 epochs spend time : 0.8945 sec  / total_loss : 0.1218 correct : 21830/22187 -> 98.3909%\n",
            "Early Stopping\n",
            "best epoch : 45/2000 / accuracy : 98.485600%\n",
            "==============================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}